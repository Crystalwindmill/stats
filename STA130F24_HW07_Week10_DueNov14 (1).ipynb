{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66d4cf7",
   "metadata": {},
   "source": [
    "# STA130 Homework 07\n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c467d",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Pre-lecture\" and \"Post-lecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Pre-lecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "    \n",
    "### Prompt Engineering? \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f84f1d",
   "metadata": {},
   "source": [
    "\n",
    "### Marking Rubric (which may award partial credit) \n",
    "\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 4\"\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 7\"\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 9\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ef03a",
   "metadata": {},
   "source": [
    "## \"Pre-lecture\" HW [*completion prior to next LEC is suggested but not mandatory*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf63d8b",
   "metadata": {},
   "source": [
    "### 1. Explain succinctly in your own words (but working with a ChatBot if needed)...<br>\n",
    "\n",
    "1. the difference between **Simple Linear Regression** and **Multiple Linear Regression**; and the benefit the latter provides over the former\n",
    "\n",
    "\n",
    "2. the difference between using a **continuous variable** and an **indicator variable** in **Simple Linear Regression**; and these two **linear forms**\n",
    "\n",
    "\n",
    "3. the change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single **indicator variable** is introduced alongside a **continuous variable** to create a **Multiple Linear Regression**; and these two **linear forms** (i.e., the **Simple Linear Regression** versus the **Multiple Linear Regression**)\n",
    "\n",
    "\n",
    "4. the effect of adding an **interaction** between a **continuous** and an **indicator variable** in **Multiple Linear Regression** models; and this **linear form**\n",
    "\n",
    "\n",
    "5. the behavior of a **Multiple Linear Regression** model (i.e., the expected nature of the data it models) based only on **indicator variables** derived from a **non-binary categorical variable**; this **linear form**; and the necessarily resulting **binary variable encodings** it utilizes\n",
    "       \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The requested **linear forms** are **equations**, and answers must include **equations** along with the explanations that interpret the **linear forms**. Write you **linear forms** using a style something like_\n",
    "> \n",
    "> - _\"outcome = $\\beta_0$ + $\\beta_A$ predictorA + $\\beta_B$ 1(predictorB)\"_ \n",
    "> - _where the \"1(.)\" notation is for indicator variables_ \n",
    "> - _or feel free to use an similar alternative if a ChatBot provides you with another notation you think is clearer and like better if you prefer_\n",
    ">\n",
    "> _DO INCLUDE the **intercept** in your **linear forms**. You don't have to include notation related to the **error** term since this is essentially always assumed (and, actually, we usually don't even bother to include the **intercept** in such shorthand specifications either, for the same reason), but don't forget to include the **intercept** here this time (for practice). The modeling **assumptions** do not need to be addressed beyond this, but explanations will likely address the number of variables and the essential use-case (perhaps illustrated through examples) the different models imply._    \n",
    "> \n",
    "> _Answers to the final question above should address the notion of a \"baseline\" group and it's role for **model interpretation**, why \"number of categories minus one\" **indicator variables** are used to represent the original **categorical variable**, and the relationship between the **binary** and **categorical variables** that are relevant for this model specification. An example use-case would likely be helpful for illustration here._ \n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details> "
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9a42be4a",
   "metadata": {},
   "source": [
    "simple linear regression:  a dependent variable (Y) and a single independent variable (X)\n",
    "Y=0 +1* X+\n",
    "while multiple linear regression :  a dependent variable (Y) and two or more independent variables (X1, X2, ... Xn).Y=0+1 * x1+2* X2+...+  +n* Xn + \n",
    "\n",
    "\n",
    "continous variable: numeric variable can take any value on range, form with linear regression line with a impact of changes from predictor to the dependent variable \n",
    "indicator variale(dummy variable): categorial variable with only values of 1 and 0 (binary variable)\n",
    "In Simple Linear Regression, using an indicator variable models a shift in the intercept depending on the category, showing a discrete shifter rather than a continous relationship.\n",
    "\n",
    "\n",
    "3.( it means that both the indicator variable and the continuous variable are included as independent variables in the regression model.)\n",
    "indicator variable is introduced: \n",
    "single linear line cannot get both variables: Simple Linear Regression with a continuous variable will model a straight-line relationship with no shift depending on categorical distinctions.\n",
    "mulitlple linear regression:\n",
    "Y= 0 +1X1+ 2 I(indicator) +\n",
    "The continuous predictor (X) shows the linear relationship between the continuous variable and the outcome.\n",
    "The indicator variable shows a shift of the intrcepty for different categories, modeling how the outcome differs for these categories (e.g., Male vs. Female).\n",
    "\n",
    "\n",
    "4  this interaction will affect  an interaction term models how the relationship between Y and X changes depending on whether another varibale D is 0 or 1.\n",
    "![image.png](attachment:image.png)\n",
    "here coutinous variable x is not only have a liner relationship with Y with a coefficient beta, but also with the indicator variable D.\n",
    "the form depends on effect of the continuous variable (e.g., age) on the dependent variable differs for different groups\n",
    "\n",
    "\n",
    "\n",
    "5. \n",
    "so for more categories, we have to not only include the indicator variables but also changes the meaning of Beta, B0 should be the value of a reference category, and the other beat, should not only with a simple continuous variable coefficient, but it should be a difference between the other categroies and the reference categroy \n",
    "for instance:\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f42ac8",
   "metadata": {},
   "source": [
    "Heres a summary of our conversation:\n",
    "\n",
    "1. **Simple vs. Multiple Linear Regression**:\n",
    "   - **Simple Linear Regression** involves one independent variable and one dependent variable, modeled as \\( Y = \\beta_0 + \\beta_1 X + \\epsilon \\).\n",
    "   - **Multiple Linear Regression** involves multiple independent variables and one dependent variable, modeled as \\( Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n + \\epsilon \\).\n",
    "\n",
    "2. **Using a Dummy Variable in Linear Regression**:\n",
    "   - **Dummy variables** (also called indicator variables) are used to convert categorical variables into numeric form for regression models.\n",
    "   - In **Multiple Linear Regression**, dummy variables allow us to incorporate non-binary categorical variables by creating multiple dummy variables. \n",
    "   - You can also use a **dummy variable in Simple Linear Regression** (if only one categorical independent variable is used), where the categorical variable is represented by a binary (0 or 1) dummy variable.\n",
    "\n",
    "3. **Single vs. Multiple Linear Regression with Categorical Variables**:\n",
    "   - When a **single indicator variable** is introduced in a regression, it still qualifies as **simple linear regression** if there is only one independent variable (the dummy). However, if there are more than one independent variable, it becomes **multiple linear regression**.\n",
    "   - **Multiple Linear Regression** allows the inclusion of multiple independent variables, which may be continuous or categorical (through dummy variables).\n",
    "\n",
    "4. **Interaction between Continuous and Indicator Variables**:\n",
    "   - An **interaction term** between a continuous variable and a dummy variable in **Multiple Linear Regression** is used to capture how the effect of the continuous variable depends on the value of the categorical variable.\n",
    "\n",
    "5. **Use of Dummy Variables in Multiple Linear Regression**:\n",
    "   - When a non-binary categorical variable with more than two categories is used in **Multiple Linear Regression**, dummy variables are created for all but one of the categories (the reference category), and the regression model predicts the relationship between the dependent variable and each category compared to the reference category.\n",
    "\n",
    "6. **Key Takeaway**:\n",
    "   - Dummy variables can be used in both **simple** and **multiple** linear regression models, but their role and how many are used depends on the number of categories in the categorical variable. Simple linear regression uses one dummy variable (if there's only one categorical predictor), while multiple regression can involve multiple dummy variables.\n",
    "\n",
    "In summary, we clarified how **dummy variables** are used in both simple and multiple linear regression, and how their introduction in the model allows for handling categorical data.\n",
    "\n",
    "\n",
    "https://chatgpt.com/share/67356ac9-15ec-8003-ac05-8b3d8c574875"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc155a4",
   "metadata": {},
   "source": [
    "### 2. Explain in your own words (but working with a ChatBot if needed) what the specific (outcome and predictor) variables are for the scenario below; whether or not any meaningful interactions might need to be taken into account when predicting the outcome; and provide the linear forms with and without the potential interactions that might need to be considered<br>\n",
    "\n",
    "> Imagine a company that sells sports equipment. The company runs advertising campaigns on TV and online platforms. The effectiveness of the TV ad might depend on the amount spent on online advertising and vice versa, leading to an interaction effect between the two advertising mediums.    \n",
    "\n",
    "1. Explain how to use these two formulas to make **predictions** of the **outcome**, and give a high level explaination in general terms of the difference between **predictions** from the models with and without the **interaction** \n",
    "\n",
    "2. Explain how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables)    \n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _While working on this question, it's important to clearly understand the (**outcome** and **predictor**) **variables** under consideration, and they way they are being considered. Similarly to the previous (first) question of this homework assignment, this question requires the **equations** of the indicated **linear forms** and an explanation of their **interpretation** and use. What is different here is that the **interactions** being considered are between two **continuous variables** or two **binary variables** (for a total of four **equations** under consideration where two include the **interactions** and two do not)._\n",
    ">\n",
    "> _The way an **interaction** actually appears and works in the context of the linear form of a **multiple linear regression** model may not necessarily be immediately intuitive, as it is actually in fact somewhat subtle and tricky. Of course, an **interaction** is when the relationship of one **predictor variable** with the **outcome variable** depends on the value of another different **predictor variable**, so the impact of one **predictor variable** changes based on the presence or magnitude of another **predictor variable**. But are you sure you know what this means in the context of the **linear form** of a **multiple linear regression** model?_\n",
    ">\n",
    "> - _Imagine you're selling smoothies, the taste of your smoothie depends on the ingredients included in the smoothie, and there are two ingredients (bananas and strawberries) in the smoothie._\n",
    ">     - _Adding more bananas into the smoothie will of course increase the \"banana\" flavor of the smoothie, and vice-versa for strawberries..._\n",
    ">     - _But is this \"banana\" influence on the flavor always constant, or can it change depending on the absolute amount of strawberries in the smoothie?_ \n",
    ">     - _If the \"banana\" flavor influence is constant and does not depend on the  absolute amount of strawberries in the smoothie, then there is no **interaction** and the **linear form** of the model is $\\beta_b b_i + \\beta_s s_i$ and the model is said to be only **additive**._\n",
    ">     - _But if the \"banana\" flavor influence does depend on the absolute amount of strawberries in the smoothie, then there IS an **interaction** and the **linear form** of the model is $\\beta_b b_i + \\beta_s s_i + \\beta_{bs} (b_i\\times s_i)$ and the model is said to be **synergistic**._\n",
    ">         \n",
    "> _These **linear forms** show that either bananas and strawberries do not have any **synergistic interaction** and contribute to the flavor independently; or, they do have a **synergistic interaction** and there is an interesting interplay between bananas and strawberries in the way they influence the taste of the smoothie._ \n",
    "> \n",
    "> - _So, if there is no **interaction**, then the effect of adding more bananas on the taste of the smoothie will always be the same, no matter how many strawberries you put in. So the effect of bananas on the smoothie is the same whether you add a lot of strawberries or just a few: $\\beta_b b_i + \\beta_s s_i$_\n",
    "> - _Or, on the other hand, if there is an **interaction**, then the effect of adding bananas (on the smootie flavor) will be different depending on how many strawberries there currently are in the smoothie: $\\beta_b b_i + \\beta_s s_i + \\beta_{bs} (b_i\\times s_i)$_\n",
    "> \n",
    "> _In this case, the right answer is probably that the **linear form** with the **interaction** is correct. This is because the flavor probably depends on the relative amount of bananas and strawberries in the smoothie; so, the effect of adding a fixed amount of bananas to the smoothie probalby depends on the absolute amount of strawberries that are in the smoothie._\n",
    "> \n",
    "> _Again, because understanding **interactions** in the context of **linear forms** is somewhat subtle and tricky and indeed not necessarily obviously intuitive, let's think about this a bit more. And we can simplify the concept a little bit by considering how this **interaction** would actually technically work in a **linear form** if we just had **binary indicator variables**._\n",
    ">         \n",
    "> - _To consider the smootie example in terms of binary variables, suppose that if both fruits are added to the smootie, they will be added in the same amount. So the smoothie will be made with either just bananas, just strawberries, or both (or neither and you won't make a smoothie)._ \n",
    ">     - _The question regarding an **interaction** then is, is the influence of the ingredients on the taste of the smoothie **additive** or **synergistic**? That is, does the way bananas affects the flavor of the smoothie change depending on the inclusion or exclusion of strawberries in the smoothie?_\n",
    ">     - _**Additive** $\\beta_b 1_{[b_i=1]}(b_i) + \\beta_s 1_{[s_i=1]}(s_i)$ means there are three different flavors but they are explained by just two **parameters**: banana $\\beta_b$, strawberry $\\beta_s$, and banana-strawberry $\\beta_b+\\beta_s$_\n",
    ">     - _**Synergistic** $\\beta_b 1_{[b_i=1]}(b_i) + \\beta_s 1_{[s_i=1]}(s_i) + \\beta_{bs}(1_{[b_i=1]}(b_i) \\times 1_{[s_i=1]}(s_i))$ means there are of course again three different flavors, but this time they are explained by three **parameters**: banana $\\beta_b$, strawberry $\\beta_s$, and banana-strawberry $\\beta_b+\\beta_s + \\beta_{bs}$, which indicates that the flavor is \"more than just sum of its parts\", meaning there is a **synergistic interaction** and there is an interesting interplay between bananas and strawberries in the way they influence the taste of the smoothie_\n",
    ">     \n",
    "> _As the **additive** and **synergistic** versions of the **linear form** of the two **binary indicator variables** context shows, we don't need an interaction to make different predictions for different combinations of things. Instead, what these show is that the prediction will either be **additive** and \"just the sum of it's parts\" or **synergistic** (**interactive**) and \"more than just sum of its parts\"._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "      \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c183a",
   "metadata": {},
   "source": [
    "Here, using continous variables, I will creating 2 formuals, with both \n",
    "Y= 0 + 1* X 1 + 2 * X 2 +\n",
    "\n",
    "where:\n",
    "Y is the outcome (effectiveness of the ads),\n",
    "X1 is the TV ad budget,\n",
    "X2 is the online ad budget,\n",
    "0 is the intercept,\n",
    "1is the coefficient for TV ad budget,\n",
    "2is the coefficient for online ad budget,\n",
    " is the error term.\n",
    "\n",
    "\n",
    "Y=0+1X1+2X2+3(X1* X2)+e\n",
    "where:\n",
    "B3 is the coefficient for the interaction term, capturing the interaction effect between TV and online ad budgets.\n",
    "\n",
    "\n",
    "Here, by using categorial variables: \n",
    "Y^=0^+1^X1+2^X2\n",
    "Y^=o^+1^X1+2^X2+3^(X1* X2)\n",
    "Here, X1 is the 0 or 1 of TV budget\n",
    "X2 is the 0 or 1 of online budget \n",
    "\n",
    "\n",
    "\n",
    "big picture:  intercation is essential \n",
    "for the non-interaction variables, it assumes that the effect of each budget is independent and constant \n",
    "while for the interaction one, the effect of one advertising medium (e.g., TV) depends on whether the other (e.g., online) is high or low, this interaction usually has a greater effect as the both high or low budegt will the product of their interaction will have a greater or higher combination of the whole budget, resulting in a greater effect in Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd4132",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/67357631-1a5c-8003-b8df-a66695827fb2\n",
    "    Heres a summary of our conversation:\n",
    "\n",
    "1. **Interaction Effects in Models**: We discussed how the effect of a continuous variable (e.g., advertising budget) on an outcome (e.g., sales) can differ when there is an interaction with a binary variable (e.g., \"high\" vs \"low\" marketing strategy). \n",
    "\n",
    "2. **Additive vs Synergistic Models**: \n",
    "   - **Additive Model**: Assumes the effects of variables (e.g., banana flavor and strawberries in a smoothie) are independent, meaning their contributions to the outcome are simply added together.\n",
    "   - **Synergistic Model**: Involves an **interaction** where the effect of one variable (e.g., banana flavor) depends on the level of another variable (e.g., strawberries). This creates a combined effect that is not just the sum of their individual effects.\n",
    "\n",
    "3. **Why Not Combine Continuous and Binary Variables**: \n",
    "   - Continuous and binary variables represent different kinds of relationships and should generally be treated separately in a model. \n",
    "   - If there is an **interaction** between them (e.g., advertising budget and marketing strategy), you would use an **interaction term** (e.g., \\( X_1 \\times X_2 \\)) to model the combined effect.\n",
    "   - The **incremental** change of a continuous variable means that its effect on the outcome changes gradually as the variable's value changes (small, continuous steps), unlike the discrete, step-like effect of binary variables.\n",
    "\n",
    "This explains the key differences in how continuous and binary variables interact with each other in statistical models, particularly when considering whether the effect of one variable changes depending on the value of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097af67",
   "metadata": {},
   "source": [
    "### 3. Use *smf* to fit *multiple linear regression* models to the course project dataset from the canadian social connection survey<br>\n",
    "\n",
    "> **EDIT: No, you probably actually care about CATEGORICAL or BINARY outcomes rather than CONTINUOUS outcomes... so you'll probably not actually want to do _multiple linear regression_ and instead do _logistic regression_ or _multi-class classification_. Okay, I'll INSTEAD guide you through doing _logistic regression_.**\n",
    "\n",
    "1. ~~for an **additive** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables** and a **CONTINUOUS OUTCOME varaible**~~ \n",
    "    1. This would have been easy to do following the instructions [here](https://www.statsmodels.org/dev/example_formulas.html). A good alternative analagous presentation for logistic regression I just found seems to be this one from a guy named [Andrew](https://www.andrewvillazon.com/logistic-regression-python-statsmodels/). He walks you through the `logit` alternative to `OLS` given [here](https://www.statsmodels.org/dev/api.html#discrete-and-count-models).\n",
    "    2. Logistic is for a **binary outcome** so go see this [piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1) describing how you can turn any **non-binary categorical variable** into a **binary variable**. \n",
    "    3. Then instead do this problem like this: **catogorical outcome** turned into a **binary outcome** for **logistic regression** and then use any **additive** combination of a couple of **continuous**, **binary**, and/or **categorical variables** as **predictor variables**. \n",
    "\n",
    "\n",
    "```python\n",
    "# Here's an example of how you can do this\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()\n",
    "```\n",
    "\n",
    "\n",
    "2. ~~for a **synertistic interaction** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables**~~\n",
    "    1. But go ahead and AGAIN do this for **logistic regression** like above.\n",
    "    2. Things are going to be A LOT simpler if you restrict yourself to **continuous** and/or **binary predictor variables**.  But of course you could *use the same trick again* to treat any **categorical variable** as just a **binary variable** (in the manner of [that piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1).\n",
    "    \n",
    "\n",
    "3. and **interpretively explain** your **linear forms** and how to use them to make **predictions**\n",
    "    1. Look, intereting **logistic regression** *IS NOT* as simple as interpreting **multivariate linear regression**. This is because it requires you to understand so-called **log odds** and that's a bit tricky. \n",
    "    2. So, INSTEAD, **just intepret you logistic regression models** *AS IF* they were **multivariate linear regression model predictions**, okay?\n",
    "\n",
    "\n",
    "4. and interpret the statistical evidence associated with the **predictor variables** for each of your model specifications \n",
    "    1. **Yeah, you're going to be able to do this based on the `.fit().summary()` table _just like with multiple linear regression_**... now you might be starting to see how AWESOME all of this stuff we're doing is going to be able to get...\n",
    "\n",
    "\n",
    "5. and finally use `plotly` to visualize the data with corresponding \"best fit lines\" for a model with **continuous** plus **binary indicator** specification under both (a) **additive** and (b) **synergistic** specifications of the **linear form** (on separate figures), commenting on the apparent necessity (or lack thereof) of the **interaction** term for the data in question\n",
    "    1. Aw, shit, you DEF not going to be able to do this if you're doing **logistic regression** because of that **log odds** thing I mentioned... hmm...\n",
    "    2. OKAY! Just *pretend* it's **multivariate linear regression** (even if you're doing **logistic regression**) and *pretend* your **fitted coefficients** belong to a **continuous** and a **binary predictor variable**; then, draw the lines as requested, and simulate **random noise** for the values of your **predictor data** and plot your lines along with that data.\n",
    "    \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _This [link](https://www.statsmodels.org/dev/examples/notebooks/generated/formulas.html) offers guidance on using `statsmodels.formula.api` (`smf`) to build statistical models in Python using formulas._\n",
    ">\n",
    "> _The \"best fit lines\" summarize the relationship between the **outcome** and **predictor variables** observed in the data as well as the **linear form** of the **multiple linear regression** allows. The statistical evidence for the these estimated realtionship characterizations of course depends on an evaluation of the **hypothesis testing** for the **coefficients** of the model. **Model building** is the process of exploring the evidence for observed relationships captured through the modeling of the data in order to arrive at reliable (**generalizable**) claims based on the data, and perhaps make predictions about the future based on these created beliefs and understandings (whose value of course depends on how trustworthy these created beliefs and understandings are)._\n",
    ">\n",
    "> _When we do not find sufficient sufficient evidence for supposed relationships that we'd like to leverage for understanding or prediction, attempting to move forward on the basis of such \"findings\" is certainly a dangerous errand..._\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c2fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>str8fyre</td>     <th>  No. Observations:  </th>  <td>   800</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   788</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 15 Nov 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.05156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>04:22:26</td>     <th>  Log-Likelihood:    </th> <td> -182.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -192.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.04757</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                <td>   -3.2644</td> <td>    0.714</td> <td>   -4.572</td> <td> 0.000</td> <td>   -4.664</td> <td>   -1.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                        <td>    4.3478</td> <td>    2.179</td> <td>    1.996</td> <td> 0.046</td> <td>    0.078</td> <td>    8.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 2\") == \"None\")[T.True]</th>         <td>    1.5432</td> <td>    0.853</td> <td>    1.810</td> <td> 0.070</td> <td>   -0.128</td> <td>    3.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>                       <td>   -0.0574</td> <td>    0.468</td> <td>   -0.123</td> <td> 0.902</td> <td>   -0.975</td> <td>    0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>                       <td>   -0.6480</td> <td>    0.466</td> <td>   -1.390</td> <td> 0.164</td> <td>   -1.561</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>                       <td>   -0.8255</td> <td>    0.545</td> <td>   -1.516</td> <td> 0.130</td> <td>   -1.893</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>                       <td>   -0.5375</td> <td>    0.449</td> <td>   -1.198</td> <td> 0.231</td> <td>   -1.417</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>                       <td>    0.3213</td> <td>    0.477</td> <td>    0.673</td> <td> 0.501</td> <td>   -0.614</td> <td>    1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                   <td>    0.0172</td> <td>    0.006</td> <td>    3.086</td> <td> 0.002</td> <td>    0.006</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                 <td>   -0.0365</td> <td>    0.019</td> <td>   -1.884</td> <td> 0.060</td> <td>   -0.074</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                  <td>   -0.0098</td> <td>    0.008</td> <td>   -1.247</td> <td> 0.213</td> <td>   -0.025</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:I(Q(\"Type 2\") == \"None\")[T.True]</th> <td>   -0.0197</td> <td>    0.012</td> <td>   -1.651</td> <td> 0.099</td> <td>   -0.043</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                           &     str8fyre     & \\textbf{  No. Observations:  } &      800    \\\\\n",
       "\\textbf{Model:}                                   &      Logit       & \\textbf{  Df Residuals:      } &      788    \\\\\n",
       "\\textbf{Method:}                                  &       MLE        & \\textbf{  Df Model:          } &       11    \\\\\n",
       "\\textbf{Date:}                                    & Fri, 15 Nov 2024 & \\textbf{  Pseudo R-squ.:     } &  0.05156    \\\\\n",
       "\\textbf{Time:}                                    &     04:22:26     & \\textbf{  Log-Likelihood:    } &   -182.49   \\\\\n",
       "\\textbf{converged:}                               &       True       & \\textbf{  LL-Null:           } &   -192.41   \\\\\n",
       "\\textbf{Covariance Type:}                         &    nonrobust     & \\textbf{  LLR p-value:       } &  0.04757    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                  & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                &      -3.2644  &        0.714     &    -4.572  &         0.000        &       -4.664    &       -1.865     \\\\\n",
       "\\textbf{Legendary[T.True]}                        &       4.3478  &        2.179     &     1.996  &         0.046        &        0.078    &        8.618     \\\\\n",
       "\\textbf{I(Q(\"Type 2\") == \"None\")[T.True]}         &       1.5432  &        0.853     &     1.810  &         0.070        &       -0.128    &        3.215     \\\\\n",
       "\\textbf{C(Generation)[T.2]}                       &      -0.0574  &        0.468     &    -0.123  &         0.902        &       -0.975    &        0.861     \\\\\n",
       "\\textbf{C(Generation)[T.3]}                       &      -0.6480  &        0.466     &    -1.390  &         0.164        &       -1.561    &        0.265     \\\\\n",
       "\\textbf{C(Generation)[T.4]}                       &      -0.8255  &        0.545     &    -1.516  &         0.130        &       -1.893    &        0.242     \\\\\n",
       "\\textbf{C(Generation)[T.5]}                       &      -0.5375  &        0.449     &    -1.198  &         0.231        &       -1.417    &        0.342     \\\\\n",
       "\\textbf{C(Generation)[T.6]}                       &       0.3213  &        0.477     &     0.673  &         0.501        &       -0.614    &        1.257     \\\\\n",
       "\\textbf{Attack}                                   &       0.0172  &        0.006     &     3.086  &         0.002        &        0.006    &        0.028     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                 &      -0.0365  &        0.019     &    -1.884  &         0.060        &       -0.074    &        0.001     \\\\\n",
       "\\textbf{Defense}                                  &      -0.0098  &        0.008     &    -1.247  &         0.213        &       -0.025    &        0.006     \\\\\n",
       "\\textbf{Defense:I(Q(\"Type 2\") == \"None\")[T.True]} &      -0.0197  &        0.012     &    -1.651  &         0.099        &       -0.043    &        0.004     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               str8fyre   No. Observations:                  800\n",
       "Model:                          Logit   Df Residuals:                      788\n",
       "Method:                           MLE   Df Model:                           11\n",
       "Date:                Fri, 15 Nov 2024   Pseudo R-squ.:                 0.05156\n",
       "Time:                        04:22:26   Log-Likelihood:                -182.49\n",
       "converged:                       True   LL-Null:                       -192.41\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
       "============================================================================================================\n",
       "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
       "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
       "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
       "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
       "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
       "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
       "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
       "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
       "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
       "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
       "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
       "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
       "============================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51bb39",
   "metadata": {},
   "source": [
    "By using the fitted model here, we are able to calculate each of the probaility of the binary value. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "the summary table:\n",
    "Coef. (Coefficient):\n",
    "This represents the estimated effect of each predictor variable on the log-odds of the outcome. log odds is \n",
    "\n",
    "Std Err. (Standard Error):\n",
    "This is the standard error of the coefficient estimate.Smaller standard errors suggest more precise estimates.\n",
    "\n",
    "z (Z-statistic): used for null hypothesis \n",
    "The z-statistic is the coefficient divided by its standard error. Its used to test whether the coefficient is significantly different from 0. The null hypothesis is that the coefficient is 0 (no effect).\n",
    "\n",
    "P>|z| (P-value):\n",
    "The p-value tests the null hypothesis that the coefficient is zero (no effect).\n",
    "If the p-value is less than 0.05, the predictor variable is considered statistically significant, meaning it has a significant impact on the outcome.\n",
    "\n",
    "[0.025 0.975] (Confidence Interval):\n",
    "This shows the 95% confidence interval for the coefficient. It gives a range where you are 95% confident that the true coefficient lies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6268a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1
          ],
          "colorscale": [
           [
            0,
            "#440154"
           ],
           [
            0.1111111111111111,
            "#482878"
           ],
           [
            0.2222222222222222,
            "#3e4989"
           ],
           [
            0.3333333333333333,
            "#31688e"
           ],
           [
            0.4444444444444444,
            "#26828e"
           ],
           [
            0.5555555555555556,
            "#1f9e89"
           ],
           [
            0.6666666666666666,
            "#35b779"
           ],
           [
            0.7777777777777778,
            "#6ece58"
           ],
           [
            0.8888888888888888,
            "#b5de2b"
           ],
           [
            1,
            "#fde725"
           ]
          ],
          "size": 8
         },
         "mode": "markers",
         "name": "Predicted Probabilities",
         "type": "scatter",
         "x": [
          49,
          62,
          82,
          100,
          52,
          64,
          84,
          130,
          104,
          48,
          63,
          83,
          103,
          30,
          20,
          45,
          35,
          25,
          90,
          150,
          45,
          60,
          80,
          80,
          56,
          81,
          60,
          90,
          60,
          85,
          55,
          90,
          75,
          100,
          47,
          62,
          92,
          57,
          72,
          102,
          45,
          70,
          41,
          76,
          45,
          70,
          45,
          80,
          50,
          65,
          80,
          70,
          95,
          55,
          65,
          55,
          80,
          45,
          70,
          52,
          82,
          80,
          105,
          70,
          110,
          50,
          65,
          95,
          20,
          35,
          50,
          50,
          80,
          100,
          130,
          75,
          90,
          105,
          40,
          70,
          80,
          95,
          120,
          85,
          100,
          65,
          75,
          75,
          35,
          60,
          65,
          85,
          110,
          45,
          70,
          80,
          105,
          65,
          95,
          35,
          50,
          65,
          65,
          45,
          48,
          73,
          105,
          130,
          30,
          50,
          40,
          95,
          50,
          80,
          120,
          105,
          55,
          65,
          90,
          85,
          130,
          5,
          55,
          95,
          125,
          40,
          65,
          67,
          92,
          45,
          75,
          45,
          110,
          50,
          83,
          95,
          125,
          155,
          100,
          10,
          125,
          155,
          85,
          48,
          55,
          65,
          65,
          130,
          60,
          40,
          60,
          80,
          115,
          105,
          135,
          110,
          85,
          90,
          100,
          64,
          84,
          134,
          110,
          190,
          150,
          100,
          49,
          62,
          82,
          52,
          64,
          84,
          65,
          80,
          105,
          46,
          76,
          30,
          50,
          20,
          35,
          60,
          90,
          90,
          38,
          58,
          40,
          25,
          30,
          20,
          40,
          50,
          75,
          40,
          55,
          75,
          95,
          80,
          20,
          50,
          100,
          75,
          35,
          45,
          55,
          70,
          30,
          75,
          65,
          45,
          85,
          65,
          65,
          85,
          75,
          60,
          72,
          33,
          80,
          65,
          90,
          70,
          75,
          85,
          125,
          80,
          120,
          95,
          130,
          150,
          10,
          125,
          185,
          95,
          80,
          130,
          40,
          50,
          50,
          100,
          55,
          65,
          105,
          55,
          40,
          80,
          60,
          90,
          90,
          95,
          60,
          120,
          80,
          95,
          20,
          35,
          95,
          30,
          63,
          75,
          80,
          10,
          85,
          115,
          75,
          64,
          84,
          134,
          164,
          90,
          130,
          100,
          45,
          65,
          85,
          110,
          60,
          85,
          120,
          160,
          70,
          85,
          110,
          150,
          55,
          90,
          30,
          70,
          45,
          35,
          70,
          35,
          50,
          30,
          50,
          70,
          40,
          70,
          100,
          55,
          85,
          30,
          50,
          25,
          35,
          65,
          85,
          30,
          60,
          40,
          130,
          60,
          80,
          160,
          45,
          90,
          90,
          51,
          71,
          91,
          60,
          120,
          20,
          45,
          45,
          65,
          75,
          85,
          85,
          105,
          70,
          90,
          110,
          140,
          40,
          60,
          100,
          45,
          75,
          75,
          50,
          40,
          73,
          47,
          60,
          43,
          73,
          90,
          120,
          140,
          70,
          90,
          60,
          100,
          120,
          85,
          25,
          45,
          60,
          100,
          70,
          100,
          85,
          115,
          40,
          70,
          110,
          115,
          100,
          55,
          95,
          48,
          78,
          80,
          120,
          40,
          70,
          41,
          81,
          95,
          125,
          15,
          60,
          70,
          90,
          75,
          115,
          165,
          40,
          70,
          68,
          50,
          130,
          150,
          23,
          50,
          80,
          120,
          40,
          60,
          80,
          64,
          104,
          84,
          90,
          30,
          75,
          95,
          135,
          145,
          55,
          75,
          135,
          145,
          100,
          50,
          75,
          80,
          100,
          90,
          130,
          100,
          150,
          150,
          180,
          150,
          180,
          100,
          150,
          180,
          70,
          95,
          68,
          89,
          109,
          58,
          78,
          104,
          51,
          66,
          86,
          55,
          75,
          120,
          45,
          85,
          25,
          85,
          65,
          85,
          120,
          30,
          70,
          125,
          165,
          42,
          52,
          29,
          59,
          79,
          69,
          94,
          30,
          80,
          45,
          65,
          105,
          35,
          60,
          48,
          83,
          100,
          50,
          80,
          66,
          76,
          136,
          60,
          125,
          55,
          82,
          30,
          63,
          93,
          24,
          89,
          80,
          25,
          5,
          65,
          92,
          70,
          90,
          130,
          170,
          85,
          70,
          110,
          145,
          72,
          112,
          50,
          90,
          61,
          106,
          100,
          49,
          69,
          20,
          62,
          92,
          132,
          120,
          70,
          85,
          140,
          100,
          123,
          95,
          50,
          76,
          110,
          60,
          95,
          130,
          80,
          125,
          165,
          55,
          100,
          80,
          50,
          65,
          65,
          65,
          65,
          65,
          75,
          105,
          125,
          120,
          120,
          90,
          160,
          100,
          120,
          70,
          80,
          100,
          90,
          100,
          103,
          120,
          100,
          45,
          60,
          75,
          63,
          93,
          123,
          55,
          75,
          100,
          55,
          85,
          60,
          80,
          110,
          50,
          88,
          53,
          98,
          53,
          98,
          53,
          98,
          25,
          55,
          55,
          77,
          115,
          60,
          100,
          75,
          105,
          135,
          45,
          57,
          85,
          135,
          60,
          60,
          80,
          105,
          140,
          50,
          65,
          95,
          100,
          125,
          53,
          63,
          103,
          45,
          55,
          100,
          27,
          67,
          35,
          60,
          92,
          72,
          82,
          117,
          90,
          140,
          30,
          86,
          65,
          95,
          75,
          90,
          58,
          30,
          50,
          78,
          108,
          112,
          140,
          50,
          95,
          65,
          105,
          50,
          95,
          30,
          45,
          55,
          30,
          40,
          65,
          44,
          87,
          50,
          65,
          95,
          60,
          100,
          75,
          75,
          135,
          55,
          85,
          40,
          60,
          75,
          47,
          77,
          50,
          94,
          55,
          80,
          100,
          55,
          85,
          115,
          55,
          75,
          30,
          40,
          55,
          87,
          117,
          147,
          70,
          110,
          50,
          40,
          70,
          66,
          85,
          125,
          120,
          74,
          124,
          85,
          125,
          110,
          83,
          123,
          55,
          65,
          97,
          109,
          65,
          85,
          105,
          85,
          60,
          90,
          129,
          90,
          115,
          100,
          115,
          105,
          120,
          150,
          125,
          145,
          130,
          170,
          120,
          72,
          72,
          77,
          128,
          120,
          61,
          78,
          107,
          45,
          59,
          69,
          56,
          63,
          95,
          36,
          56,
          50,
          73,
          81,
          35,
          22,
          52,
          50,
          68,
          38,
          45,
          65,
          65,
          100,
          82,
          124,
          80,
          48,
          48,
          48,
          80,
          110,
          150,
          50,
          52,
          72,
          48,
          80,
          54,
          92,
          52,
          105,
          60,
          75,
          53,
          73,
          38,
          55,
          89,
          121,
          59,
          77,
          65,
          92,
          58,
          50,
          50,
          75,
          100,
          80,
          70,
          110,
          66,
          66,
          66,
          66,
          90,
          85,
          95,
          100,
          69,
          117,
          30,
          70,
          131,
          131,
          100,
          100,
          160,
          110,
          160,
          110
         ],
         "y": [
          0.051870473179323924,
          0.0562301500356128,
          0.06452240255249116,
          0.059595518237456443,
          0.10921569516568261,
          0.08819827987208945,
          0.06974961890010782,
          0.10659853335945497,
          0.0955738197415481,
          0.056409578379128064,
          0.04730675941190599,
          0.037323707104900995,
          0.029382384994346398,
          0.09621588215197491,
          0.04732861630052293,
          0.048144460101753495,
          0.049311848481598815,
          0.03464484042083051,
          0.10776618489458129,
          0.25268481505313284,
          0.05285852656686191,
          0.05863268860969511,
          0.06725399523897199,
          0.06423324229943546,
          0.14259352530836222,
          0.10878377093351249,
          0.07377504356372007,
          0.08629716011335885,
          0.12014572151371682,
          0.09109299585472368,
          0.12360583408838692,
          0.14170934673517535,
          0.050001712140962835,
          0.0371936643882512,
          0.079414602415652,
          0.0668625145656812,
          0.07297755953456761,
          0.12737124532234215,
          0.10256447730390458,
          0.09347894888303555,
          0.08576130672392186,
          0.06441444581781801,
          0.09984785249106373,
          0.0671101688592015,
          0.06362116646247476,
          0.07543212351598053,
          0.055376006342985125,
          0.07040611935612995,
          0.04984954062002233,
          0.05531420315121147,
          0.061339245746797985,
          0.06885122769923148,
          0.08154856188258308,
          0.056645094911320436,
          0.06068553761796319,
          0.18008427745087108,
          0.13882452937898687,
          0.12103758317971619,
          0.09179169359219992,
          0.09565830651640869,
          0.06801440523154229,
          0.20066457544995583,
          0.15558434691725545,
          0.13598913424780898,
          0.10008678911549554,
          0.11460897958951419,
          0.0740977040593947,
          0.07115660592753953,
          0.13931664363419663,
          0.11851547849269839,
          0.10045760192271261,
          0.05826407444667093,
          0.13882452937898687,
          0.11179494800966176,
          0.13550692335128253,
          0.0893229026433946,
          0.09866541464238301,
          0.10886826823713752,
          0.05105605550203429,
          0.06280546575762443,
          0.05337296090864082,
          0.05919980630954484,
          0.07695521816313397,
          0.13159265307892584,
          0.11179494800966176,
          0.057941484029988256,
          0.04479781373813964,
          0.023013135558839274,
          0.033812323439761975,
          0.04032774643072385,
          0.06355078022564949,
          0.09545836118703475,
          0.1124698280315702,
          0.070881301393685,
          0.05465984776330975,
          0.13882452937898687,
          0.10579815539747389,
          0.027681622255203958,
          0.03213123234417902,
          0.049311848481598815,
          0.05472096218640811,
          0.06068553761796319,
          0.05039278493188995,
          0.016850802428263294,
          0.09739901977961364,
          0.07338794187974378,
          0.07061109198995269,
          0.05281741278997153,
          0.06398770345571217,
          0.05067173351024046,
          0.03340333670311388,
          0.07793940905861907,
          0.024876939927353025,
          0.026678565875096944,
          0.22663756083361328,
          0.09513264341056984,
          0.047777286596981765,
          0.03194518138728216,
          0.02364722309978597,
          0.06061818891481711,
          0.09845536234010355,
          0.14392841169203668,
          0.015165862986623167,
          0.07917454376802902,
          0.07381620741652305,
          0.04302662550586858,
          0.03194518138728216,
          0.08758997202922922,
          0.11283092844997043,
          0.070881301393685,
          0.05658196017483262,
          0.04181547655412865,
          0.10301770186218005,
          0.060038350620542025,
          0.12128445332950208,
          0.14499121903418713,
          0.07381620741652305,
          0.14361533538361398,
          0.05674483185497528,
          0.04016656063467179,
          0.13045491821679245,
          0.15744461333352608,
          0.06958638075876422,
          0.08988394827350045,
          0.09500390749234944,
          0.08488619399000018,
          0.08488619399000018,
          0.22053837064925386,
          0.05959066076716101,
          0.027601890286133315,
          0.03033391468713402,
          0.0585674761030412,
          0.0891307731119873,
          0.10886826823713752,
          0.14376078453869331,
          0.14762906039734883,
          0.1762480486113337,
          0.18377507257021367,
          0.15016593961189845,
          0.12434062742262586,
          0.0998042825353299,
          0.1301159600616955,
          0.1038206171270304,
          0.02738792833708632,
          0.08807068110353662,
          0.04934008985593955,
          0.054305143516034035,
          0.04405752529991949,
          0.034735152628004216,
          0.10375491995698168,
          0.0836896439137949,
          0.0665652361037985,
          0.07220781295931927,
          0.0590572209496348,
          0.05068843735642205,
          0.11990190912558267,
          0.08591756380827868,
          0.04301612442630243,
          0.04946183385936434,
          0.03648167097200262,
          0.03867266230176373,
          0.06381175329451398,
          0.07825453179637254,
          0.07144642956990946,
          0.04549207996828337,
          0.0522885251428027,
          0.17722800612111592,
          0.10188258298493393,
          0.049517440881453365,
          0.03373656957333932,
          0.030127706342904837,
          0.0518263922435799,
          0.06159173440657877,
          0.09334344742670495,
          0.07877819506699905,
          0.0473442730549452,
          0.06152344587912112,
          0.038742054167873796,
          0.030162266874830024,
          0.03729239677873011,
          0.03050851580786981,
          0.06259011189630395,
          0.042500707520645775,
          0.04558099233480385,
          0.04449779268209772,
          0.09959707647517824,
          0.10435424353971108,
          0.10755864887591944,
          0.06603321227879817,
          0.04776934795096591,
          0.06299377342201352,
          0.08053191571178347,
          0.019615711948172986,
          0.09307706661702816,
          0.056144985839731695,
          0.07440409473051283,
          0.1233924516125418,
          0.05092629935150408,
          0.06987028990722849,
          0.034856910684017556,
          0.04089680521632072,
          0.06631944979760646,
          0.04444755901520337,
          0.021227570558388174,
          0.031072954936106383,
          0.13210286342411884,
          0.1262590503676548,
          0.08093531543754523,
          0.11153512547623604,
          0.10664042108448803,
          0.004438869255329869,
          0.12842060033046365,
          0.2177056732483556,
          0.09683028133133766,
          0.13210286342411884,
          0.14643023787877926,
          0.09334344742670495,
          0.025469570331042437,
          0.054297532891429356,
          0.08369959606725073,
          0.0386287387861989,
          0.15486982234014343,
          0.10048898035517813,
          0.05620766204516892,
          0.034751845803471786,
          0.03467262709386443,
          0.06994715257313312,
          0.09367667819301771,
          0.06518873917299493,
          0.06745468079433349,
          0.07440409473051283,
          0.03685285965154846,
          0.044630959990167446,
          0.12137315660606464,
          0.07806226544116798,
          0.09871222008718898,
          0.04954958731575383,
          0.049517440881453365,
          0.14303856361723302,
          0.17017399792510124,
          0.029124697744320697,
          0.12984782288237898,
          0.21634331076312505,
          0.10323364298605688,
          0.09320219881423597,
          0.06205740337916958,
          0.0711442201839988,
          0.10862096570786064,
          0.120924302670877,
          0.12014242722382995,
          0.08548080343795633,
          0.06979350536499049,
          0.06719527168237419,
          0.07025706008066092,
          0.055714081766946304,
          0.05936231855131566,
          0.07440696118679894,
          0.04546607224086786,
          0.07295634159423983,
          0.12409208033476844,
          0.06632202710627258,
          0.04138247965686184,
          0.05163865776629049,
          0.08158975924621727,
          0.07878121569481625,
          0.05254755116153139,
          0.04456897062068404,
          0.04882603228147658,
          0.06719527168237419,
          0.03252275744947776,
          0.03904419957454759,
          0.03252275744947776,
          0.023132111090121806,
          0.02429810631443843,
          0.02802115850210112,
          0.032295789221156286,
          0.04072562592520388,
          0.04290738023347443,
          0.05803609796215426,
          0.03683296538084088,
          0.04546607224086786,
          0.02429810631443843,
          0.01732270057523507,
          0.02344543503991117,
          0.025180988202122095,
          0.031171428851162086,
          0.043378425009559865,
          0.023835984277098446,
          0.029516142958260383,
          0.030631557150789336,
          0.07806397207408473,
          0.04263643911708119,
          0.033604191512519685,
          0.07063661204546288,
          0.017538370246117083,
          0.05673771943648918,
          0.05673771943648918,
          0.1021741228051385,
          0.08160429614930415,
          0.06487637694922106,
          0.09747585340751368,
          0.11085337609104713,
          0.018656892896365684,
          0.0037451665858079888,
          0.05088859270537859,
          0.04018155323160608,
          0.033459304886803196,
          0.024513501498687536,
          0.035908879273376104,
          0.034205249579592735,
          0.024242118983801117,
          0.023078746879052942,
          0.021969947603565125,
          0.0011591384343730238,
          0.02259586999929652,
          0.02606506647028684,
          0.045963857792239264,
          0.05851142380781372,
          0.054468796017418664,
          0.03092707962018982,
          0.0634200290355794,
          0.04072562592520388,
          0.06060915294438334,
          0.03966291276789847,
          0.034702814094700815,
          0.03930002801886176,
          0.027448539613654985,
          0.07142781697480925,
          0.09560801986152842,
          0.09984037966605065,
          0.0996008090635004,
          0.10397458733812405,
          0.036388817664069845,
          0.05288606464831723,
          0.05534294582943419,
          0.0064008436701467036,
          0.048626732047206044,
          0.028847051210592417,
          0.04263643911708119,
          0.12107895391451112,
          0.03904419957454759,
          0.04816966716564266,
          0.10988071879223255,
          0.07381262944432673,
          0.021534653431827613,
          0.02668139043132475,
          0.04281039508421264,
          0.10267598384636932,
          0.08127160626886232,
          0.026386656986600414,
          0.042345245486266406,
          0.028977675897108212,
          0.03583812532785983,
          0.0513691914593447,
          0.06358351477286069,
          0.02259586999929652,
          0.023105413942746404,
          0.018589258729220997,
          0.029977198219791147,
          0.058728360862490045,
          0.06000268340893539,
          0.06283415317161434,
          0.02478224521265604,
          0.037862422732203825,
          0.05254755116153139,
          0.10756264419514595,
          0.08984851427351866,
          0.1476779930415968,
          0.012862449622658773,
          0.006646707887018933,
          0.02759347245147438,
          0.027163592259996882,
          0.12892651737748456,
          0.17259540279753194,
          0.03254863293575175,
          0.04798238456234363,
          0.033604191512519685,
          0.06460708703208608,
          0.023708115637899506,
          0.027343300525769717,
          0.031517877075793295,
          0.022289847936940575,
          0.024473400258871847,
          0.017489150549401353,
          0.02540400608561074,
          0.029929130933218397,
          0.054468796017418664,
          0.024312388431344444,
          0.08446570915722838,
          0.06276826444891771,
          0.022849254856176813,
          0.026356305056444197,
          0.05340075188802477,
          0.05214187357658875,
          0.002847300760632544,
          0.1256567528397001,
          0.019855367944312568,
          0.11973054103875547,
          0.06438126305875612,
          0.11009799994691538,
          0.04482915253397503,
          0.06847884132719896,
          0.02722641913633798,
          0.006353121961930724,
          0.00980540597040601,
          0.03399611538410161,
          0.01755507791808677,
          0.07729859567680272,
          0.08357115770999683,
          0.11023877434033122,
          0.016335355862495123,
          0.07490144147590404,
          0.03662037545533557,
          0.028479885063891302,
          0.03723014056484569,
          0.054638739880564766,
          0.036856845620579295,
          0.047245657424595695,
          0.03780819377493721,
          0.031606536704974084,
          0.029884806592491233,
          0.03102815086589091,
          0.03574462922231229,
          0.06182396510923579,
          0.04946558143677571,
          0.03835487418385169,
          0.03460908904225074,
          0.06950674808109636,
          0.08051053307178913,
          0.07342489205869758,
          0.0562200178403158,
          0.019465490981318678,
          0.028517431257430056,
          0.17034459604717822,
          0.1842979084540904,
          0.010664573816654496,
          0.007764296495317743,
          0.03299312117658866,
          0.01957389733852702,
          0.02258984664396102,
          0.02102903055494826,
          0.048847412938126326,
          0.01819369262894121,
          0.023643593530778113,
          0.0210077879507896,
          0.07835141289993867,
          0.08554633208200581,
          0.036440111898527985,
          0.02700714136535058,
          0.041466772735801745,
          0.03439711074578529,
          0.058421291950165176,
          0.027478807480882012,
          0.04108558619778404,
          0.062177473081203014,
          0.023587332013051184,
          0.06408939184640872,
          0.035951145726858195,
          0.0789440408029604,
          0.05503200587527252,
          0.04610444982134587,
          0.02907426074594365,
          0.03013917243185417,
          0.040958606294561245,
          0.010727882421295094,
          0.024031816659632874,
          0.01835421388063504,
          0.016244589250764472,
          0.06859356581818327,
          0.03175800319584795,
          0.027281706794517968,
          0.03450448056008485,
          0.03972771804012366,
          0.05764813131069022,
          0.09075126259296194,
          0.09368344562572292,
          0.07399910583112917,
          0.05258947181454367,
          0.0781464727999439,
          0.026220919943375467,
          0.016150461303045937,
          0.016025917915505943,
          0.02588495051532916,
          0.031165671154128587,
          0.05162932101815939,
          0.04940416874873126,
          0.03358251765084933,
          0.026415362023362355,
          0.014222698229928358,
          0.02880447147685569,
          0.03735451221211008,
          0.05426491206236836,
          0.0647392535271875,
          0.017632934100108456,
          0.01996541503169846,
          0.04894973085061288,
          0.010749222061457503,
          0.08205137126383823,
          0.052391752737565696,
          0.015268439475726469,
          0.025781985815895712,
          0.011006609977183555,
          0.008447067363049367,
          0.02437193162917433,
          0.06620788583536469,
          0.03764673565803267,
          0.07013140200522901,
          0.10033338312363434,
          0.010224564620875886,
          0.008022829889737326,
          0.03211075469957042,
          0.018172588963521173,
          0.01751066391430199,
          0.01751066391430199,
          0.01751066391430199,
          0.01751066391430199,
          0.01751066391430199,
          0.029709056949540354,
          0.03464609663780117,
          0.06416104836762267,
          0.03768207827282846,
          0.04550306509192179,
          0.07425939826331802,
          0.010590055831799953,
          0.05448045812691987,
          0.04550306509192179,
          0.007471370738964538,
          0.028293278844360087,
          0.022228983982389268,
          0.06948241548654446,
          0.04380995713890562,
          0.07805062105451488,
          0.01695733326506623,
          0.08555263209489919,
          0.04266640566292494,
          0.030949082421137048,
          0.022374438754279125,
          0.07539542231277906,
          0.06023534582921659,
          0.08858483593706268,
          0.06636748677006353,
          0.060444735196553204,
          0.045088644794102274,
          0.07822592886599872,
          0.055311620808814695,
          0.07188482765708855,
          0.05702557314753779,
          0.046129940622520224,
          0.07632039057980616,
          0.09749776412478069,
          0.05914605885953171,
          0.08034605877313673,
          0.05914605885953171,
          0.08034605877313673,
          0.05914605885953171,
          0.08034605877313673,
          0.04075430664761874,
          0.021351444083389362,
          0.033890337908171864,
          0.04349056473846526,
          0.06812461498501025,
          0.10209065728097505,
          0.0829183285601656,
          0.02983126062467596,
          0.027711856274733056,
          0.02228470913006103,
          0.03068389091150264,
          0.03340667029573343,
          0.12115707573963036,
          0.11145494705636572,
          0.02255913646973078,
          0.017774896035985692,
          0.07514308478273002,
          0.04892971608484065,
          0.0652530934305157,
          0.07030501386897685,
          0.03813409651249302,
          0.051670662613393585,
          0.045088644794102274,
          0.0887682187331669,
          0.02708762795475931,
          0.026431738838101335,
          0.05616039041829851,
          0.02633271894258931,
          0.02120265025911013,
          0.04918384397871582,
          0.01928562528308396,
          0.029638757702073446,
          0.04169987468413533,
          0.030949082421137048,
          0.06916060751295382,
          0.0516167875415878,
          0.05531959826298822,
          0.07033554101995253,
          0.11472295396232049,
          0.18530287684400812,
          0.013123373257285437,
          0.05943171720608386,
          0.028667645541335083,
          0.03224236215530978,
          0.03902436505170379,
          0.032637614020019905,
          0.026757729062606202,
          0.014008571101143603,
          0.003393126468208829,
          0.029975572005004607,
          0.037063255258363084,
          0.08923518158685084,
          0.11513102327923368,
          0.037991489644898666,
          0.045209496922386704,
          0.08910124918192056,
          0.09717922812604735,
          0.07030501386897685,
          0.08313174924231855,
          0.03840344159831305,
          0.02782281651104357,
          0.015979381468708816,
          0.05092405067545789,
          0.04526594364774321,
          0.03362803282910667,
          0.028226145383228658,
          0.05073777448129945,
          0.05328678532667379,
          0.044664038893314946,
          0.04153593583735767,
          0.03681421113486708,
          0.058699137566765855,
          0.04288567079082781,
          0.09106206026794907,
          0.07456001965053133,
          0.03553832133642964,
          0.045992516056266225,
          0.026403446767343283,
          0.030439023752511195,
          0.03441419124204736,
          0.02967277450428924,
          0.04431644217770628,
          0.021056157529543908,
          0.02994808637732963,
          0.03285922228167706,
          0.02432980341715731,
          0.019098330951022795,
          0.07612300459696003,
          0.05378874936336485,
          0.06611326023856558,
          0.05025086957339825,
          0.03967239947032148,
          0.021284511153198925,
          0.023988959266774443,
          0.023120165454297847,
          0.07325084657996049,
          0.08961234390317137,
          0.08361174530537412,
          0.09631355500990993,
          0.06100978212932532,
          0.09222887311803973,
          0.016587026943392067,
          0.09631355500990993,
          0.02942897767509831,
          0.09306161356796988,
          0.13172100438714357,
          0.054295170738270525,
          0.04634607072738599,
          0.07860543791141623,
          0.045992516056266225,
          0.06654303411502362,
          0.04005154100060237,
          0.053669281532446815,
          0.08095693836892384,
          0.02669836260774568,
          0.023668547726057573,
          0.0728711505930225,
          0.04593031481220533,
          0.03997990123009259,
          0.045992516056266225,
          0.05285958522779604,
          0.05291879947855883,
          0.03192472610864699,
          0.0786125142498881,
          0.055677129347539545,
          0.13004097569105674,
          0.09984352538337163,
          0.09934192951182977,
          0.08597858919469241,
          0.10241985714347938,
          0.05977925809051198,
          0.02842600208930475,
          0.059881117523841475,
          0.04149058295047566,
          0.05467035646780879,
          0.023634594494125388,
          0.06555408824848251,
          0.030709586648980144,
          0.030709586648980144,
          0.037749111102485244,
          0.07647759623928241,
          0.06430560483493235,
          0.09340692893955756,
          0.05381212634292487,
          0.0905029839177724,
          0.14075851754505078,
          0.10906415882271936,
          0.07816197933632188,
          0.1651666461613087,
          0.1353417309763271,
          0.12214042563382645,
          0.1296159875389641,
          0.060664380902028164,
          0.0752801918653581,
          0.09693831776671215,
          0.09518876860564394,
          0.1212576354229487,
          0.057638180659136826,
          0.07291004584438446,
          0.06562995174459638,
          0.07693468383271918,
          0.13015671888691965,
          0.11756328394078819,
          0.0917303113708651,
          0.15419123397937715,
          0.18019650439127677,
          0.13897340816504675,
          0.17037418326893772,
          0.14196091135479094,
          0.10239011520964553,
          0.056221457885781326,
          0.056221457885781326,
          0.07213700700422954,
          0.07367950378560785,
          0.2970390234311171,
          0.027627808770443196,
          0.0928355584740105,
          0.09189551555804581,
          0.07410357166133799,
          0.07130172742016307,
          0.07323520969443968,
          0.09705537428781863,
          0.06238196953029654,
          0.0933874285454061,
          0.07558201994576999,
          0.07298443542345509,
          0.08936639797474964,
          0.06031174896680442,
          0.06812972472357946,
          0.07508863402394508,
          0.10214318409900554,
          0.11529119258978314,
          0.08145612467820758,
          0.08864198707978886,
          0.09938289761535657,
          0.10885519210795275,
          0.07524728229549742,
          0.027627808770443196,
          0.17142350819764252,
          0.15733703551586514,
          0.14788932305191665,
          0.07829244254234746,
          0.09847167161351529,
          0.14142552530121827,
          0.07590153998552239,
          0.07590153998552239,
          0.07590153998552239,
          0.07590153998552239,
          0.06919134489550352,
          0.06386650972292104,
          0.07492460277152312,
          0.08109153639465008,
          0.06145296198087098,
          0.007955224735505402,
          0.05881672746634976,
          0.07384102674627135,
          0.08413699881148344,
          0.11305384186698365,
          0.15225963654282523,
          0.11896345815998786,
          0.05910171857274614,
          0.21248404179745775,
          0.09315673656754642,
          0.130076410148015
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Logistic Regression Curve",
         "type": "scatter",
         "x": [
          5,
          6.8686868686868685,
          8.737373737373737,
          10.606060606060606,
          12.474747474747474,
          14.343434343434344,
          16.21212121212121,
          18.080808080808083,
          19.949494949494948,
          21.81818181818182,
          23.68686868686869,
          25.555555555555557,
          27.424242424242426,
          29.292929292929294,
          31.161616161616163,
          33.03030303030303,
          34.898989898989896,
          36.76767676767677,
          38.63636363636364,
          40.505050505050505,
          42.37373737373738,
          44.24242424242424,
          46.111111111111114,
          47.97979797979798,
          49.84848484848485,
          51.717171717171716,
          53.58585858585859,
          55.45454545454545,
          57.323232323232325,
          59.19191919191919,
          61.06060606060606,
          62.92929292929293,
          64.79797979797979,
          66.66666666666667,
          68.53535353535354,
          70.4040404040404,
          72.27272727272728,
          74.14141414141415,
          76.01010101010101,
          77.87878787878788,
          79.74747474747475,
          81.61616161616162,
          83.48484848484848,
          85.35353535353535,
          87.22222222222223,
          89.0909090909091,
          90.95959595959596,
          92.82828282828282,
          94.6969696969697,
          96.56565656565657,
          98.43434343434343,
          100.30303030303031,
          102.17171717171718,
          104.04040404040404,
          105.9090909090909,
          107.77777777777779,
          109.64646464646465,
          111.51515151515152,
          113.38383838383838,
          115.25252525252526,
          117.12121212121212,
          118.98989898989899,
          120.85858585858585,
          122.72727272727273,
          124.5959595959596,
          126.46464646464646,
          128.33333333333334,
          130.2020202020202,
          132.07070707070707,
          133.93939393939394,
          135.8080808080808,
          137.67676767676767,
          139.54545454545456,
          141.41414141414143,
          143.2828282828283,
          145.15151515151516,
          147.02020202020202,
          148.88888888888889,
          150.75757575757575,
          152.62626262626262,
          154.4949494949495,
          156.36363636363637,
          158.23232323232324,
          160.1010101010101,
          161.96969696969697,
          163.83838383838383,
          165.7070707070707,
          167.5757575757576,
          169.44444444444446,
          171.31313131313132,
          173.1818181818182,
          175.05050505050505,
          176.91919191919192,
          178.78787878787878,
          180.65656565656565,
          182.52525252525254,
          184.3939393939394,
          186.26262626262627,
          188.13131313131314,
          190
         ],
         "y": [
          0.02406936701550484,
          0.024834085731011604,
          0.0256224627872379,
          0.026435189017312587,
          0.02727297278253038,
          0.028136540244041223,
          0.02902663562642819,
          0.0299440214720502,
          0.03088947888495356,
          0.03186380776308497,
          0.03286782701746186,
          0.03390237477687845,
          0.03496830857664566,
          0.03606650552977914,
          0.03719786247896634,
          0.03836329612755446,
          0.039563743147714464,
          0.04080016026384359,
          0.042073524309178836,
          0.043384832253499026,
          0.04473510119970125,
          0.04612536834694091,
          0.047556690917933055,
          0.04903014604791635,
          0.05054683063269042,
          0.052107861133045254,
          0.05371437333281206,
          0.0553675220476805,
          0.05706848078184275,
          0.05881844132944976,
          0.06061861331778967,
          0.06247022368903552,
          0.06437451611734775,
          0.0663327503580699,
          0.0683462015257126,
          0.07041615929739445,
          0.07254392703838917,
          0.07473082084642561,
          0.07697816851140005,
          0.0792873083871872,
          0.08165958817228398,
          0.08409636359608617,
          0.08659899700768817,
          0.08916885586420452,
          0.09180731111575208,
          0.09451573548439256,
          0.09729550163452952,
          0.10014798023247368,
          0.103074537893148,
          0.1060765350121893,
          0.1091553234820281,
          0.1123122442908875,
          0.11554862500403928,
          0.11886577712709422,
          0.12226499335157646,
          0.12574754468355473,
          0.12931467745665526,
          0.1329676102313882,
          0.13670753058335403,
          0.14053559178358246,
          0.1444529093749756,
          0.1484605576495863,
          0.15255956603225954,
          0.15675091537699265,
          0.16103553418323174,
          0.16541429474020863,
          0.16988800920833336,
          0.17445742564758737,
          0.17912322400380318,
          0.18388601206466687,
          0.1887463213982277,
          0.19370460328764127,
          0.19876122467680038,
          0.2039164641424083,
          0.20917050790892264,
          0.21452344592362133,
          0.21997526800982065,
          0.22552586011698453,
          0.23117500068710398,
          0.23692235715727755,
          0.2427674826188831,
          0.2487098126540814,
          0.25474866237062954,
          0.26088322365608496,
          0.2671125626724581,
          0.2734356176121924,
          0.2798511967360244,
          0.28635797671278523,
          0.2929545012805516,
          0.2996391802477253,
          0.30641028885161886,
          0.31326596749095004,
          0.32020422184729547,
          0.327222923409031,
          0.33431981040959435,
          0.34149248919005665,
          0.3487384359939796,
          0.35605499920039857,
          0.3634394019984893,
          0.3708887455050957
         ]
        }
       ],
       "layout": {
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Logistic Regression: Predicted Probability of 'Fire' Type Pokemon vs Attack"
        },
        "xaxis": {
         "title": {
          "text": "Attack Value"
         }
        },
        "yaxis": {
         "title": {
          "text": "Predicted Probability"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"5e09b9b9-479a-4070-b25e-4fe72d00eccc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5e09b9b9-479a-4070-b25e-4fe72d00eccc\")) {                    Plotly.newPlot(                        \"5e09b9b9-479a-4070-b25e-4fe72d00eccc\",                        [{\"marker\":{\"color\":[0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"size\":8},\"mode\":\"markers\",\"name\":\"Predicted Probabilities\",\"x\":[49,62,82,100,52,64,84,130,104,48,63,83,103,30,20,45,35,25,90,150,45,60,80,80,56,81,60,90,60,85,55,90,75,100,47,62,92,57,72,102,45,70,41,76,45,70,45,80,50,65,80,70,95,55,65,55,80,45,70,52,82,80,105,70,110,50,65,95,20,35,50,50,80,100,130,75,90,105,40,70,80,95,120,85,100,65,75,75,35,60,65,85,110,45,70,80,105,65,95,35,50,65,65,45,48,73,105,130,30,50,40,95,50,80,120,105,55,65,90,85,130,5,55,95,125,40,65,67,92,45,75,45,110,50,83,95,125,155,100,10,125,155,85,48,55,65,65,130,60,40,60,80,115,105,135,110,85,90,100,64,84,134,110,190,150,100,49,62,82,52,64,84,65,80,105,46,76,30,50,20,35,60,90,90,38,58,40,25,30,20,40,50,75,40,55,75,95,80,20,50,100,75,35,45,55,70,30,75,65,45,85,65,65,85,75,60,72,33,80,65,90,70,75,85,125,80,120,95,130,150,10,125,185,95,80,130,40,50,50,100,55,65,105,55,40,80,60,90,90,95,60,120,80,95,20,35,95,30,63,75,80,10,85,115,75,64,84,134,164,90,130,100,45,65,85,110,60,85,120,160,70,85,110,150,55,90,30,70,45,35,70,35,50,30,50,70,40,70,100,55,85,30,50,25,35,65,85,30,60,40,130,60,80,160,45,90,90,51,71,91,60,120,20,45,45,65,75,85,85,105,70,90,110,140,40,60,100,45,75,75,50,40,73,47,60,43,73,90,120,140,70,90,60,100,120,85,25,45,60,100,70,100,85,115,40,70,110,115,100,55,95,48,78,80,120,40,70,41,81,95,125,15,60,70,90,75,115,165,40,70,68,50,130,150,23,50,80,120,40,60,80,64,104,84,90,30,75,95,135,145,55,75,135,145,100,50,75,80,100,90,130,100,150,150,180,150,180,100,150,180,70,95,68,89,109,58,78,104,51,66,86,55,75,120,45,85,25,85,65,85,120,30,70,125,165,42,52,29,59,79,69,94,30,80,45,65,105,35,60,48,83,100,50,80,66,76,136,60,125,55,82,30,63,93,24,89,80,25,5,65,92,70,90,130,170,85,70,110,145,72,112,50,90,61,106,100,49,69,20,62,92,132,120,70,85,140,100,123,95,50,76,110,60,95,130,80,125,165,55,100,80,50,65,65,65,65,65,75,105,125,120,120,90,160,100,120,70,80,100,90,100,103,120,100,45,60,75,63,93,123,55,75,100,55,85,60,80,110,50,88,53,98,53,98,53,98,25,55,55,77,115,60,100,75,105,135,45,57,85,135,60,60,80,105,140,50,65,95,100,125,53,63,103,45,55,100,27,67,35,60,92,72,82,117,90,140,30,86,65,95,75,90,58,30,50,78,108,112,140,50,95,65,105,50,95,30,45,55,30,40,65,44,87,50,65,95,60,100,75,75,135,55,85,40,60,75,47,77,50,94,55,80,100,55,85,115,55,75,30,40,55,87,117,147,70,110,50,40,70,66,85,125,120,74,124,85,125,110,83,123,55,65,97,109,65,85,105,85,60,90,129,90,115,100,115,105,120,150,125,145,130,170,120,72,72,77,128,120,61,78,107,45,59,69,56,63,95,36,56,50,73,81,35,22,52,50,68,38,45,65,65,100,82,124,80,48,48,48,80,110,150,50,52,72,48,80,54,92,52,105,60,75,53,73,38,55,89,121,59,77,65,92,58,50,50,75,100,80,70,110,66,66,66,66,90,85,95,100,69,117,30,70,131,131,100,100,160,110,160,110],\"y\":[0.051870473179323924,0.0562301500356128,0.06452240255249116,0.059595518237456443,0.10921569516568261,0.08819827987208945,0.06974961890010782,0.10659853335945497,0.0955738197415481,0.056409578379128064,0.04730675941190599,0.037323707104900995,0.029382384994346398,0.09621588215197491,0.04732861630052293,0.048144460101753495,0.049311848481598815,0.03464484042083051,0.10776618489458129,0.25268481505313284,0.05285852656686191,0.05863268860969511,0.06725399523897199,0.06423324229943546,0.14259352530836222,0.10878377093351249,0.07377504356372007,0.08629716011335885,0.12014572151371682,0.09109299585472368,0.12360583408838692,0.14170934673517535,0.050001712140962835,0.0371936643882512,0.079414602415652,0.0668625145656812,0.07297755953456761,0.12737124532234215,0.10256447730390458,0.09347894888303555,0.08576130672392186,0.06441444581781801,0.09984785249106373,0.0671101688592015,0.06362116646247476,0.07543212351598053,0.055376006342985125,0.07040611935612995,0.04984954062002233,0.05531420315121147,0.061339245746797985,0.06885122769923148,0.08154856188258308,0.056645094911320436,0.06068553761796319,0.18008427745087108,0.13882452937898687,0.12103758317971619,0.09179169359219992,0.09565830651640869,0.06801440523154229,0.20066457544995583,0.15558434691725545,0.13598913424780898,0.10008678911549554,0.11460897958951419,0.0740977040593947,0.07115660592753953,0.13931664363419663,0.11851547849269839,0.10045760192271261,0.05826407444667093,0.13882452937898687,0.11179494800966176,0.13550692335128253,0.0893229026433946,0.09866541464238301,0.10886826823713752,0.05105605550203429,0.06280546575762443,0.05337296090864082,0.05919980630954484,0.07695521816313397,0.13159265307892584,0.11179494800966176,0.057941484029988256,0.04479781373813964,0.023013135558839274,0.033812323439761975,0.04032774643072385,0.06355078022564949,0.09545836118703475,0.1124698280315702,0.070881301393685,0.05465984776330975,0.13882452937898687,0.10579815539747389,0.027681622255203958,0.03213123234417902,0.049311848481598815,0.05472096218640811,0.06068553761796319,0.05039278493188995,0.016850802428263294,0.09739901977961364,0.07338794187974378,0.07061109198995269,0.05281741278997153,0.06398770345571217,0.05067173351024046,0.03340333670311388,0.07793940905861907,0.024876939927353025,0.026678565875096944,0.22663756083361328,0.09513264341056984,0.047777286596981765,0.03194518138728216,0.02364722309978597,0.06061818891481711,0.09845536234010355,0.14392841169203668,0.015165862986623167,0.07917454376802902,0.07381620741652305,0.04302662550586858,0.03194518138728216,0.08758997202922922,0.11283092844997043,0.070881301393685,0.05658196017483262,0.04181547655412865,0.10301770186218005,0.060038350620542025,0.12128445332950208,0.14499121903418713,0.07381620741652305,0.14361533538361398,0.05674483185497528,0.04016656063467179,0.13045491821679245,0.15744461333352608,0.06958638075876422,0.08988394827350045,0.09500390749234944,0.08488619399000018,0.08488619399000018,0.22053837064925386,0.05959066076716101,0.027601890286133315,0.03033391468713402,0.0585674761030412,0.0891307731119873,0.10886826823713752,0.14376078453869331,0.14762906039734883,0.1762480486113337,0.18377507257021367,0.15016593961189845,0.12434062742262586,0.0998042825353299,0.1301159600616955,0.1038206171270304,0.02738792833708632,0.08807068110353662,0.04934008985593955,0.054305143516034035,0.04405752529991949,0.034735152628004216,0.10375491995698168,0.0836896439137949,0.0665652361037985,0.07220781295931927,0.0590572209496348,0.05068843735642205,0.11990190912558267,0.08591756380827868,0.04301612442630243,0.04946183385936434,0.03648167097200262,0.03867266230176373,0.06381175329451398,0.07825453179637254,0.07144642956990946,0.04549207996828337,0.0522885251428027,0.17722800612111592,0.10188258298493393,0.049517440881453365,0.03373656957333932,0.030127706342904837,0.0518263922435799,0.06159173440657877,0.09334344742670495,0.07877819506699905,0.0473442730549452,0.06152344587912112,0.038742054167873796,0.030162266874830024,0.03729239677873011,0.03050851580786981,0.06259011189630395,0.042500707520645775,0.04558099233480385,0.04449779268209772,0.09959707647517824,0.10435424353971108,0.10755864887591944,0.06603321227879817,0.04776934795096591,0.06299377342201352,0.08053191571178347,0.019615711948172986,0.09307706661702816,0.056144985839731695,0.07440409473051283,0.1233924516125418,0.05092629935150408,0.06987028990722849,0.034856910684017556,0.04089680521632072,0.06631944979760646,0.04444755901520337,0.021227570558388174,0.031072954936106383,0.13210286342411884,0.1262590503676548,0.08093531543754523,0.11153512547623604,0.10664042108448803,0.004438869255329869,0.12842060033046365,0.2177056732483556,0.09683028133133766,0.13210286342411884,0.14643023787877926,0.09334344742670495,0.025469570331042437,0.054297532891429356,0.08369959606725073,0.0386287387861989,0.15486982234014343,0.10048898035517813,0.05620766204516892,0.034751845803471786,0.03467262709386443,0.06994715257313312,0.09367667819301771,0.06518873917299493,0.06745468079433349,0.07440409473051283,0.03685285965154846,0.044630959990167446,0.12137315660606464,0.07806226544116798,0.09871222008718898,0.04954958731575383,0.049517440881453365,0.14303856361723302,0.17017399792510124,0.029124697744320697,0.12984782288237898,0.21634331076312505,0.10323364298605688,0.09320219881423597,0.06205740337916958,0.0711442201839988,0.10862096570786064,0.120924302670877,0.12014242722382995,0.08548080343795633,0.06979350536499049,0.06719527168237419,0.07025706008066092,0.055714081766946304,0.05936231855131566,0.07440696118679894,0.04546607224086786,0.07295634159423983,0.12409208033476844,0.06632202710627258,0.04138247965686184,0.05163865776629049,0.08158975924621727,0.07878121569481625,0.05254755116153139,0.04456897062068404,0.04882603228147658,0.06719527168237419,0.03252275744947776,0.03904419957454759,0.03252275744947776,0.023132111090121806,0.02429810631443843,0.02802115850210112,0.032295789221156286,0.04072562592520388,0.04290738023347443,0.05803609796215426,0.03683296538084088,0.04546607224086786,0.02429810631443843,0.01732270057523507,0.02344543503991117,0.025180988202122095,0.031171428851162086,0.043378425009559865,0.023835984277098446,0.029516142958260383,0.030631557150789336,0.07806397207408473,0.04263643911708119,0.033604191512519685,0.07063661204546288,0.017538370246117083,0.05673771943648918,0.05673771943648918,0.1021741228051385,0.08160429614930415,0.06487637694922106,0.09747585340751368,0.11085337609104713,0.018656892896365684,0.0037451665858079888,0.05088859270537859,0.04018155323160608,0.033459304886803196,0.024513501498687536,0.035908879273376104,0.034205249579592735,0.024242118983801117,0.023078746879052942,0.021969947603565125,0.0011591384343730238,0.02259586999929652,0.02606506647028684,0.045963857792239264,0.05851142380781372,0.054468796017418664,0.03092707962018982,0.0634200290355794,0.04072562592520388,0.06060915294438334,0.03966291276789847,0.034702814094700815,0.03930002801886176,0.027448539613654985,0.07142781697480925,0.09560801986152842,0.09984037966605065,0.0996008090635004,0.10397458733812405,0.036388817664069845,0.05288606464831723,0.05534294582943419,0.0064008436701467036,0.048626732047206044,0.028847051210592417,0.04263643911708119,0.12107895391451112,0.03904419957454759,0.04816966716564266,0.10988071879223255,0.07381262944432673,0.021534653431827613,0.02668139043132475,0.04281039508421264,0.10267598384636932,0.08127160626886232,0.026386656986600414,0.042345245486266406,0.028977675897108212,0.03583812532785983,0.0513691914593447,0.06358351477286069,0.02259586999929652,0.023105413942746404,0.018589258729220997,0.029977198219791147,0.058728360862490045,0.06000268340893539,0.06283415317161434,0.02478224521265604,0.037862422732203825,0.05254755116153139,0.10756264419514595,0.08984851427351866,0.1476779930415968,0.012862449622658773,0.006646707887018933,0.02759347245147438,0.027163592259996882,0.12892651737748456,0.17259540279753194,0.03254863293575175,0.04798238456234363,0.033604191512519685,0.06460708703208608,0.023708115637899506,0.027343300525769717,0.031517877075793295,0.022289847936940575,0.024473400258871847,0.017489150549401353,0.02540400608561074,0.029929130933218397,0.054468796017418664,0.024312388431344444,0.08446570915722838,0.06276826444891771,0.022849254856176813,0.026356305056444197,0.05340075188802477,0.05214187357658875,0.002847300760632544,0.1256567528397001,0.019855367944312568,0.11973054103875547,0.06438126305875612,0.11009799994691538,0.04482915253397503,0.06847884132719896,0.02722641913633798,0.006353121961930724,0.00980540597040601,0.03399611538410161,0.01755507791808677,0.07729859567680272,0.08357115770999683,0.11023877434033122,0.016335355862495123,0.07490144147590404,0.03662037545533557,0.028479885063891302,0.03723014056484569,0.054638739880564766,0.036856845620579295,0.047245657424595695,0.03780819377493721,0.031606536704974084,0.029884806592491233,0.03102815086589091,0.03574462922231229,0.06182396510923579,0.04946558143677571,0.03835487418385169,0.03460908904225074,0.06950674808109636,0.08051053307178913,0.07342489205869758,0.0562200178403158,0.019465490981318678,0.028517431257430056,0.17034459604717822,0.1842979084540904,0.010664573816654496,0.007764296495317743,0.03299312117658866,0.01957389733852702,0.02258984664396102,0.02102903055494826,0.048847412938126326,0.01819369262894121,0.023643593530778113,0.0210077879507896,0.07835141289993867,0.08554633208200581,0.036440111898527985,0.02700714136535058,0.041466772735801745,0.03439711074578529,0.058421291950165176,0.027478807480882012,0.04108558619778404,0.062177473081203014,0.023587332013051184,0.06408939184640872,0.035951145726858195,0.0789440408029604,0.05503200587527252,0.04610444982134587,0.02907426074594365,0.03013917243185417,0.040958606294561245,0.010727882421295094,0.024031816659632874,0.01835421388063504,0.016244589250764472,0.06859356581818327,0.03175800319584795,0.027281706794517968,0.03450448056008485,0.03972771804012366,0.05764813131069022,0.09075126259296194,0.09368344562572292,0.07399910583112917,0.05258947181454367,0.0781464727999439,0.026220919943375467,0.016150461303045937,0.016025917915505943,0.02588495051532916,0.031165671154128587,0.05162932101815939,0.04940416874873126,0.03358251765084933,0.026415362023362355,0.014222698229928358,0.02880447147685569,0.03735451221211008,0.05426491206236836,0.0647392535271875,0.017632934100108456,0.01996541503169846,0.04894973085061288,0.010749222061457503,0.08205137126383823,0.052391752737565696,0.015268439475726469,0.025781985815895712,0.011006609977183555,0.008447067363049367,0.02437193162917433,0.06620788583536469,0.03764673565803267,0.07013140200522901,0.10033338312363434,0.010224564620875886,0.008022829889737326,0.03211075469957042,0.018172588963521173,0.01751066391430199,0.01751066391430199,0.01751066391430199,0.01751066391430199,0.01751066391430199,0.029709056949540354,0.03464609663780117,0.06416104836762267,0.03768207827282846,0.04550306509192179,0.07425939826331802,0.010590055831799953,0.05448045812691987,0.04550306509192179,0.007471370738964538,0.028293278844360087,0.022228983982389268,0.06948241548654446,0.04380995713890562,0.07805062105451488,0.01695733326506623,0.08555263209489919,0.04266640566292494,0.030949082421137048,0.022374438754279125,0.07539542231277906,0.06023534582921659,0.08858483593706268,0.06636748677006353,0.060444735196553204,0.045088644794102274,0.07822592886599872,0.055311620808814695,0.07188482765708855,0.05702557314753779,0.046129940622520224,0.07632039057980616,0.09749776412478069,0.05914605885953171,0.08034605877313673,0.05914605885953171,0.08034605877313673,0.05914605885953171,0.08034605877313673,0.04075430664761874,0.021351444083389362,0.033890337908171864,0.04349056473846526,0.06812461498501025,0.10209065728097505,0.0829183285601656,0.02983126062467596,0.027711856274733056,0.02228470913006103,0.03068389091150264,0.03340667029573343,0.12115707573963036,0.11145494705636572,0.02255913646973078,0.017774896035985692,0.07514308478273002,0.04892971608484065,0.0652530934305157,0.07030501386897685,0.03813409651249302,0.051670662613393585,0.045088644794102274,0.0887682187331669,0.02708762795475931,0.026431738838101335,0.05616039041829851,0.02633271894258931,0.02120265025911013,0.04918384397871582,0.01928562528308396,0.029638757702073446,0.04169987468413533,0.030949082421137048,0.06916060751295382,0.0516167875415878,0.05531959826298822,0.07033554101995253,0.11472295396232049,0.18530287684400812,0.013123373257285437,0.05943171720608386,0.028667645541335083,0.03224236215530978,0.03902436505170379,0.032637614020019905,0.026757729062606202,0.014008571101143603,0.003393126468208829,0.029975572005004607,0.037063255258363084,0.08923518158685084,0.11513102327923368,0.037991489644898666,0.045209496922386704,0.08910124918192056,0.09717922812604735,0.07030501386897685,0.08313174924231855,0.03840344159831305,0.02782281651104357,0.015979381468708816,0.05092405067545789,0.04526594364774321,0.03362803282910667,0.028226145383228658,0.05073777448129945,0.05328678532667379,0.044664038893314946,0.04153593583735767,0.03681421113486708,0.058699137566765855,0.04288567079082781,0.09106206026794907,0.07456001965053133,0.03553832133642964,0.045992516056266225,0.026403446767343283,0.030439023752511195,0.03441419124204736,0.02967277450428924,0.04431644217770628,0.021056157529543908,0.02994808637732963,0.03285922228167706,0.02432980341715731,0.019098330951022795,0.07612300459696003,0.05378874936336485,0.06611326023856558,0.05025086957339825,0.03967239947032148,0.021284511153198925,0.023988959266774443,0.023120165454297847,0.07325084657996049,0.08961234390317137,0.08361174530537412,0.09631355500990993,0.06100978212932532,0.09222887311803973,0.016587026943392067,0.09631355500990993,0.02942897767509831,0.09306161356796988,0.13172100438714357,0.054295170738270525,0.04634607072738599,0.07860543791141623,0.045992516056266225,0.06654303411502362,0.04005154100060237,0.053669281532446815,0.08095693836892384,0.02669836260774568,0.023668547726057573,0.0728711505930225,0.04593031481220533,0.03997990123009259,0.045992516056266225,0.05285958522779604,0.05291879947855883,0.03192472610864699,0.0786125142498881,0.055677129347539545,0.13004097569105674,0.09984352538337163,0.09934192951182977,0.08597858919469241,0.10241985714347938,0.05977925809051198,0.02842600208930475,0.059881117523841475,0.04149058295047566,0.05467035646780879,0.023634594494125388,0.06555408824848251,0.030709586648980144,0.030709586648980144,0.037749111102485244,0.07647759623928241,0.06430560483493235,0.09340692893955756,0.05381212634292487,0.0905029839177724,0.14075851754505078,0.10906415882271936,0.07816197933632188,0.1651666461613087,0.1353417309763271,0.12214042563382645,0.1296159875389641,0.060664380902028164,0.0752801918653581,0.09693831776671215,0.09518876860564394,0.1212576354229487,0.057638180659136826,0.07291004584438446,0.06562995174459638,0.07693468383271918,0.13015671888691965,0.11756328394078819,0.0917303113708651,0.15419123397937715,0.18019650439127677,0.13897340816504675,0.17037418326893772,0.14196091135479094,0.10239011520964553,0.056221457885781326,0.056221457885781326,0.07213700700422954,0.07367950378560785,0.2970390234311171,0.027627808770443196,0.0928355584740105,0.09189551555804581,0.07410357166133799,0.07130172742016307,0.07323520969443968,0.09705537428781863,0.06238196953029654,0.0933874285454061,0.07558201994576999,0.07298443542345509,0.08936639797474964,0.06031174896680442,0.06812972472357946,0.07508863402394508,0.10214318409900554,0.11529119258978314,0.08145612467820758,0.08864198707978886,0.09938289761535657,0.10885519210795275,0.07524728229549742,0.027627808770443196,0.17142350819764252,0.15733703551586514,0.14788932305191665,0.07829244254234746,0.09847167161351529,0.14142552530121827,0.07590153998552239,0.07590153998552239,0.07590153998552239,0.07590153998552239,0.06919134489550352,0.06386650972292104,0.07492460277152312,0.08109153639465008,0.06145296198087098,0.007955224735505402,0.05881672746634976,0.07384102674627135,0.08413699881148344,0.11305384186698365,0.15225963654282523,0.11896345815998786,0.05910171857274614,0.21248404179745775,0.09315673656754642,0.130076410148015],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"Logistic Regression Curve\",\"x\":[5.0,6.8686868686868685,8.737373737373737,10.606060606060606,12.474747474747474,14.343434343434344,16.21212121212121,18.080808080808083,19.949494949494948,21.81818181818182,23.68686868686869,25.555555555555557,27.424242424242426,29.292929292929294,31.161616161616163,33.03030303030303,34.898989898989896,36.76767676767677,38.63636363636364,40.505050505050505,42.37373737373738,44.24242424242424,46.111111111111114,47.97979797979798,49.84848484848485,51.717171717171716,53.58585858585859,55.45454545454545,57.323232323232325,59.19191919191919,61.06060606060606,62.92929292929293,64.79797979797979,66.66666666666667,68.53535353535354,70.4040404040404,72.27272727272728,74.14141414141415,76.01010101010101,77.87878787878788,79.74747474747475,81.61616161616162,83.48484848484848,85.35353535353535,87.22222222222223,89.0909090909091,90.95959595959596,92.82828282828282,94.6969696969697,96.56565656565657,98.43434343434343,100.30303030303031,102.17171717171718,104.04040404040404,105.9090909090909,107.77777777777779,109.64646464646465,111.51515151515152,113.38383838383838,115.25252525252526,117.12121212121212,118.98989898989899,120.85858585858585,122.72727272727273,124.5959595959596,126.46464646464646,128.33333333333334,130.2020202020202,132.07070707070707,133.93939393939394,135.8080808080808,137.67676767676767,139.54545454545456,141.41414141414143,143.2828282828283,145.15151515151516,147.02020202020202,148.88888888888889,150.75757575757575,152.62626262626262,154.4949494949495,156.36363636363637,158.23232323232324,160.1010101010101,161.96969696969697,163.83838383838383,165.7070707070707,167.5757575757576,169.44444444444446,171.31313131313132,173.1818181818182,175.05050505050505,176.91919191919192,178.78787878787878,180.65656565656565,182.52525252525254,184.3939393939394,186.26262626262627,188.13131313131314,190.0],\"y\":[0.02406936701550484,0.024834085731011604,0.0256224627872379,0.026435189017312587,0.02727297278253038,0.028136540244041223,0.02902663562642819,0.0299440214720502,0.03088947888495356,0.03186380776308497,0.03286782701746186,0.03390237477687845,0.03496830857664566,0.03606650552977914,0.03719786247896634,0.03836329612755446,0.039563743147714464,0.04080016026384359,0.042073524309178836,0.043384832253499026,0.04473510119970125,0.04612536834694091,0.047556690917933055,0.04903014604791635,0.05054683063269042,0.052107861133045254,0.05371437333281206,0.0553675220476805,0.05706848078184275,0.05881844132944976,0.06061861331778967,0.06247022368903552,0.06437451611734775,0.0663327503580699,0.0683462015257126,0.07041615929739445,0.07254392703838917,0.07473082084642561,0.07697816851140005,0.0792873083871872,0.08165958817228398,0.08409636359608617,0.08659899700768817,0.08916885586420452,0.09180731111575208,0.09451573548439256,0.09729550163452952,0.10014798023247368,0.103074537893148,0.1060765350121893,0.1091553234820281,0.1123122442908875,0.11554862500403928,0.11886577712709422,0.12226499335157646,0.12574754468355473,0.12931467745665526,0.1329676102313882,0.13670753058335403,0.14053559178358246,0.1444529093749756,0.1484605576495863,0.15255956603225954,0.15675091537699265,0.16103553418323174,0.16541429474020863,0.16988800920833336,0.17445742564758737,0.17912322400380318,0.18388601206466687,0.1887463213982277,0.19370460328764127,0.19876122467680038,0.2039164641424083,0.20917050790892264,0.21452344592362133,0.21997526800982065,0.22552586011698453,0.23117500068710398,0.23692235715727755,0.2427674826188831,0.2487098126540814,0.25474866237062954,0.26088322365608496,0.2671125626724581,0.2734356176121924,0.2798511967360244,0.28635797671278523,0.2929545012805516,0.2996391802477253,0.30641028885161886,0.31326596749095004,0.32020422184729547,0.327222923409031,0.33431981040959435,0.34149248919005665,0.3487384359939796,0.35605499920039857,0.3634394019984893,0.3708887455050957],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Logistic Regression: Predicted Probability of 'Fire' Type Pokemon vs Attack\"},\"xaxis\":{\"title\":{\"text\":\"Attack Value\"}},\"yaxis\":{\"title\":{\"text\":\"Predicted Probability\"}},\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5e09b9b9-479a-4070-b25e-4fe72d00eccc');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               str8fyre   No. Observations:                  800\n",
      "Model:                          Logit   Df Residuals:                      788\n",
      "Method:                           MLE   Df Model:                           11\n",
      "Date:                Fri, 15 Nov 2024   Pseudo R-squ.:                 0.05156\n",
      "Time:                        04:37:10   Log-Likelihood:                -182.49\n",
      "converged:                       True   LL-Null:                       -192.41\n",
      "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
      "============================================================================================================\n",
      "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
      "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
      "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
      "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
      "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
      "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
      "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
      "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
      "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
      "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
      "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
      "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
      "============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load and process the data\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "# Create the binary indicator for \"Fire\" type\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1'] == 'Fire').astype(int)\n",
    "\n",
    "# 2. Specify and fit the logistic regression model\n",
    "linear_model_specification_formula = 'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "\n",
    "# 3. Predict probabilities using the fitted logistic regression model\n",
    "pokeaman['predicted_probabilities'] = log_reg_fit.predict(pokeaman)\n",
    "\n",
    "# 4. Generate data for the best-fit logistic regression curve\n",
    "attack_values = np.linspace(pokeaman['Attack'].min(), pokeaman['Attack'].max(), 100)\n",
    "predicted_probabilities_curve = log_reg_fit.predict(pd.DataFrame({\n",
    "    'Attack': attack_values,\n",
    "    'Legendary': [pokeaman['Legendary'].mode()[0]] * 100,\n",
    "    'Defense': [pokeaman['Defense'].mode()[0]] * 100,\n",
    "    'Type 2': ['None'] * 100,\n",
    "    'Generation': [pokeaman['Generation'].mode()[0]] * 100\n",
    "}))\n",
    "\n",
    "# 5. Visualize the data with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Scatter plot of Attack vs predicted probability for the \"Fire\" type classification\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=pokeaman['Attack'], \n",
    "    y=pokeaman['predicted_probabilities'], \n",
    "    mode='markers', \n",
    "    marker=dict(color=pokeaman['str8fyre'], colorscale='Viridis', size=8),\n",
    "    name=\"Predicted Probabilities\"\n",
    "))\n",
    "\n",
    "# Plot the logistic regression curve (best-fit line)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=attack_values, \n",
    "    y=predicted_probabilities_curve, \n",
    "    mode='lines', \n",
    "    line=dict(color='red', width=2),\n",
    "    name=\"Logistic Regression Curve\"\n",
    "))\n",
    "\n",
    "# Update layout for the plot\n",
    "fig.update_layout(\n",
    "    title=\"Logistic Regression: Predicted Probability of 'Fire' Type Pokemon vs Attack\",\n",
    "    xaxis_title=\"Attack Value\",\n",
    "    yaxis_title=\"Predicted Probability\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# 6. Print the summary of the logistic regression model\n",
    "print(log_reg_fit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183774de",
   "metadata": {},
   "source": [
    "Here's a summary of our conversation:\n",
    "\n",
    "1. **Logistic Regression Explanation**:\n",
    "   - You initially inquired about logistic regression, which is used when the outcome variable is binary (e.g., 0 vs. 1).\n",
    "   - Logistic regression calculates the probability of an event happening (e.g., being \"Fire\" or \"Not Fire\" for Pokmon) using a logistic function (sigmoid curve). The output is the probability of the event occurring, not a raw linear value.\n",
    "\n",
    "2. **Key Points on Binary Outcome**:\n",
    "   - We clarified that in logistic regression, the model calculates probabilities (from 0 to 1) using the logistic function, and these are interpreted as the likelihood of the binary outcome (e.g., \"Fire\" Pokmon vs. \"Non-Fire\").\n",
    "\n",
    "3. **Model Specification and Fit**:\n",
    "   - You learned how to fit a logistic regression model using the formula API (`smf.logit`) in Python, applying it to a Pokmon dataset to predict whether a Pokmon is \"Fire\" based on various features like `Attack`, `Legendary`, `Defense`, and `Generation`.\n",
    "   - The fitted model gives coefficients that explain how each predictor variable affects the log-odds of the outcome, and these coefficients can be used to compute probabilities.\n",
    "\n",
    "4. **Interpreting Results**:\n",
    "   - The summary of the logistic regression model includes statistical evidence like p-values, coefficients, and confidence intervals, which help assess the importance of each predictor.\n",
    "   - We discussed how the log-odds of an outcome are linked to the probability and how to interpret those odds in the context of logistic regression.\n",
    "\n",
    "5. **Best-Fit Line (Logistic Curve)**:\n",
    "   - You wanted to visualize the logistic regression \"best-fit line\" (which in this case is an S-shaped curve).\n",
    "   - We updated the code to visualize both the scatter plot of observed data and the logistic regression curve (showing how predicted probabilities change with respect to a predictor variable like `Attack`).\n",
    "\n",
    "6. **Plotting with Plotly**:\n",
    "   - We used **Plotly** to create an interactive plot with both the data points and the logistic regression curve. This allows you to visualize how the probability of being a \"Fire\" Pokmon changes as the `Attack` value varies, based on the logistic regression model.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Logistic regression** is suitable for binary classification problems, where the outcome variable has two possible states (e.g., \"Fire\" vs. \"Not Fire\").\n",
    "- The **best-fit line** for logistic regression is not linear but rather an S-shaped curve (sigmoid function).\n",
    "- **Statistical interpretation** involves understanding the coefficients, p-values, and how they relate to the odds of the binary outcome.\n",
    "- **Visualization** with Plotly helps in presenting how predictor variables affect the predicted probability of the outcome. \n",
    "\n",
    "https://chatgpt.com/share/6736d025-143c-8003-a44d-f047028dbcda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ac555",
   "metadata": {},
   "source": [
    "### 4. Explain the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" while at the same time \"many of the *coefficients* are larger than 10 while having *strong* or *very strong evidence against* the *null hypothesis* of 'no effect'\"<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _How do we simultaneously interpret **hypothesis testing** results regarding **coefficient estimates** based on **p-values** and **R-squared** \"the proportion of variation in (outcome) $y$ explained by the model ($\\hat y_i$)\"? How can both be meaningfully understood at the same time? Do they address different aspects of a model?_\n",
    ">    \n",
    "> _As introduced in the previous homework, **R-squared** is_\n",
    ">\n",
    "> _$$R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i-\\hat y)^2}{\\sum_{i=1}^n(y_i-\\bar y)^2}$$_\n",
    ">    \n",
    "> _which describes the **explanatory power** of a model; whereas, **p-values** allow us to characterize **evidence against** a **null hypothesis**, and **coefficients** in a **multiple linear regression** context allow us to interpret the relationship between the **outcome** and a **predictor variable** \"with all other **predictor variables** 'held constant'\". Are these concepts thus contradictory or conflictual in some manner?_\n",
    "\n",
    "|p-value|Evidence|\n",
    "|-|-|\n",
    "|$$p > 0.1$$|No evidence against the null hypothesis|\n",
    "|$$0.1 \\ge p > 0.05$$|Weak evidence against the null hypothesis|\n",
    "|$$0.05 \\ge p > 0.01$$|Moderate evidence against the null hypothesis|\n",
    "|$$0.01 \\ge p > 0.001$$|Strong evidence against the null hypothesis|\n",
    "|$$0.001 \\ge p$$|Very strong evidence against the null hypothesis|\n",
    "    \n",
    "> _In `formula='HP ~ Q(\"Sp. Def\") * C(Generation)'` the `Q` stands for \"quote\" and is needed to access column names when they have a \"space\" in their name, while the `C` indicates a **categorical** use of what is actually an **integer** valued column. Despite technically being **continuous** numbers, **integer** often simply indicate categories which should not necessarily be treated as an incremental **continuous predictor variable**. Remember, a model such as $\\beta_0 + \\beta_1 x$ means for each unit increase in $x$ the outcome increases \"on average\" by $\\beta_1$; so, if $x$ takes on the values `1` through `6` as the `Generation` **predictor variable** here does, then this means the average value for \"Generation 1\" must be $\\beta_0 + \\beta_1$ while for \"Generation 2\" it must be $\\beta_0 + 2\\times \\beta_1$ (and so on up to \"Generation 6\" which must be $\\beta_0 + 6\\times \\beta_1$). This might be a very strange restriction to place on something that is really actually a **categorical predictor variable**. You can see in the given model fit below how this six-level **categorical predictor variable** is actually appropriately treated in the specification of the **linear form** using \"Generation 1\" for the \"baseline\" and **binary indicators** to model the \"contrast\" (\"offsets\") for the other \"Generations\"; and, how these are in turn used in the context of the **interaction** considered by the model specification._ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4ccce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url)\n",
    "pokeaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4a80b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:45:27</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     01:45:27     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        01:45:27   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf5c4a",
   "metadata": {},
   "source": [
    "This is about the case between the large predictor coefficient and the samll r-squared. It is important to note that even if the predictor cofficient is large, such as C(Generation)[T.2] of 20.0049, meaning that there is a significant positive relationship between the generation 2 and the Hp level, this is only about the generation 2 alone has a significant role in the HP. While looking at theQ(\"Sp. Def\") of 0.5634, its combination of Q(\"Sp. Def\"):C(Generation)[T.2] of -0.2350 means a even smaller relationship: for Generation 2, the effect of Sp. Def on HP is 0.5634 - 0.2350 = 0.3284. Therefore, it helps to explain that even if generation 2 will have significant correaltion with dependent variables, it also weaken the effect of the impact of Sp. Def on HP. Thus, this can be explained by the small r-squared, meaning that the model only depends on the Sp.Def and Generation is not enough to capture the whole variability becasue of other factors or other independent variables not being presented here, thus ending up with a small r-square to say the only a small variability of dependent variable is explained by the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b701e1",
   "metadata": {},
   "source": [
    "You're welcome! Here's a summary of our conversation:\n",
    "\n",
    "- **Model Interpretation**: We discussed a regression model with `Sp. Def` and `Generation` as predictors for **HP**. The coefficient for `C(Generation)[T.2]` was 20.0449, indicating a significant positive effect of Generation 2 on **HP**, while the coefficient for `Q(\"Sp. Def\")` was 0.5634, showing a positive relationship between **Sp. Def** and **HP**.\n",
    "  \n",
    "- **Interaction Term**: The interaction term `Q(\"Sp. Def\"):C(Generation)[T.2]` had a coefficient of -0.2350, indicating that the effect of `Sp. Def` on **HP** is weaker for Generation 2 (net effect of 0.3284) compared to other generations. This suggests that the influence of `Sp. Def` on **HP** varies by generation.\n",
    "\n",
    "- **Small R-squared**: Despite significant coefficients, the model explains only 17.6% of the variability in **HP**. This low **R-squared** suggests that other factors, not included in the model, likely influence **HP**, and the model does not capture most of the variation in the dependent variable.\n",
    "\n",
    "- **Conclusion**: Even though `Generation` and `Sp. Def` show significant relationships with **HP**, the small R-squared indicates that the model does not account for much of the total variability in **HP**, implying the presence of other important variables not considered in the model.\n",
    "\n",
    "https://chatgpt.com/share/6735178a-c8e8-8003-b5e4-3c0eb8b77f88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614675a5",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "### Pre-lecture VS Post-lecture HW\n",
    "    \n",
    "Feel free to work on the \"Post-lecture\" HW below if you're making good progress and want to continue: in this case the \"Post-lecture\" HW just builds on the \"Post-lecture\" HW, introducing and extending the considerations available in the **multiple linear regression context**. That said, as \"question 3\" above hopefully suggests and reminds you, the **course project** is well upon us, and prioritizing work on that (even over the homework) may very well be indicated at this point...\n",
    "\n",
    "*The benefits of continue would are that (a) it might be fun to try to tackle the challenge of working through some problems without additional preparation or guidance; and (b) this is a very valable skill to be comfortable with; and (c) it will let you build experience interacting with ChatBots (and beginning to understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a392bc2",
   "metadata": {},
   "source": [
    "## \"Post-lecture\" HW [*submission along with \"Pre-lecture\" HW is due prior to next TUT*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edb40a",
   "metadata": {},
   "source": [
    "### 5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _Recall from the previous week's homework that the **R-squared** \"the proportion of variation in (outcome) $y$ explained by the model ($\\hat y_i$)\" quantity (re-introduced in the previous problem) can be calculated as `np.corrcoef(y,fitted_model.fittedvalues)[0,1]**2` (as well as several other ways in the **simple linear regression** context). The **squared correlation** between the outcome $y$ and it's **fitted values** $\\hat y$ is the most generally useful formulation of **R-squared** since this can be use in the **multiple linear regression** context._\n",
    "> \n",
    "> _This question therefore thus addresses the question of model **generalizability** on the basis of \"in sample\" and \"out of sample\" **model performance** (measured by **R-squared**)._\n",
    "> \n",
    "> - _The **squared correlation** between the **outcomes** $y$ and their **fitted values** $\\hat y$ is an \"in sample\" **model performance** metric since the $\\hat y$ \"predictions\" for the $y$ **outcomes** are based on using those already **observed outcomes** to fit the model to generate the $\\hat y$._  \n",
    "> \n",
    "> - _If we instead calculate **squared correlation** between **outcomes** $y$ that were not used to fit the model and their corresponding $\\hat y$ **predictions** (which are indeed now actually **predictions** as opposed to **fitted values**), then we are now  calculating an \"out of sample\" **model performance** metric._\n",
    "> \n",
    "> _When an \"out of sample\" metric performs more poorly than a comparitive \"in sample\" metric, then the **predictions** of the **fitted model** are not **generalizing** to data being the dataset the model is fit on. In this case we say the model is **overfit** (to the data its fit was based on). The purpose of using different **training** and **testing** datasets is to consider \"in sample\" versus \"out of sample\" **model performance** in order to try to confirm that the model is not **overfit** and that the **predictions** do indeed seem to **generalizable** beyond the dataset used for **model fitting**._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4dd90ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4513f",
   "metadata": {},
   "source": [
    "This code box here simply split the whole dataset by using 50 of 50 of the whole data set nnumebrs to seperate the first half of the data in to pokeman_train _   and the rest into group pokeman_ test. Then print pokeman_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a8b379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:45:28</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     01:45:28     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        01:45:28   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a481cb9d",
   "metadata": {},
   "source": [
    "Then this uses smf.ols, by setting Y= HP and x is the predictors of attack and defence, then using data pokeman_train to fit the model as model3_fit and find the summary of the table. \n",
    "expression mainly like this \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac52c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7948b0e",
   "metadata": {},
   "source": [
    "Then using the pokeman_test data to test the fittness of the model3.\n",
    "we print the r square of model 3 by using the origianl data to test the fitness\n",
    "we also use the unseen data, which is pokeman_test to test the fitness of this model by calculating of the r square as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cfa038c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.23e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:45:28</td>     <th>  Log-Likelihood:    </th> <td> -1738.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3603.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   337</td>      <th>  BIC:               </th> <td>   3855.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    62</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td>  521.5715</td> <td>  130.273</td> <td>    4.004</td> <td> 0.000</td> <td>  265.322</td> <td>  777.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                                                <td>   -6.1179</td> <td>    2.846</td> <td>   -2.150</td> <td> 0.032</td> <td>  -11.716</td> <td>   -0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                                           <td>   -8.1938</td> <td>    2.329</td> <td>   -3.518</td> <td> 0.000</td> <td>  -12.775</td> <td>   -3.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                                         <td>-1224.9610</td> <td>  545.105</td> <td>   -2.247</td> <td> 0.025</td> <td>-2297.199</td> <td> -152.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                                          <td>   -6.1989</td> <td>    2.174</td> <td>   -2.851</td> <td> 0.005</td> <td>  -10.475</td> <td>   -1.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]</th>                                        <td> -102.4030</td> <td>   96.565</td> <td>   -1.060</td> <td> 0.290</td> <td> -292.350</td> <td>   87.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense</th>                                                   <td>    0.0985</td> <td>    0.033</td> <td>    2.982</td> <td> 0.003</td> <td>    0.034</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]</th>                                 <td>   14.6361</td> <td>    6.267</td> <td>    2.336</td> <td> 0.020</td> <td>    2.310</td> <td>   26.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                                            <td>   -7.2261</td> <td>    2.178</td> <td>   -3.318</td> <td> 0.001</td> <td>  -11.511</td> <td>   -2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]</th>                                          <td>  704.8798</td> <td>  337.855</td> <td>    2.086</td> <td> 0.038</td> <td>   40.309</td> <td> 1369.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                                                     <td>    0.1264</td> <td>    0.038</td> <td>    3.351</td> <td> 0.001</td> <td>    0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]</th>                                   <td>    5.8648</td> <td>    2.692</td> <td>    2.179</td> <td> 0.030</td> <td>    0.570</td> <td>   11.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed</th>                                                    <td>    0.1026</td> <td>    0.039</td> <td>    2.634</td> <td> 0.009</td> <td>    0.026</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]</th>                                  <td>   -6.9266</td> <td>    3.465</td> <td>   -1.999</td> <td> 0.046</td> <td>  -13.742</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed</th>                                             <td>   -0.0016</td> <td>    0.001</td> <td>   -2.837</td> <td> 0.005</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]</th>                           <td>   -0.0743</td> <td>    0.030</td> <td>   -2.477</td> <td> 0.014</td> <td>   -0.133</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                                                     <td>   -5.3982</td> <td>    1.938</td> <td>   -2.785</td> <td> 0.006</td> <td>   -9.211</td> <td>   -1.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\")</th>                                   <td> -282.2496</td> <td>  126.835</td> <td>   -2.225</td> <td> 0.027</td> <td> -531.738</td> <td>  -32.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                                              <td>    0.1094</td> <td>    0.034</td> <td>    3.233</td> <td> 0.001</td> <td>    0.043</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\")</th>                            <td>   12.6503</td> <td>    5.851</td> <td>    2.162</td> <td> 0.031</td> <td>    1.141</td> <td>   24.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\")</th>                                             <td>    0.0628</td> <td>    0.028</td> <td>    2.247</td> <td> 0.025</td> <td>    0.008</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                           <td>    3.3949</td> <td>    1.783</td> <td>    1.904</td> <td> 0.058</td> <td>   -0.112</td> <td>    6.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\")</th>                                      <td>   -0.0012</td> <td>    0.000</td> <td>   -2.730</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                    <td>   -0.1456</td> <td>    0.065</td> <td>   -2.253</td> <td> 0.025</td> <td>   -0.273</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                                               <td>    0.0624</td> <td>    0.031</td> <td>    2.027</td> <td> 0.043</td> <td>    0.002</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                             <td>   -3.2219</td> <td>    1.983</td> <td>   -1.625</td> <td> 0.105</td> <td>   -7.122</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>                                        <td>   -0.0014</td> <td>    0.001</td> <td>   -2.732</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                      <td>   -0.0695</td> <td>    0.033</td> <td>   -2.100</td> <td> 0.036</td> <td>   -0.135</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\")</th>                                       <td>   -0.0008</td> <td>    0.000</td> <td>   -1.743</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                     <td>    0.0334</td> <td>    0.021</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\")</th>                                <td> 1.629e-05</td> <td> 6.92e-06</td> <td>    2.355</td> <td> 0.019</td> <td> 2.68e-06</td> <td> 2.99e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>              <td>    0.0008</td> <td>    0.000</td> <td>    2.433</td> <td> 0.015</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                                                     <td>   -8.3636</td> <td>    2.346</td> <td>   -3.565</td> <td> 0.000</td> <td>  -12.978</td> <td>   -3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Atk\")</th>                                   <td>  850.5436</td> <td>  385.064</td> <td>    2.209</td> <td> 0.028</td> <td>   93.112</td> <td> 1607.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                                              <td>    0.1388</td> <td>    0.040</td> <td>    3.500</td> <td> 0.001</td> <td>    0.061</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Atk\")</th>                            <td>    2.1809</td> <td>    1.136</td> <td>    1.920</td> <td> 0.056</td> <td>   -0.054</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Atk\")</th>                                             <td>    0.0831</td> <td>    0.038</td> <td>    2.162</td> <td> 0.031</td> <td>    0.007</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                           <td>   -7.3121</td> <td>    3.376</td> <td>   -2.166</td> <td> 0.031</td> <td>  -13.953</td> <td>   -0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Atk\")</th>                                      <td>   -0.0014</td> <td>    0.001</td> <td>   -2.480</td> <td> 0.014</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                    <td>   -0.0434</td> <td>    0.022</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                                               <td>    0.1011</td> <td>    0.035</td> <td>    2.872</td> <td> 0.004</td> <td>    0.032</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                             <td>  -12.6343</td> <td>    5.613</td> <td>   -2.251</td> <td> 0.025</td> <td>  -23.674</td> <td>   -1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>                                        <td>   -0.0018</td> <td>    0.001</td> <td>   -3.102</td> <td> 0.002</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                      <td>    0.0151</td> <td>    0.009</td> <td>    1.609</td> <td> 0.109</td> <td>   -0.003</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Atk\")</th>                                       <td>   -0.0012</td> <td>    0.001</td> <td>   -1.860</td> <td> 0.064</td> <td>   -0.002</td> <td> 6.62e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                     <td>    0.1210</td> <td>    0.054</td> <td>    2.260</td> <td> 0.024</td> <td>    0.016</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Atk\")</th>                                <td> 2.125e-05</td> <td>  9.1e-06</td> <td>    2.334</td> <td> 0.020</td> <td> 3.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>              <td> 6.438e-06</td> <td> 7.69e-05</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                        <td>    0.1265</td> <td>    0.033</td> <td>    3.821</td> <td> 0.000</td> <td>    0.061</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                      <td>   -5.0544</td> <td>    2.506</td> <td>   -2.017</td> <td> 0.044</td> <td>   -9.983</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                 <td>   -0.0021</td> <td>    0.001</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>               <td>   -0.0346</td> <td>    0.017</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.069</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                <td>   -0.0012</td> <td>    0.000</td> <td>   -2.406</td> <td> 0.017</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0446</td> <td>    0.025</td> <td>    1.794</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                         <td> 1.973e-05</td> <td> 7.28e-06</td> <td>    2.710</td> <td> 0.007</td> <td> 5.41e-06</td> <td>  3.4e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.957</td> <td> 0.051</td> <td>-2.56e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                  <td>   -0.0013</td> <td>    0.000</td> <td>   -2.740</td> <td> 0.006</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                <td>    0.0841</td> <td>    0.040</td> <td>    2.125</td> <td> 0.034</td> <td>    0.006</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                           <td> 2.379e-05</td> <td> 7.85e-06</td> <td>    3.030</td> <td> 0.003</td> <td> 8.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>         <td> 2.864e-05</td> <td> 7.73e-05</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                          <td> 1.284e-05</td> <td> 7.46e-06</td> <td>    1.721</td> <td> 0.086</td> <td>-1.83e-06</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0008</td> <td>    0.000</td> <td>   -2.085</td> <td> 0.038</td> <td>   -0.002</td> <td>-4.68e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                   <td> -2.53e-07</td> <td>  1.1e-07</td> <td>   -2.292</td> <td> 0.023</td> <td> -4.7e-07</td> <td>-3.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>-1.425e-06</td> <td> 1.14e-06</td> <td>   -1.249</td> <td> 0.212</td> <td>-3.67e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.2e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                   &        HP        & \\textbf{  R-squared:         } &     0.467   \\\\\n",
       "\\textbf{Model:}                                                           &       OLS        & \\textbf{  Adj. R-squared:    } &     0.369   \\\\\n",
       "\\textbf{Method:}                                                          &  Least Squares   & \\textbf{  F-statistic:       } &     4.764   \\\\\n",
       "\\textbf{Date:}                                                            & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.23e-21   \\\\\n",
       "\\textbf{Time:}                                                            &     01:45:28     & \\textbf{  Log-Likelihood:    } &   -1738.6   \\\\\n",
       "\\textbf{No. Observations:}                                                &         400      & \\textbf{  AIC:               } &     3603.   \\\\\n",
       "\\textbf{Df Residuals:}                                                    &         337      & \\textbf{  BIC:               } &     3855.   \\\\\n",
       "\\textbf{Df Model:}                                                        &          62      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                                 &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                        &     521.5715  &      130.273     &     4.004  &         0.000        &      265.322    &      777.821     \\\\\n",
       "\\textbf{Legendary[T.True]}                                                &      -6.1179  &        2.846     &    -2.150  &         0.032        &      -11.716    &       -0.520     \\\\\n",
       "\\textbf{Attack}                                                           &      -8.1938  &        2.329     &    -3.518  &         0.000        &      -12.775    &       -3.612     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                                         &   -1224.9610  &      545.105     &    -2.247  &         0.025        &    -2297.199    &     -152.723     \\\\\n",
       "\\textbf{Defense}                                                          &      -6.1989  &        2.174     &    -2.851  &         0.005        &      -10.475    &       -1.923     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]}                                        &    -102.4030  &       96.565     &    -1.060  &         0.290        &     -292.350    &       87.544     \\\\\n",
       "\\textbf{Attack:Defense}                                                   &       0.0985  &        0.033     &     2.982  &         0.003        &        0.034    &        0.164     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]}                                 &      14.6361  &        6.267     &     2.336  &         0.020        &        2.310    &       26.963     \\\\\n",
       "\\textbf{Speed}                                                            &      -7.2261  &        2.178     &    -3.318  &         0.001        &      -11.511    &       -2.942     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]}                                          &     704.8798  &      337.855     &     2.086  &         0.038        &       40.309    &     1369.450     \\\\\n",
       "\\textbf{Attack:Speed}                                                     &       0.1264  &        0.038     &     3.351  &         0.001        &        0.052    &        0.201     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]}                                   &       5.8648  &        2.692     &     2.179  &         0.030        &        0.570    &       11.160     \\\\\n",
       "\\textbf{Defense:Speed}                                                    &       0.1026  &        0.039     &     2.634  &         0.009        &        0.026    &        0.179     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]}                                  &      -6.9266  &        3.465     &    -1.999  &         0.046        &      -13.742    &       -0.111     \\\\\n",
       "\\textbf{Attack:Defense:Speed}                                             &      -0.0016  &        0.001     &    -2.837  &         0.005        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]}                           &      -0.0743  &        0.030     &    -2.477  &         0.014        &       -0.133    &       -0.015     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                                                     &      -5.3982  &        1.938     &    -2.785  &         0.006        &       -9.211    &       -1.586     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\")}                                   &    -282.2496  &      126.835     &    -2.225  &         0.027        &     -531.738    &      -32.761     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                                              &       0.1094  &        0.034     &     3.233  &         0.001        &        0.043    &        0.176     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\")}                            &      12.6503  &        5.851     &     2.162  &         0.031        &        1.141    &       24.160     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\")}                                             &       0.0628  &        0.028     &     2.247  &         0.025        &        0.008    &        0.118     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\")}                           &       3.3949  &        1.783     &     1.904  &         0.058        &       -0.112    &        6.902     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\")}                                      &      -0.0012  &        0.000     &    -2.730  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")}                    &      -0.1456  &        0.065     &    -2.253  &         0.025        &       -0.273    &       -0.018     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                                               &       0.0624  &        0.031     &     2.027  &         0.043        &        0.002    &        0.123     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\")}                             &      -3.2219  &        1.983     &    -1.625  &         0.105        &       -7.122    &        0.678     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}                                        &      -0.0014  &        0.001     &    -2.732  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                      &      -0.0695  &        0.033     &    -2.100  &         0.036        &       -0.135    &       -0.004     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\")}                                       &      -0.0008  &        0.000     &    -1.743  &         0.082        &       -0.002    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                     &       0.0334  &        0.021     &     1.569  &         0.117        &       -0.008    &        0.075     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\")}                                &    1.629e-05  &     6.92e-06     &     2.355  &         0.019        &     2.68e-06    &     2.99e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}              &       0.0008  &        0.000     &     2.433  &         0.015        &        0.000    &        0.001     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                                                     &      -8.3636  &        2.346     &    -3.565  &         0.000        &      -12.978    &       -3.749     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Atk\")}                                   &     850.5436  &      385.064     &     2.209  &         0.028        &       93.112    &     1607.975     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                                              &       0.1388  &        0.040     &     3.500  &         0.001        &        0.061    &        0.217     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Atk\")}                            &       2.1809  &        1.136     &     1.920  &         0.056        &       -0.054    &        4.416     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Atk\")}                                             &       0.0831  &        0.038     &     2.162  &         0.031        &        0.007    &        0.159     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                           &      -7.3121  &        3.376     &    -2.166  &         0.031        &      -13.953    &       -0.671     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Atk\")}                                      &      -0.0014  &        0.001     &    -2.480  &         0.014        &       -0.003    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                    &      -0.0434  &        0.022     &    -2.010  &         0.045        &       -0.086    &       -0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                                               &       0.1011  &        0.035     &     2.872  &         0.004        &        0.032    &        0.170     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                             &     -12.6343  &        5.613     &    -2.251  &         0.025        &      -23.674    &       -1.594     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}                                        &      -0.0018  &        0.001     &    -3.102  &         0.002        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                      &       0.0151  &        0.009     &     1.609  &         0.109        &       -0.003    &        0.034     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Atk\")}                                       &      -0.0012  &        0.001     &    -1.860  &         0.064        &       -0.002    &     6.62e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                     &       0.1210  &        0.054     &     2.260  &         0.024        &        0.016    &        0.226     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Atk\")}                                &    2.125e-05  &      9.1e-06     &     2.334  &         0.020        &     3.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}              &    6.438e-06  &     7.69e-05     &     0.084  &         0.933        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                        &       0.1265  &        0.033     &     3.821  &         0.000        &        0.061    &        0.192     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                      &      -5.0544  &        2.506     &    -2.017  &         0.044        &       -9.983    &       -0.126     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                 &      -0.0021  &        0.001     &    -3.606  &         0.000        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}               &      -0.0346  &        0.017     &    -1.992  &         0.047        &       -0.069    &       -0.000     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                &      -0.0012  &        0.000     &    -2.406  &         0.017        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0446  &        0.025     &     1.794  &         0.074        &       -0.004    &        0.093     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                         &    1.973e-05  &     7.28e-06     &     2.710  &         0.007        &     5.41e-06    &      3.4e-05     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &       0.0005  &        0.000     &     1.957  &         0.051        &    -2.56e-06    &        0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                  &      -0.0013  &        0.000     &    -2.740  &         0.006        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                &       0.0841  &        0.040     &     2.125  &         0.034        &        0.006    &        0.162     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                           &    2.379e-05  &     7.85e-06     &     3.030  &         0.003        &     8.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}         &    2.864e-05  &     7.73e-05     &     0.370  &         0.711        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                          &    1.284e-05  &     7.46e-06     &     1.721  &         0.086        &    -1.83e-06    &     2.75e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0008  &        0.000     &    -2.085  &         0.038        &       -0.002    &    -4.68e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                   &    -2.53e-07  &      1.1e-07     &    -2.292  &         0.023        &     -4.7e-07    &    -3.59e-08     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &   -1.425e-06  &     1.14e-06     &    -1.249  &         0.212        &    -3.67e-06    &     8.19e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.2e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.467\n",
       "Model:                            OLS   Adj. R-squared:                  0.369\n",
       "Method:                 Least Squares   F-statistic:                     4.764\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           4.23e-21\n",
       "Time:                        01:45:28   Log-Likelihood:                -1738.6\n",
       "No. Observations:                 400   AIC:                             3603.\n",
       "Df Residuals:                     337   BIC:                             3855.\n",
       "Df Model:                          62                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                          521.5715    130.273      4.004      0.000     265.322     777.821\n",
       "Legendary[T.True]                                                   -6.1179      2.846     -2.150      0.032     -11.716      -0.520\n",
       "Attack                                                              -8.1938      2.329     -3.518      0.000     -12.775      -3.612\n",
       "Attack:Legendary[T.True]                                         -1224.9610    545.105     -2.247      0.025   -2297.199    -152.723\n",
       "Defense                                                             -6.1989      2.174     -2.851      0.005     -10.475      -1.923\n",
       "Defense:Legendary[T.True]                                         -102.4030     96.565     -1.060      0.290    -292.350      87.544\n",
       "Attack:Defense                                                       0.0985      0.033      2.982      0.003       0.034       0.164\n",
       "Attack:Defense:Legendary[T.True]                                    14.6361      6.267      2.336      0.020       2.310      26.963\n",
       "Speed                                                               -7.2261      2.178     -3.318      0.001     -11.511      -2.942\n",
       "Speed:Legendary[T.True]                                            704.8798    337.855      2.086      0.038      40.309    1369.450\n",
       "Attack:Speed                                                         0.1264      0.038      3.351      0.001       0.052       0.201\n",
       "Attack:Speed:Legendary[T.True]                                       5.8648      2.692      2.179      0.030       0.570      11.160\n",
       "Defense:Speed                                                        0.1026      0.039      2.634      0.009       0.026       0.179\n",
       "Defense:Speed:Legendary[T.True]                                     -6.9266      3.465     -1.999      0.046     -13.742      -0.111\n",
       "Attack:Defense:Speed                                                -0.0016      0.001     -2.837      0.005      -0.003      -0.001\n",
       "Attack:Defense:Speed:Legendary[T.True]                              -0.0743      0.030     -2.477      0.014      -0.133      -0.015\n",
       "Q(\"Sp. Def\")                                                        -5.3982      1.938     -2.785      0.006      -9.211      -1.586\n",
       "Legendary[T.True]:Q(\"Sp. Def\")                                    -282.2496    126.835     -2.225      0.027    -531.738     -32.761\n",
       "Attack:Q(\"Sp. Def\")                                                  0.1094      0.034      3.233      0.001       0.043       0.176\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\")                               12.6503      5.851      2.162      0.031       1.141      24.160\n",
       "Defense:Q(\"Sp. Def\")                                                 0.0628      0.028      2.247      0.025       0.008       0.118\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\")                               3.3949      1.783      1.904      0.058      -0.112       6.902\n",
       "Attack:Defense:Q(\"Sp. Def\")                                         -0.0012      0.000     -2.730      0.007      -0.002      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")                       -0.1456      0.065     -2.253      0.025      -0.273      -0.018\n",
       "Speed:Q(\"Sp. Def\")                                                   0.0624      0.031      2.027      0.043       0.002       0.123\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\")                                -3.2219      1.983     -1.625      0.105      -7.122       0.678\n",
       "Attack:Speed:Q(\"Sp. Def\")                                           -0.0014      0.001     -2.732      0.007      -0.002      -0.000\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         -0.0695      0.033     -2.100      0.036      -0.135      -0.004\n",
       "Defense:Speed:Q(\"Sp. Def\")                                          -0.0008      0.000     -1.743      0.082      -0.002       0.000\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         0.0334      0.021      1.569      0.117      -0.008       0.075\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\")                                 1.629e-05   6.92e-06      2.355      0.019    2.68e-06    2.99e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                  0.0008      0.000      2.433      0.015       0.000       0.001\n",
       "Q(\"Sp. Atk\")                                                        -8.3636      2.346     -3.565      0.000     -12.978      -3.749\n",
       "Legendary[T.True]:Q(\"Sp. Atk\")                                     850.5436    385.064      2.209      0.028      93.112    1607.975\n",
       "Attack:Q(\"Sp. Atk\")                                                  0.1388      0.040      3.500      0.001       0.061       0.217\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Atk\")                                2.1809      1.136      1.920      0.056      -0.054       4.416\n",
       "Defense:Q(\"Sp. Atk\")                                                 0.0831      0.038      2.162      0.031       0.007       0.159\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Atk\")                              -7.3121      3.376     -2.166      0.031     -13.953      -0.671\n",
       "Attack:Defense:Q(\"Sp. Atk\")                                         -0.0014      0.001     -2.480      0.014      -0.003      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")                       -0.0434      0.022     -2.010      0.045      -0.086      -0.001\n",
       "Speed:Q(\"Sp. Atk\")                                                   0.1011      0.035      2.872      0.004       0.032       0.170\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Atk\")                               -12.6343      5.613     -2.251      0.025     -23.674      -1.594\n",
       "Attack:Speed:Q(\"Sp. Atk\")                                           -0.0018      0.001     -3.102      0.002      -0.003      -0.001\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                          0.0151      0.009      1.609      0.109      -0.003       0.034\n",
       "Defense:Speed:Q(\"Sp. Atk\")                                          -0.0012      0.001     -1.860      0.064      -0.002    6.62e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                         0.1210      0.054      2.260      0.024       0.016       0.226\n",
       "Attack:Defense:Speed:Q(\"Sp. Atk\")                                 2.125e-05    9.1e-06      2.334      0.020    3.34e-06    3.92e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")               6.438e-06   7.69e-05      0.084      0.933      -0.000       0.000\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                            0.1265      0.033      3.821      0.000       0.061       0.192\n",
       "Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                         -5.0544      2.506     -2.017      0.044      -9.983      -0.126\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                    -0.0021      0.001     -3.606      0.000      -0.003      -0.001\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  -0.0346      0.017     -1.992      0.047      -0.069      -0.000\n",
       "Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                   -0.0012      0.000     -2.406      0.017      -0.002      -0.000\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0446      0.025      1.794      0.074      -0.004       0.093\n",
       "Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                          1.973e-05   7.28e-06      2.710      0.007    5.41e-06     3.4e-05\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           0.0005      0.000      1.957      0.051   -2.56e-06       0.001\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                     -0.0013      0.000     -2.740      0.006      -0.002      -0.000\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    0.0841      0.040      2.125      0.034       0.006       0.162\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                            2.379e-05   7.85e-06      3.030      0.003    8.34e-06    3.92e-05\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          2.864e-05   7.73e-05      0.370      0.711      -0.000       0.000\n",
       "Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                           1.284e-05   7.46e-06      1.721      0.086   -1.83e-06    2.75e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0008      0.000     -2.085      0.038      -0.002   -4.68e-05\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    -2.53e-07    1.1e-07     -2.292      0.023    -4.7e-07   -3.59e-08\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\") -1.425e-06   1.14e-06     -1.249      0.212   -3.67e-06    8.19e-07\n",
       "==============================================================================\n",
       "Omnibus:                      214.307   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2354.664\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.174   Cond. No.                     1.20e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.2e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712d6ed",
   "metadata": {},
   "source": [
    "Now we construct a second model_4, where there are 6 main effects( 6 mian variables) Attack * Defense * Speed * Legendary' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'. Since it is using * now, it also indicates the interaction between those variables are the permutations and combinations of thos main variables. \n",
    "Then again using smf.ols to fit a new model with the formual above and data pokeaman_train data.\n",
    "Print the summary table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d225cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c4b85",
   "metadata": {},
   "source": [
    "Again, we print the model 4 'In sample' R-squared by using the pokeaman_train data, and 'Out of sample' R-squared' by pokeman _ test data. as a result, we find that the in sample fitness is much more better than the out of sample fitness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3de47",
   "metadata": {},
   "source": [
    "Here's a summary of our conversation:\n",
    "\n",
    "1. **Data Analysis and Model Fitting**:\n",
    "   - We discussed your use of a dataset (`pokeaman`) and how you split it into training and test sets using `train_test_split`. \n",
    "   - You worked on building multiple regression models using `statsmodels` (like `smf.ols`) to predict `HP` based on various variables like `Attack`, `Defense`, `Speed`, `Legendary`, and others, including interactions between these variables.\n",
    "   - We also talked about how to interpret the regression formula, where the `*` operator not only adds main effects but also includes interaction terms between variables.\n",
    "   \n",
    "2. **Mathematical Model Explanation**:\n",
    "   - The equation of the regression model was explained, showing how the main effects (predictors) and interactions between those predictors contribute to the prediction of `HP`.\n",
    "\n",
    "3. **Variables and Interactions**:\n",
    "   - You clarified that the use of `*` in the formula includes both the main effects and the interaction terms between the variables.\n",
    "   - You also asked about the total number of variables, including both main effects and interactions, which are far more than the 6 original variables due to the exponential increase in interactions as more variables are added.\n",
    "\n",
    "4. **Translation of \"\"**:\n",
    "   - You asked about the English translation of \",\" which is **\"permutations and combinations\"**. This refers to the mathematical concepts of arranging items in order and selecting items without regard to order, respectively.\n",
    "\n",
    "https://chatgpt.com/share/67357d4a-9250-8003-8222-4f9c32743a4e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecc5b7",
   "metadata": {},
   "source": [
    "### 6. Work with a ChatBot to understand how the *model4_linear_form* (*linear form* specification of  *model4*) creates new *predictor variables* as the columns of the so-called \"design matrix\" *model4_spec.exog* (*model4_spec.exog.shape*) used to predict the *outcome variable*  *model4_spec.endog* and why the so-called *multicollinearity* in this \"design matrix\" (observed in *np.corrcoef(model4_spec.exog)*) contribues to the lack of \"out of sample\" *generalization* of *predictions* from *model4_fit*; then, explain this consisely in your own works<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The **overfitting** observed in the previous problem is a question of **model complexity** relative to the amount of information or evidence available in a given dataset (or we could just say \"the amount of data in a dataset\"). The **model fit** for  `model4` resulted in an **overfit model** because the specification of its **linear form** was overly complex (relative to the the amount of available data). Indeed, `model4` is rediculously complex as can be seen from `model4_fit.summary()`. This in turn allowed the **model fit** to \"detect\" idiosyncratic associations spuriously present specifically in the **training** dataset but which did not **generalize** to the **testing** dataset. If a model is too **complex** then it will find and model \"patterns\" in a **training** dataset which are actually just accidental \"noise\" from the random sampling nature of the dataset. The simpler (or more parsimoneous) `model3` on the other hand was able to identify **predictive associations** in the **training** dataset which did **generalize** to the **testing** dataset. This is because `model3` only sought to understand simpler **predictive association** for which there was enough data in the **training** dataset to reliably detect and obviously identify. And these patterns were indeed sufficiently \"real\" in the sense that they were present and **generalized** into the **testing** dataset as well as the **training** dataset. So they could be \"found\" in the **training** and then used in (**generalized** to) the **testing** dataset._\n",
    "> \n",
    "> _This question therefore addresses the topic of the **evidence** a given dataset provides for the **predictive associations** detected by a **fitted model**. It should be increasingly clear at this point that evidence for a model can be addressed using **coefficent hypothesis testing** in the context of **multiple linear regression**, but that examinations of \"in sample\" versus \"out of sample\" **model performance** metrics are what in fact are directly designed to address this question of **generalizability**. That said, this question introduces another consideration of **multicollinearity** as something that affects the **generalizability** of **model fits** in the **multiple linear regression** context. A good question that a ChatBot could help you understand is (a) \"why is **generalizability** more uncertain if two **predictor variables** are highly **correlated**?\" and (b) \"why is **generalizability** more uncertain if multiple **predictor variables** are highly **multicollinear**?\"_\n",
    ">\n",
    "> _The four code cells below are not necessary for answering this question; however, they introduce two very practical helpful tools for the **multiple linear regression** context that are immediately relevant for this question. The first is the so-called **condition number** (of a \"design matrix\") which provides a very simple diagnostic which can serve as a measure the degree of **multicollinearity** that is present in a **model fit**. If this number is \"very large\" then there is a large degree of **multicollinearity** and suggests room for doubt regarding the **generalizability** of the **fitted model**. The second tool(s) are the `center` and `scale` functions. It is best practice to \"center and scale\" **continuous predictor variables** (but not **indicator variables**) in the **multiple linear regression** context as is done below. While \"centering and scaling\" does make interpreting the predictions on the original scale of the data slighly more complicated, it also must be done in order to get a \"true\" evaluation of the degree of **multicollinearity** present in a **model fit** using the **condition number** of the model (\"design matrix\"). The examples below show that the **condition number** reported by a **fitted model** are \"artificially inflacted\" if \"centering and scaling\" is not used. Specically, they show that the **condition number** of `model3_fit` is really `1.66` (as opposed to the \"very large\" `343` which is reported without \"centering and scaling\"); whereas, the **condition number** for `model4_fit` is \"very (VERY) large\" irrespective of \"centering and scaling\", showing that the overwheling presense of **multicollinearity** in `model4_fit` is in fact a very real thing.  Indeed, we have already seen that `model4_fit` is grossly **overfit** and does not remotely **generalize** beyond its **training** dataset. Without knowing this, however, the comically large **condition number** for `model4_fit` (after \"centering and scaling\") makes it abundantly clear that we should have great doubts about the likely **generalizability** of `model4_fit` (even without examining specific aspects of **multicollinearity** directly or examining \"in sample\" versus \"out of sample\" **model performance** comparisions)._\n",
    ">\n",
    "> - _The \"specific aspects of **multicollinearity**\" reference above refer to understanding and attributing the detrmimental affects of specific **predictor variables** towards **multicollinearity**. This can be done using so-called **variance inflation factors**, but this is beyond the scope of STA130. We should be aware that the presence of excessive **multicollinearity** as indicated by a large **condition number** for a (\"centered and scaled\") **fitted model** raises grave concerns regarding the potential **generalizability** of the model._\n",
    ">\n",
    "> _The `np.corrcoef(model4_spec.exog)` examination of the **correlations** of a \"design matrix\" considered in ths problems prompt is analogous to the examination of the **correlations** present in a dataset that might considered when initially examining the **predictor variables** of a dataset, such as `pokeaman.iloc[:,4:12].corr()`. Indeed, such an examination is often the first step in examining the potential presence of **multicollinearity** among the **predictor variables** of a dataset. However, these are consideration of **pairwise correlation**, whereas **multicollinearity** generalizes this notion to the full collection of **predictor variables** together. A **condition number** for a \"centered and scale\" version of a **fit model** can therefore be viewed as serving the analogous purposes of a multivariate generalization of **pairwise correlation**._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66dd4227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:45:29</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     01:45:29     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        01:45:29   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dedf01b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:45:29</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>              <td>   69.3025</td> <td>    1.186</td> <td>   58.439</td> <td> 0.000</td> <td>   66.971</td> <td>   71.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Attack))</th>  <td>    8.1099</td> <td>    1.340</td> <td>    6.051</td> <td> 0.000</td> <td>    5.475</td> <td>   10.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Defense))</th> <td>    2.9496</td> <td>    1.340</td> <td>    2.201</td> <td> 0.028</td> <td>    0.315</td> <td>    5.585</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    1.66</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}         &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}                &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}                  & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}                  &     01:45:29     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:}      &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}          &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}              &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}              &      69.3025  &        1.186     &    58.439  &         0.000        &       66.971    &       71.634     \\\\\n",
       "\\textbf{scale(center(Attack))}  &       8.1099  &        1.340     &     6.051  &         0.000        &        5.475    &       10.745     \\\\\n",
       "\\textbf{scale(center(Defense))} &       2.9496  &        1.340     &     2.201  &         0.028        &        0.315    &        5.585     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     1.66  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        01:45:29   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Intercept                 69.3025      1.186     58.439      0.000      66.971      71.634\n",
       "scale(center(Attack))      8.1099      1.340      6.051      0.000       5.475      10.745\n",
       "scale(center(Defense))     2.9496      1.340      2.201      0.028       0.315       5.585\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         1.66\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patsy import center, scale\n",
    "\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))' \n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb64e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.54e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.663  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.54e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary' \n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "\n",
    "# The condition number is still bad even after centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f19d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just as the condition number was very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05544d24",
   "metadata": {},
   "source": [
    "design matrix is the form that stores predicted variables from the data, later for stats models more easily to process them ,each column here, represents the predicted variables and also their interactions. \n",
    "BY using the code import statsmodels.formula.api as smf and define the model model4_CS_spec by smf.ols(formula=model4_linear_form_CS, data=pokeaman_train), the stats model will reads the formula and builds the design matrix (exog) based on those instructions from stas pokeaman_train, stroing it in model4_CS_spec.exog(below is how we can access the design matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc873789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 64)\n"
     ]
    }
   ],
   "source": [
    "# Access the design matrix directly\n",
    "design_matrix = model4_CS_spec.exog\n",
    "print(design_matrix.shape)  # Displays the shape of the design matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3f4b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the code above, when we have the interactions of two variables, such  'HP ~ scale(center(Attack)) * scale(center(Defense))' and so on, it will appear multicollinearity, where complexity will be increased, like attack * Defense, also there will be more redundant information, carrying overlapping content. For example, Attack and Defense have seperately variables in the expression, while there appears a new one variable called \"attack* defense\", so there will be a overlapping information regarding those two variables, resulting in redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67275915",
   "metadata": {},
   "source": [
    "Here's a summary of our conversation:\n",
    "\n",
    "1. **Design Matrix**: We discussed that a **design matrix** is a matrix that represents the values of predictor variables used in a regression model. It is created automatically when you define and fit a model using `statsmodels` (like `smf.ols`). In your code, the design matrix is accessed through `model4_CS_spec.exog`.\n",
    "\n",
    "2. **Multicollinearity**: We explored **multicollinearity**, which occurs when predictor variables in the design matrix are highly correlated. This makes it difficult for the model to estimate the individual effects of each predictor. In your code, multicollinearity arises due to the interaction terms (`*` operator) and transformations (e.g., `scale(center(...))`), which create new variables that are highly correlated with the original ones.\n",
    "\n",
    "3. **Impact on Model Generalization**: We discussed how multicollinearity contributes to the **lack of out-of-sample generalization**. It causes **unstable coefficient estimates**, leading to overfitting, where the model learns the noise in the training data rather than general patterns. This results in poor predictions on new, unseen data.\n",
    "\n",
    "4. **Interaction Terms**: We clarified that interaction terms (e.g., `Attack * Defense`) can create multicollinearity by introducing variables that are closely related to the original predictors. This increases the correlation between predictor variables, further amplifying multicollinearity.\n",
    "\n",
    "In summary, interactions, transformations, and complex relationships between predictors in your model contribute to multicollinearity, which in turn hinders the model's ability to generalize to new data.\n",
    "\n",
    "https://chatgpt.com/share/6736ab14-3e18-8003-ac96-33fa4265b60f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577c7dd",
   "metadata": {},
   "source": [
    "### 7. Discuss with a ChatBot the rationale and principles by which *model5_linear_form* is  extended and developed from *model3_fit* and *model4_fit*; *model6_linear_form* is  extended and developed from *model5_linear_form*; and *model7_linear_form* is  extended and developed from *model6_linear_form*; then, explain this breifly and consisely in your own words<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _We again include the **condition number** for the \"centered and scaled\" version of `model7_fit` to show that **multicollinearity** does not appear to be a major concern for this model (and the same would be true regarding `model6_fit` if the analogous \"centered and scaled\" version of the model was considered). While it is true that the **condition number** of `15.4` observed for `model7_fit` is perhaps \"large\", this would not be considered \"vary large\"._\n",
    ">\n",
    "> - _Regarding **condition numbers**, a ChatBot gave me cutoffs of `<30` not a big problem, up to `<300` maybe an issue, up to `<1000` definitely **multicollinearity**, and beyond that is pretty much likely to be \"serious\" problems with **multicollinearity**. Personally, cutoffs around `10`, `100`, and `1000` seem about right to me._\n",
    ">\n",
    "> _This question addresses the **model building** exercise using both an **evidence** based approach using **coefficient hypothesis testing** as well as examinations of **generalizability** using comparisions of \"in sample\" versus \"out of sample\" **model performance** metrics. Through these tools, different models were considered, extended, and developed, finally arriving at `model7_fit`. When we feel we can improve the **model performance** in a **generalizable** manner, then all relatively underperforming models are said to be **underfit**, meaning that they do not leverage all the **predictive associations** available to improve **predictions**._\n",
    "> \n",
    "> _While the previous \"Question 6\" above introduced and explored the impact of **multicollinearity** in the **multiple linear regression** context_ \n",
    ">     \n",
    "> - _(whereby \"the effects\" of multiple **predictor variables** are \"tangled up\" and therefore do not allow the model to reliably determine contribution attributions between the **predictor variables**, which potentially leads to poor **estimation** of their \"effects\" in the model, which in turn is the problematic state of affairs which leads to a lack of **generalizability** in such high **multicollinearity** settings)_\n",
    "> \n",
    "> _there is still the (actually even more important) consideration of the actual **evidence** of **predictive associations**. The question is whether or not there is sufficient **evidence** in the data backing up the **estimated** fit of the **linear form** specification. Quantifying the **evidence** for a **estimated** model is a separate question from the problem of **multicollinearity**, the assessment of which is actually the primary purpose of **multiple linear regression** methodology._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5486d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 14 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>9.48e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:59:10</td>     <th>  Log-Likelihood:    </th> <td> -1765.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3624.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   353</td>      <th>  BIC:               </th> <td>   3812.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    46</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                  <td>   10.1046</td> <td>   14.957</td> <td>    0.676</td> <td> 0.500</td> <td>  -19.312</td> <td>   39.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>          <td>   -3.2717</td> <td>    4.943</td> <td>   -0.662</td> <td> 0.508</td> <td>  -12.992</td> <td>    6.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>         <td>    9.2938</td> <td>    4.015</td> <td>    2.315</td> <td> 0.021</td> <td>    1.398</td> <td>   17.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>         <td>    2.3150</td> <td>    3.915</td> <td>    0.591</td> <td> 0.555</td> <td>   -5.385</td> <td>   10.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>         <td>    4.8353</td> <td>    4.149</td> <td>    1.165</td> <td> 0.245</td> <td>   -3.325</td> <td>   12.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>         <td>   11.4838</td> <td>    3.960</td> <td>    2.900</td> <td> 0.004</td> <td>    3.696</td> <td>   19.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>         <td>    4.9206</td> <td>    4.746</td> <td>    1.037</td> <td> 0.300</td> <td>   -4.413</td> <td>   14.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Dark]</th>     <td>   -1.4155</td> <td>    6.936</td> <td>   -0.204</td> <td> 0.838</td> <td>  -15.057</td> <td>   12.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Dragon]</th>   <td>    0.8509</td> <td>    6.900</td> <td>    0.123</td> <td> 0.902</td> <td>  -12.720</td> <td>   14.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Electric]</th> <td>   -6.3641</td> <td>    6.537</td> <td>   -0.974</td> <td> 0.331</td> <td>  -19.220</td> <td>    6.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fairy]</th>    <td>   -1.9486</td> <td>   10.124</td> <td>   -0.192</td> <td> 0.847</td> <td>  -21.859</td> <td>   17.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fighting]</th> <td>    7.0308</td> <td>    7.432</td> <td>    0.946</td> <td> 0.345</td> <td>   -7.586</td> <td>   21.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fire]</th>     <td>    3.0779</td> <td>    6.677</td> <td>    0.461</td> <td> 0.645</td> <td>  -10.055</td> <td>   16.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Flying]</th>   <td>   -2.1231</td> <td>   22.322</td> <td>   -0.095</td> <td> 0.924</td> <td>  -46.025</td> <td>   41.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ghost]</th>    <td>    5.7343</td> <td>    8.488</td> <td>    0.676</td> <td> 0.500</td> <td>  -10.960</td> <td>   22.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Grass]</th>    <td>    3.3275</td> <td>    5.496</td> <td>    0.605</td> <td> 0.545</td> <td>   -7.481</td> <td>   14.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ground]</th>   <td>    9.5118</td> <td>    7.076</td> <td>    1.344</td> <td> 0.180</td> <td>   -4.404</td> <td>   23.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ice]</th>      <td>   -0.9313</td> <td>    7.717</td> <td>   -0.121</td> <td> 0.904</td> <td>  -16.108</td> <td>   14.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Normal]</th>   <td>   18.4816</td> <td>    5.312</td> <td>    3.479</td> <td> 0.001</td> <td>    8.034</td> <td>   28.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Poison]</th>   <td>    8.3411</td> <td>    7.735</td> <td>    1.078</td> <td> 0.282</td> <td>   -6.871</td> <td>   23.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Psychic]</th>  <td>    1.8061</td> <td>    6.164</td> <td>    0.293</td> <td> 0.770</td> <td>  -10.317</td> <td>   13.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Rock]</th>     <td>   -3.8558</td> <td>    6.503</td> <td>   -0.593</td> <td> 0.554</td> <td>  -16.645</td> <td>    8.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Steel]</th>    <td>   -4.0053</td> <td>    8.044</td> <td>   -0.498</td> <td> 0.619</td> <td>  -19.826</td> <td>   11.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Water]</th>    <td>    9.7988</td> <td>    5.166</td> <td>    1.897</td> <td> 0.059</td> <td>   -0.361</td> <td>   19.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Dark]</th>     <td>    5.8719</td> <td>   15.185</td> <td>    0.387</td> <td> 0.699</td> <td>  -23.993</td> <td>   35.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Dragon]</th>   <td>   13.2777</td> <td>   14.895</td> <td>    0.891</td> <td> 0.373</td> <td>  -16.016</td> <td>   42.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Electric]</th> <td>   14.3228</td> <td>   17.314</td> <td>    0.827</td> <td> 0.409</td> <td>  -19.728</td> <td>   48.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fairy]</th>    <td>    2.8426</td> <td>   14.268</td> <td>    0.199</td> <td> 0.842</td> <td>  -25.218</td> <td>   30.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fighting]</th> <td>    1.9741</td> <td>   14.089</td> <td>    0.140</td> <td> 0.889</td> <td>  -25.735</td> <td>   29.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fire]</th>     <td>    0.2001</td> <td>   15.730</td> <td>    0.013</td> <td> 0.990</td> <td>  -30.736</td> <td>   31.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Flying]</th>   <td>    6.7292</td> <td>   13.581</td> <td>    0.495</td> <td> 0.621</td> <td>  -19.980</td> <td>   33.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ghost]</th>    <td>  -10.9402</td> <td>   15.895</td> <td>   -0.688</td> <td> 0.492</td> <td>  -42.201</td> <td>   20.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Grass]</th>    <td>    2.5119</td> <td>   14.540</td> <td>    0.173</td> <td> 0.863</td> <td>  -26.084</td> <td>   31.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ground]</th>   <td>   13.6042</td> <td>   13.655</td> <td>    0.996</td> <td> 0.320</td> <td>  -13.250</td> <td>   40.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ice]</th>      <td>   19.7950</td> <td>   15.068</td> <td>    1.314</td> <td> 0.190</td> <td>   -9.840</td> <td>   49.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.None]</th>     <td>    7.6068</td> <td>   13.162</td> <td>    0.578</td> <td> 0.564</td> <td>  -18.279</td> <td>   33.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Normal]</th>   <td>   17.3191</td> <td>   17.764</td> <td>    0.975</td> <td> 0.330</td> <td>  -17.618</td> <td>   52.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Poison]</th>   <td>    0.7770</td> <td>   14.575</td> <td>    0.053</td> <td> 0.958</td> <td>  -27.887</td> <td>   29.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Psychic]</th>  <td>    4.2480</td> <td>   14.174</td> <td>    0.300</td> <td> 0.765</td> <td>  -23.628</td> <td>   32.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Rock]</th>     <td>    6.8858</td> <td>   16.221</td> <td>    0.424</td> <td> 0.671</td> <td>  -25.017</td> <td>   38.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Steel]</th>    <td>  -11.9623</td> <td>   14.973</td> <td>   -0.799</td> <td> 0.425</td> <td>  -41.409</td> <td>   17.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Water]</th>    <td>    5.8097</td> <td>   14.763</td> <td>    0.394</td> <td> 0.694</td> <td>  -23.225</td> <td>   34.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                     <td>    0.2508</td> <td>    0.051</td> <td>    4.940</td> <td> 0.000</td> <td>    0.151</td> <td>    0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                    <td>   -0.0096</td> <td>    0.060</td> <td>   -0.160</td> <td> 0.873</td> <td>   -0.127</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                      <td>   -0.1538</td> <td>    0.051</td> <td>   -2.998</td> <td> 0.003</td> <td>   -0.255</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>               <td>    0.3484</td> <td>    0.059</td> <td>    5.936</td> <td> 0.000</td> <td>    0.233</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>               <td>    0.1298</td> <td>    0.051</td> <td>    2.525</td> <td> 0.012</td> <td>    0.029</td> <td>    0.231</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>286.476</td> <th>  Durbin-Watson:     </th> <td>   1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5187.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.807</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>19.725</td>  <th>  Cond. No.          </th> <td>9.21e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 9.21e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}             &        HP        & \\textbf{  R-squared:         } &     0.392   \\\\\n",
       "\\textbf{Model:}                     &       OLS        & \\textbf{  Adj. R-squared:    } &     0.313   \\\\\n",
       "\\textbf{Method:}                    &  Least Squares   & \\textbf{  F-statistic:       } &     4.948   \\\\\n",
       "\\textbf{Date:}                      & Thu, 14 Nov 2024 & \\textbf{  Prob (F-statistic):} &  9.48e-19   \\\\\n",
       "\\textbf{Time:}                      &     01:59:10     & \\textbf{  Log-Likelihood:    } &   -1765.0   \\\\\n",
       "\\textbf{No. Observations:}          &         400      & \\textbf{  AIC:               } &     3624.   \\\\\n",
       "\\textbf{Df Residuals:}              &         353      & \\textbf{  BIC:               } &     3812.   \\\\\n",
       "\\textbf{Df Model:}                  &          46      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}           &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                  &      10.1046  &       14.957     &     0.676  &         0.500        &      -19.312    &       39.521     \\\\\n",
       "\\textbf{Legendary[T.True]}          &      -3.2717  &        4.943     &    -0.662  &         0.508        &      -12.992    &        6.449     \\\\\n",
       "\\textbf{C(Generation)[T.2]}         &       9.2938  &        4.015     &     2.315  &         0.021        &        1.398    &       17.189     \\\\\n",
       "\\textbf{C(Generation)[T.3]}         &       2.3150  &        3.915     &     0.591  &         0.555        &       -5.385    &       10.015     \\\\\n",
       "\\textbf{C(Generation)[T.4]}         &       4.8353  &        4.149     &     1.165  &         0.245        &       -3.325    &       12.995     \\\\\n",
       "\\textbf{C(Generation)[T.5]}         &      11.4838  &        3.960     &     2.900  &         0.004        &        3.696    &       19.272     \\\\\n",
       "\\textbf{C(Generation)[T.6]}         &       4.9206  &        4.746     &     1.037  &         0.300        &       -4.413    &       14.254     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Dark]}     &      -1.4155  &        6.936     &    -0.204  &         0.838        &      -15.057    &       12.226     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Dragon]}   &       0.8509  &        6.900     &     0.123  &         0.902        &      -12.720    &       14.422     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Electric]} &      -6.3641  &        6.537     &    -0.974  &         0.331        &      -19.220    &        6.491     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fairy]}    &      -1.9486  &       10.124     &    -0.192  &         0.847        &      -21.859    &       17.962     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fighting]} &       7.0308  &        7.432     &     0.946  &         0.345        &       -7.586    &       21.648     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fire]}     &       3.0779  &        6.677     &     0.461  &         0.645        &      -10.055    &       16.210     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Flying]}   &      -2.1231  &       22.322     &    -0.095  &         0.924        &      -46.025    &       41.779     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ghost]}    &       5.7343  &        8.488     &     0.676  &         0.500        &      -10.960    &       22.429     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Grass]}    &       3.3275  &        5.496     &     0.605  &         0.545        &       -7.481    &       14.136     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ground]}   &       9.5118  &        7.076     &     1.344  &         0.180        &       -4.404    &       23.428     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ice]}      &      -0.9313  &        7.717     &    -0.121  &         0.904        &      -16.108    &       14.246     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Normal]}   &      18.4816  &        5.312     &     3.479  &         0.001        &        8.034    &       28.929     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Poison]}   &       8.3411  &        7.735     &     1.078  &         0.282        &       -6.871    &       23.554     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Psychic]}  &       1.8061  &        6.164     &     0.293  &         0.770        &      -10.317    &       13.930     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Rock]}     &      -3.8558  &        6.503     &    -0.593  &         0.554        &      -16.645    &        8.933     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Steel]}    &      -4.0053  &        8.044     &    -0.498  &         0.619        &      -19.826    &       11.816     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Water]}    &       9.7988  &        5.166     &     1.897  &         0.059        &       -0.361    &       19.959     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Dark]}     &       5.8719  &       15.185     &     0.387  &         0.699        &      -23.993    &       35.737     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Dragon]}   &      13.2777  &       14.895     &     0.891  &         0.373        &      -16.016    &       42.571     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Electric]} &      14.3228  &       17.314     &     0.827  &         0.409        &      -19.728    &       48.374     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fairy]}    &       2.8426  &       14.268     &     0.199  &         0.842        &      -25.218    &       30.903     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fighting]} &       1.9741  &       14.089     &     0.140  &         0.889        &      -25.735    &       29.683     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fire]}     &       0.2001  &       15.730     &     0.013  &         0.990        &      -30.736    &       31.136     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Flying]}   &       6.7292  &       13.581     &     0.495  &         0.621        &      -19.980    &       33.438     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ghost]}    &     -10.9402  &       15.895     &    -0.688  &         0.492        &      -42.201    &       20.321     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Grass]}    &       2.5119  &       14.540     &     0.173  &         0.863        &      -26.084    &       31.108     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ground]}   &      13.6042  &       13.655     &     0.996  &         0.320        &      -13.250    &       40.459     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ice]}      &      19.7950  &       15.068     &     1.314  &         0.190        &       -9.840    &       49.430     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.None]}     &       7.6068  &       13.162     &     0.578  &         0.564        &      -18.279    &       33.493     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Normal]}   &      17.3191  &       17.764     &     0.975  &         0.330        &      -17.618    &       52.256     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Poison]}   &       0.7770  &       14.575     &     0.053  &         0.958        &      -27.887    &       29.441     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Psychic]}  &       4.2480  &       14.174     &     0.300  &         0.765        &      -23.628    &       32.124     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Rock]}     &       6.8858  &       16.221     &     0.424  &         0.671        &      -25.017    &       38.788     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Steel]}    &     -11.9623  &       14.973     &    -0.799  &         0.425        &      -41.409    &       17.485     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Water]}    &       5.8097  &       14.763     &     0.394  &         0.694        &      -23.225    &       34.845     \\\\\n",
       "\\textbf{Attack}                     &       0.2508  &        0.051     &     4.940  &         0.000        &        0.151    &        0.351     \\\\\n",
       "\\textbf{Defense}                    &      -0.0096  &        0.060     &    -0.160  &         0.873        &       -0.127    &        0.108     \\\\\n",
       "\\textbf{Speed}                      &      -0.1538  &        0.051     &    -2.998  &         0.003        &       -0.255    &       -0.053     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}               &       0.3484  &        0.059     &     5.936  &         0.000        &        0.233    &        0.464     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}               &       0.1298  &        0.051     &     2.525  &         0.012        &        0.029    &        0.231     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 286.476 & \\textbf{  Durbin-Watson:     } &    1.917  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5187.327  \\\\\n",
       "\\textbf{Skew:}          &   2.807 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  19.725 & \\textbf{  Cond. No.          } & 9.21e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 9.21e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.392\n",
       "Model:                            OLS   Adj. R-squared:                  0.313\n",
       "Method:                 Least Squares   F-statistic:                     4.948\n",
       "Date:                Thu, 14 Nov 2024   Prob (F-statistic):           9.48e-19\n",
       "Time:                        01:59:10   Log-Likelihood:                -1765.0\n",
       "No. Observations:                 400   AIC:                             3624.\n",
       "Df Residuals:                     353   BIC:                             3812.\n",
       "Df Model:                          46                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "Intercept                     10.1046     14.957      0.676      0.500     -19.312      39.521\n",
       "Legendary[T.True]             -3.2717      4.943     -0.662      0.508     -12.992       6.449\n",
       "C(Generation)[T.2]             9.2938      4.015      2.315      0.021       1.398      17.189\n",
       "C(Generation)[T.3]             2.3150      3.915      0.591      0.555      -5.385      10.015\n",
       "C(Generation)[T.4]             4.8353      4.149      1.165      0.245      -3.325      12.995\n",
       "C(Generation)[T.5]            11.4838      3.960      2.900      0.004       3.696      19.272\n",
       "C(Generation)[T.6]             4.9206      4.746      1.037      0.300      -4.413      14.254\n",
       "C(Q(\"Type 1\"))[T.Dark]        -1.4155      6.936     -0.204      0.838     -15.057      12.226\n",
       "C(Q(\"Type 1\"))[T.Dragon]       0.8509      6.900      0.123      0.902     -12.720      14.422\n",
       "C(Q(\"Type 1\"))[T.Electric]    -6.3641      6.537     -0.974      0.331     -19.220       6.491\n",
       "C(Q(\"Type 1\"))[T.Fairy]       -1.9486     10.124     -0.192      0.847     -21.859      17.962\n",
       "C(Q(\"Type 1\"))[T.Fighting]     7.0308      7.432      0.946      0.345      -7.586      21.648\n",
       "C(Q(\"Type 1\"))[T.Fire]         3.0779      6.677      0.461      0.645     -10.055      16.210\n",
       "C(Q(\"Type 1\"))[T.Flying]      -2.1231     22.322     -0.095      0.924     -46.025      41.779\n",
       "C(Q(\"Type 1\"))[T.Ghost]        5.7343      8.488      0.676      0.500     -10.960      22.429\n",
       "C(Q(\"Type 1\"))[T.Grass]        3.3275      5.496      0.605      0.545      -7.481      14.136\n",
       "C(Q(\"Type 1\"))[T.Ground]       9.5118      7.076      1.344      0.180      -4.404      23.428\n",
       "C(Q(\"Type 1\"))[T.Ice]         -0.9313      7.717     -0.121      0.904     -16.108      14.246\n",
       "C(Q(\"Type 1\"))[T.Normal]      18.4816      5.312      3.479      0.001       8.034      28.929\n",
       "C(Q(\"Type 1\"))[T.Poison]       8.3411      7.735      1.078      0.282      -6.871      23.554\n",
       "C(Q(\"Type 1\"))[T.Psychic]      1.8061      6.164      0.293      0.770     -10.317      13.930\n",
       "C(Q(\"Type 1\"))[T.Rock]        -3.8558      6.503     -0.593      0.554     -16.645       8.933\n",
       "C(Q(\"Type 1\"))[T.Steel]       -4.0053      8.044     -0.498      0.619     -19.826      11.816\n",
       "C(Q(\"Type 1\"))[T.Water]        9.7988      5.166      1.897      0.059      -0.361      19.959\n",
       "C(Q(\"Type 2\"))[T.Dark]         5.8719     15.185      0.387      0.699     -23.993      35.737\n",
       "C(Q(\"Type 2\"))[T.Dragon]      13.2777     14.895      0.891      0.373     -16.016      42.571\n",
       "C(Q(\"Type 2\"))[T.Electric]    14.3228     17.314      0.827      0.409     -19.728      48.374\n",
       "C(Q(\"Type 2\"))[T.Fairy]        2.8426     14.268      0.199      0.842     -25.218      30.903\n",
       "C(Q(\"Type 2\"))[T.Fighting]     1.9741     14.089      0.140      0.889     -25.735      29.683\n",
       "C(Q(\"Type 2\"))[T.Fire]         0.2001     15.730      0.013      0.990     -30.736      31.136\n",
       "C(Q(\"Type 2\"))[T.Flying]       6.7292     13.581      0.495      0.621     -19.980      33.438\n",
       "C(Q(\"Type 2\"))[T.Ghost]      -10.9402     15.895     -0.688      0.492     -42.201      20.321\n",
       "C(Q(\"Type 2\"))[T.Grass]        2.5119     14.540      0.173      0.863     -26.084      31.108\n",
       "C(Q(\"Type 2\"))[T.Ground]      13.6042     13.655      0.996      0.320     -13.250      40.459\n",
       "C(Q(\"Type 2\"))[T.Ice]         19.7950     15.068      1.314      0.190      -9.840      49.430\n",
       "C(Q(\"Type 2\"))[T.None]         7.6068     13.162      0.578      0.564     -18.279      33.493\n",
       "C(Q(\"Type 2\"))[T.Normal]      17.3191     17.764      0.975      0.330     -17.618      52.256\n",
       "C(Q(\"Type 2\"))[T.Poison]       0.7770     14.575      0.053      0.958     -27.887      29.441\n",
       "C(Q(\"Type 2\"))[T.Psychic]      4.2480     14.174      0.300      0.765     -23.628      32.124\n",
       "C(Q(\"Type 2\"))[T.Rock]         6.8858     16.221      0.424      0.671     -25.017      38.788\n",
       "C(Q(\"Type 2\"))[T.Steel]      -11.9623     14.973     -0.799      0.425     -41.409      17.485\n",
       "C(Q(\"Type 2\"))[T.Water]        5.8097     14.763      0.394      0.694     -23.225      34.845\n",
       "Attack                         0.2508      0.051      4.940      0.000       0.151       0.351\n",
       "Defense                       -0.0096      0.060     -0.160      0.873      -0.127       0.108\n",
       "Speed                         -0.1538      0.051     -2.998      0.003      -0.255      -0.053\n",
       "Q(\"Sp. Def\")                   0.3484      0.059      5.936      0.000       0.233       0.464\n",
       "Q(\"Sp. Atk\")                   0.1298      0.051      2.525      0.012       0.029       0.231\n",
       "==============================================================================\n",
       "Omnibus:                      286.476   Durbin-Watson:                   1.917\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5187.327\n",
       "Skew:                           2.807   Prob(JB):                         0.00\n",
       "Kurtosis:                      19.725   Cond. No.                     9.21e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 9.21e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a0dc7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.30015614488652215\n"
     ]
    }
   ],
   "source": [
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eea79d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   24.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 14 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>2.25e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:59:11</td>     <th>  Log-Likelihood:    </th> <td> -1783.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3585.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   391</td>      <th>  BIC:               </th> <td>   3621.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                   <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                          <td>   22.8587</td> <td>    3.876</td> <td>    5.897</td> <td> 0.000</td> <td>   15.238</td> <td>   30.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Normal\")[T.True]</th> <td>   17.5594</td> <td>    3.339</td> <td>    5.258</td> <td> 0.000</td> <td>   10.994</td> <td>   24.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Water\")[T.True]</th>  <td>    9.0301</td> <td>    3.172</td> <td>    2.847</td> <td> 0.005</td> <td>    2.794</td> <td>   15.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 2)[T.True]</th>         <td>    6.5293</td> <td>    2.949</td> <td>    2.214</td> <td> 0.027</td> <td>    0.732</td> <td>   12.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 5)[T.True]</th>         <td>    8.4406</td> <td>    2.711</td> <td>    3.114</td> <td> 0.002</td> <td>    3.112</td> <td>   13.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                             <td>    0.2454</td> <td>    0.037</td> <td>    6.639</td> <td> 0.000</td> <td>    0.173</td> <td>    0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                              <td>   -0.1370</td> <td>    0.045</td> <td>   -3.028</td> <td> 0.003</td> <td>   -0.226</td> <td>   -0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                       <td>    0.3002</td> <td>    0.045</td> <td>    6.662</td> <td> 0.000</td> <td>    0.212</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                       <td>    0.1192</td> <td>    0.042</td> <td>    2.828</td> <td> 0.005</td> <td>    0.036</td> <td>    0.202</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>271.290</td> <th>  Durbin-Watson:     </th> <td>   1.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4238.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.651</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>18.040</td>  <th>  Cond. No.          </th> <td>    618.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                     &        HP        & \\textbf{  R-squared:         } &     0.333   \\\\\n",
       "\\textbf{Model:}                             &       OLS        & \\textbf{  Adj. R-squared:    } &     0.319   \\\\\n",
       "\\textbf{Method:}                            &  Least Squares   & \\textbf{  F-statistic:       } &     24.36   \\\\\n",
       "\\textbf{Date:}                              & Thu, 14 Nov 2024 & \\textbf{  Prob (F-statistic):} &  2.25e-30   \\\\\n",
       "\\textbf{Time:}                              &     01:59:11     & \\textbf{  Log-Likelihood:    } &   -1783.6   \\\\\n",
       "\\textbf{No. Observations:}                  &         400      & \\textbf{  AIC:               } &     3585.   \\\\\n",
       "\\textbf{Df Residuals:}                      &         391      & \\textbf{  BIC:               } &     3621.   \\\\\n",
       "\\textbf{Df Model:}                          &           8      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                   &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                            & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                          &      22.8587  &        3.876     &     5.897  &         0.000        &       15.238    &       30.479     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Normal\")[T.True]} &      17.5594  &        3.339     &     5.258  &         0.000        &       10.994    &       24.125     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Water\")[T.True]}  &       9.0301  &        3.172     &     2.847  &         0.005        &        2.794    &       15.266     \\\\\n",
       "\\textbf{I(Generation == 2)[T.True]}         &       6.5293  &        2.949     &     2.214  &         0.027        &        0.732    &       12.327     \\\\\n",
       "\\textbf{I(Generation == 5)[T.True]}         &       8.4406  &        2.711     &     3.114  &         0.002        &        3.112    &       13.770     \\\\\n",
       "\\textbf{Attack}                             &       0.2454  &        0.037     &     6.639  &         0.000        &        0.173    &        0.318     \\\\\n",
       "\\textbf{Speed}                              &      -0.1370  &        0.045     &    -3.028  &         0.003        &       -0.226    &       -0.048     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                       &       0.3002  &        0.045     &     6.662  &         0.000        &        0.212    &        0.389     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                       &       0.1192  &        0.042     &     2.828  &         0.005        &        0.036    &        0.202     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 271.290 & \\textbf{  Durbin-Watson:     } &    1.999  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 4238.692  \\\\\n",
       "\\textbf{Skew:}          &   2.651 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  18.040 & \\textbf{  Cond. No.          } &     618.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.333\n",
       "Model:                            OLS   Adj. R-squared:                  0.319\n",
       "Method:                 Least Squares   F-statistic:                     24.36\n",
       "Date:                Thu, 14 Nov 2024   Prob (F-statistic):           2.25e-30\n",
       "Time:                        01:59:11   Log-Likelihood:                -1783.6\n",
       "No. Observations:                 400   AIC:                             3585.\n",
       "Df Residuals:                     391   BIC:                             3621.\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================================\n",
       "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------\n",
       "Intercept                             22.8587      3.876      5.897      0.000      15.238      30.479\n",
       "I(Q(\"Type 1\") == \"Normal\")[T.True]    17.5594      3.339      5.258      0.000      10.994      24.125\n",
       "I(Q(\"Type 1\") == \"Water\")[T.True]      9.0301      3.172      2.847      0.005       2.794      15.266\n",
       "I(Generation == 2)[T.True]             6.5293      2.949      2.214      0.027       0.732      12.327\n",
       "I(Generation == 5)[T.True]             8.4406      2.711      3.114      0.002       3.112      13.770\n",
       "Attack                                 0.2454      0.037      6.639      0.000       0.173       0.318\n",
       "Speed                                 -0.1370      0.045     -3.028      0.003      -0.226      -0.048\n",
       "Q(\"Sp. Def\")                           0.3002      0.045      6.662      0.000       0.212       0.389\n",
       "Q(\"Sp. Atk\")                           0.1192      0.042      2.828      0.005       0.036       0.202\n",
       "==============================================================================\n",
       "Omnibus:                      271.290   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4238.692\n",
       "Skew:                           2.651   Prob(JB):                         0.00\n",
       "Kurtosis:                      18.040   Cond. No.                         618.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91abe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.29572460427079933\n"
     ]
    }
   ],
   "source": [
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "210a2353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   12.16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 14 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.20e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:59:11</td>     <th>  Log-Likelihood:    </th> <td> -1769.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3579.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   380</td>      <th>  BIC:               </th> <td>   3659.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td>   95.1698</td> <td>   34.781</td> <td>    2.736</td> <td> 0.007</td> <td>   26.783</td> <td>  163.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Normal\")[T.True]</th>     <td>   18.3653</td> <td>    3.373</td> <td>    5.445</td> <td> 0.000</td> <td>   11.733</td> <td>   24.997</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Water\")[T.True]</th>      <td>    9.2913</td> <td>    3.140</td> <td>    2.959</td> <td> 0.003</td> <td>    3.117</td> <td>   15.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 2)[T.True]</th>             <td>    7.0711</td> <td>    2.950</td> <td>    2.397</td> <td> 0.017</td> <td>    1.271</td> <td>   12.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 5)[T.True]</th>             <td>    7.8557</td> <td>    2.687</td> <td>    2.923</td> <td> 0.004</td> <td>    2.572</td> <td>   13.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                 <td>   -0.6975</td> <td>    0.458</td> <td>   -1.523</td> <td> 0.129</td> <td>   -1.598</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                  <td>   -1.8147</td> <td>    0.554</td> <td>   -3.274</td> <td> 0.001</td> <td>   -2.905</td> <td>   -0.725</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                           <td>    0.0189</td> <td>    0.007</td> <td>    2.882</td> <td> 0.004</td> <td>    0.006</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                           <td>   -0.5532</td> <td>    0.546</td> <td>   -1.013</td> <td> 0.312</td> <td>   -1.627</td> <td>    0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                    <td>    0.0090</td> <td>    0.007</td> <td>    1.311</td> <td> 0.191</td> <td>   -0.004</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                     <td>    0.0208</td> <td>    0.008</td> <td>    2.571</td> <td> 0.011</td> <td>    0.005</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>              <td>   -0.0002</td> <td> 9.06e-05</td> <td>   -2.277</td> <td> 0.023</td> <td>   -0.000</td> <td>-2.82e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                           <td>   -0.7277</td> <td>    0.506</td> <td>   -1.439</td> <td> 0.151</td> <td>   -1.722</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                    <td>    0.0136</td> <td>    0.005</td> <td>    2.682</td> <td> 0.008</td> <td>    0.004</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                     <td>    0.0146</td> <td>    0.007</td> <td>    2.139</td> <td> 0.033</td> <td>    0.001</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>              <td>   -0.0002</td> <td>  5.4e-05</td> <td>   -3.383</td> <td> 0.001</td> <td>   -0.000</td> <td>-7.65e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0103</td> <td>    0.007</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.003</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>   -0.0001</td> <td> 6.71e-05</td> <td>   -2.119</td> <td> 0.035</td> <td>   -0.000</td> <td>-1.03e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0002</td> <td> 8.82e-05</td> <td>   -2.075</td> <td> 0.039</td> <td>   -0.000</td> <td>-9.62e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>  2.03e-06</td> <td> 7.42e-07</td> <td>    2.734</td> <td> 0.007</td> <td>  5.7e-07</td> <td> 3.49e-06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.34e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                         &        HP        & \\textbf{  R-squared:         } &     0.378   \\\\\n",
       "\\textbf{Model:}                                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.347   \\\\\n",
       "\\textbf{Method:}                                &  Least Squares   & \\textbf{  F-statistic:       } &     12.16   \\\\\n",
       "\\textbf{Date:}                                  & Thu, 14 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.20e-29   \\\\\n",
       "\\textbf{Time:}                                  &     01:59:11     & \\textbf{  Log-Likelihood:    } &   -1769.5   \\\\\n",
       "\\textbf{No. Observations:}                      &         400      & \\textbf{  AIC:               } &     3579.   \\\\\n",
       "\\textbf{Df Residuals:}                          &         380      & \\textbf{  BIC:               } &     3659.   \\\\\n",
       "\\textbf{Df Model:}                              &          19      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                              &      95.1698  &       34.781     &     2.736  &         0.007        &       26.783    &      163.556     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Normal\")[T.True]}     &      18.3653  &        3.373     &     5.445  &         0.000        &       11.733    &       24.997     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Water\")[T.True]}      &       9.2913  &        3.140     &     2.959  &         0.003        &        3.117    &       15.466     \\\\\n",
       "\\textbf{I(Generation == 2)[T.True]}             &       7.0711  &        2.950     &     2.397  &         0.017        &        1.271    &       12.871     \\\\\n",
       "\\textbf{I(Generation == 5)[T.True]}             &       7.8557  &        2.687     &     2.923  &         0.004        &        2.572    &       13.140     \\\\\n",
       "\\textbf{Attack}                                 &      -0.6975  &        0.458     &    -1.523  &         0.129        &       -1.598    &        0.203     \\\\\n",
       "\\textbf{Speed}                                  &      -1.8147  &        0.554     &    -3.274  &         0.001        &       -2.905    &       -0.725     \\\\\n",
       "\\textbf{Attack:Speed}                           &       0.0189  &        0.007     &     2.882  &         0.004        &        0.006    &        0.032     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                           &      -0.5532  &        0.546     &    -1.013  &         0.312        &       -1.627    &        0.521     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                    &       0.0090  &        0.007     &     1.311  &         0.191        &       -0.004    &        0.023     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                     &       0.0208  &        0.008     &     2.571  &         0.011        &        0.005    &        0.037     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}              &      -0.0002  &     9.06e-05     &    -2.277  &         0.023        &       -0.000    &    -2.82e-05     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                           &      -0.7277  &        0.506     &    -1.439  &         0.151        &       -1.722    &        0.267     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                    &       0.0136  &        0.005     &     2.682  &         0.008        &        0.004    &        0.024     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                     &       0.0146  &        0.007     &     2.139  &         0.033        &        0.001    &        0.028     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}              &      -0.0002  &      5.4e-05     &    -3.383  &         0.001        &       -0.000    &    -7.65e-05     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0103  &        0.007     &     1.516  &         0.130        &       -0.003    &        0.024     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &      -0.0001  &     6.71e-05     &    -2.119  &         0.035        &       -0.000    &    -1.03e-05     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0002  &     8.82e-05     &    -2.075  &         0.039        &       -0.000    &    -9.62e-06     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &     2.03e-06  &     7.42e-07     &     2.734  &         0.007        &      5.7e-07    &     3.49e-06     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.34e+09. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.378\n",
       "Model:                            OLS   Adj. R-squared:                  0.347\n",
       "Method:                 Least Squares   F-statistic:                     12.16\n",
       "Date:                Thu, 14 Nov 2024   Prob (F-statistic):           4.20e-29\n",
       "Time:                        01:59:11   Log-Likelihood:                -1769.5\n",
       "No. Observations:                 400   AIC:                             3579.\n",
       "Df Residuals:                     380   BIC:                             3659.\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                                 95.1698     34.781      2.736      0.007      26.783     163.556\n",
       "I(Q(\"Type 1\") == \"Normal\")[T.True]        18.3653      3.373      5.445      0.000      11.733      24.997\n",
       "I(Q(\"Type 1\") == \"Water\")[T.True]          9.2913      3.140      2.959      0.003       3.117      15.466\n",
       "I(Generation == 2)[T.True]                 7.0711      2.950      2.397      0.017       1.271      12.871\n",
       "I(Generation == 5)[T.True]                 7.8557      2.687      2.923      0.004       2.572      13.140\n",
       "Attack                                    -0.6975      0.458     -1.523      0.129      -1.598       0.203\n",
       "Speed                                     -1.8147      0.554     -3.274      0.001      -2.905      -0.725\n",
       "Attack:Speed                               0.0189      0.007      2.882      0.004       0.006       0.032\n",
       "Q(\"Sp. Def\")                              -0.5532      0.546     -1.013      0.312      -1.627       0.521\n",
       "Attack:Q(\"Sp. Def\")                        0.0090      0.007      1.311      0.191      -0.004       0.023\n",
       "Speed:Q(\"Sp. Def\")                         0.0208      0.008      2.571      0.011       0.005       0.037\n",
       "Attack:Speed:Q(\"Sp. Def\")                 -0.0002   9.06e-05     -2.277      0.023      -0.000   -2.82e-05\n",
       "Q(\"Sp. Atk\")                              -0.7277      0.506     -1.439      0.151      -1.722       0.267\n",
       "Attack:Q(\"Sp. Atk\")                        0.0136      0.005      2.682      0.008       0.004       0.024\n",
       "Speed:Q(\"Sp. Atk\")                         0.0146      0.007      2.139      0.033       0.001       0.028\n",
       "Attack:Speed:Q(\"Sp. Atk\")                 -0.0002    5.4e-05     -3.383      0.001      -0.000   -7.65e-05\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0103      0.007      1.516      0.130      -0.003       0.024\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          -0.0001   6.71e-05     -2.119      0.035      -0.000   -1.03e-05\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0002   8.82e-05     -2.075      0.039      -0.000   -9.62e-06\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")   2.03e-06   7.42e-07      2.734      0.007     5.7e-07    3.49e-06\n",
       "==============================================================================\n",
       "Omnibus:                      252.300   Durbin-Watson:                   1.953\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3474.611\n",
       "Skew:                           2.438   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.590   Cond. No.                     2.34e+09\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.34e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b677e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.35055389205977444\n"
     ]
    }
   ],
   "source": [
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feeb1ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>    15.4</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } &     15.4  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "821a61e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf5dff",
   "metadata": {},
   "source": [
    "transforming from model 3 and model 4 to data 7 is a step by step process. First, we have a model 3 with the formula 'HP ~ Attack + Defense' and model 4 with 'HP ~ Attack * Defense * Speed * Legendary + Q(\"Sp. Def\") * Q(\"Sp. Atk\")', here model 3 is the simplest way by having two continous variables, and model 4 here is have 4 continous variables with interactions between four contious variables, and interations between the other Q(\"Sp. Def\") and Q(\"Sp. Atk\"). Then based on model 3 and model 4, we get a model 5 with expression 'HP ~ Attack + Defense + Speed + Legendary + Q(\"Sp. Def\") + Q(\"Sp. Atk\") + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'. Here model 5 deletes the interaction terms from model 4, keeps the variables from model 3 and 4, and adds categorial variables such as  C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\")). This reduction of interaction helps to reduce the multicollinearity but is not complete, as there will be the possibility that variables will be highly correlated. Then based on model 5, model 6 develops the formula 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\") + I(Q(\"Type 1\")==\"Normal\") + I(Q(\"Type 1\")==\"Water\") + I(Generation==2) + I(Generation==5)'. Here, model 6 removes some of the categorical variables such as C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\")) and contious variables such as Defense or Legendary to simplify the expression, then adding the indicator variable of like I(Generation==2), I(Generation==5). This reduces complexity of various variables and helps to focus on some main ones. Finally, the model 7 gives a expression 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\") + I(Q(\"Type 1\")==\"Normal\") + I(Q(\"Type 1\")==\"Water\") + I(Generation==2) + I(Generation==5)' based on model 6. This one is the comprehensive one, by keeping the variables from model 6 and reintroducing the interations of continous variables again. Doing this is to completet the prediction with as much as main varibles while not adding too much relationship of variables. \n",
    "\n",
    "so the mian principle and rationale is that the prediction models are being improved and adding levels of complexity step by step, but it also pays attention to delete unnecessary variables during the process, finally forming a complete one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ca532",
   "metadata": {},
   "source": [
    "Heres a summary of our conversation:\n",
    "\n",
    "1. **Dummy Variables**: We discussed that dummy variables (also known as indicator or binary variables) are used to represent **categorical data** in a numerical format for statistical models. Each category in a categorical variable is assigned a dummy variable, where it takes the value **1** if the observation belongs to that category, and **0** otherwise.\n",
    "\n",
    "2. **Multiple Categories**: We clarified that **dummy variables can represent a variety of categories**. For example, if you have a variable `Color` with categories such as Red, Blue, Green, and Yellow, dummy variables like `Is_Red`, `Is_Blue`, `Is_Green`, and `Is_Yellow` are created. You typically **drop one category** (the reference category) to avoid multicollinearity, and the model will compare the other categories to this reference.\n",
    "\n",
    "3. **Use in Regression**: Dummy variables are commonly used in regression models to include categorical variables as predictors. The dropped category serves as a reference, and the model coefficients show the effect of the other categories relative to this reference.\n",
    "\n",
    "4. **Reference Category**: We discussed the importance of **dropping one category** when using dummy variables in regression to avoid perfect multicollinearity, which would make the model unable to differentiate between the categories.\n",
    "\n",
    "https://chatgpt.com/share/6736b824-9358-8003-a5e0-9ad870fb7609"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbf01d",
   "metadata": {},
   "source": [
    "### 8. Work with a ChatBot to write a *for* loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" *model performance* metric actualizations (by not using *np.random.seed(130)* within each loop iteration); and explain in your own words the meaning of your results and purpose of this demonstration<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _The following code could likely be slightly edited and repurposed to match the 50-50 **train-test split** analysis and data above (in the `train_test_split` method of \"Question 5\").  Considering the behavior of the `model3_fit` **linear form** specification would be the suggested way to start, but it might also be interesting and/or helpful to consider the different available **linear form** specifications in the manner of this problem..._\n",
    ">    \n",
    "> ```python\n",
    "> import plotly.express as px  # etc.\n",
    ">\n",
    "> songs_training_data,songs_testing_data = train_test_split(songs, train_size=31)\n",
    "> linear_form = 'danceability ~ energy * loudness + energy * mode'\n",
    ">    \n",
    "> reps = 100\n",
    "> in_sample_Rsquared = np.array([0.0]*reps)\n",
    "> out_of_sample_Rsquared = np.array([0.0]*reps)\n",
    "> for i in range(reps):\n",
    ">     songs_training_data,songs_testing_data = \\\n",
    ">       train_test_split(songs, train_size=31)\n",
    ">     final_model_fit = smf.ols(formula=linear_form, \n",
    ">                               data=songs_training_data).fit()\n",
    ">     in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    ">     out_of_sample_Rsquared[i] = \\\n",
    ">       np.corrcoef(songs_testing_data.danceability, \n",
    ">                   final_model_fit.predict(songs_testing_data))[0,1]**2\n",
    ">     \n",
    "> df = pd.DataFrame({\"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    ">                    \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared})   >  \n",
    "> fig = px.scatter(df, x=\"In Sample Performance (Rsquared)\", \n",
    ">                      y=\"Out of Sample Performance (Rsquared)\")\n",
    "> fig.add_trace(go.Scatter(x=[0,1], y=[0,1], name=\"y=x\", line_shape='linear'))  \n",
    "> ```\n",
    ">\n",
    "> _When you first look at this question, you might be unsure about the specific issue that the code is addressing. Take a moment to think about why the code repeatedly randomly re-splits the data, fits the model, and compares the \"in sample\" versus \"out of sample\" **R-squared** values (over and over). Of course, if a **fit model** performs well on the **training** dataset but doesn't do as well on the **testing** dataset then we might be observing the affects of **overfitting**. But why might it sometimes be the opposite situation (which we actually encountered right away for `model3_fit` when the **train-test split** was based on  `np.random.seed(130)` and resulted in a better \"out of sample\" **R-squared** of about `0.21` vereses the 'in-sample\" **R-squared** of about `0.15`)? If you're thinking that this should therefore vice-versa intuitively mean **underfitting**, actually that's not right because **underfitting** is when the **generalizability** of a different model **linear form** specification that provides improved **model performance** is **validated**. What were seeing here, the variable, is something else..._\n",
    ">        \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83da69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.graph_objects as go\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url)\n",
    "\n",
    "# Clean data: Select numeric columns and handle missing values\n",
    "pokeaman = pokeaman.select_dtypes(include=[np.number])\n",
    "pokeaman = pokeaman.fillna(pokeaman.mean())\n",
    "\n",
    "# Split the data into features and target\n",
    "X = pokeaman[['Attack', 'Defense']]\n",
    "y = pokeaman['HP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dddef540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "In Sample Performance (Rsquared) - Pokeaman=%{x}<br>Out of Sample Performance (Rsquared) - Pokeaman=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.2000664556666908,
          0.23524823658591643,
          0.12344872382804506,
          0.13603460861304895,
          0.11409935324213782,
          0.15790248806148532,
          0.3218427503534491,
          0.22842519786324378,
          0.15423934251705262,
          0.17705315786960873,
          0.1881609499697945,
          0.10734436178227724,
          0.1955825248909222,
          0.16008778782321675,
          0.33135411718844476,
          0.15467916227976675,
          0.09871599820122057,
          0.1381173082562055,
          0.2651288754682992,
          0.14735865110056157,
          0.1748760996273705,
          0.13831809859618127,
          0.1472822997517096,
          0.2149010324528925,
          0.1772344701872005,
          0.11975079855033766,
          0.19555024253990516,
          0.15926885913135103,
          0.1449560260571271,
          0.19116205017025956,
          0.184898561484805,
          0.1205191247298264,
          0.18575535177435865,
          0.14309096643523556,
          0.18849875836652086,
          0.22487050530768016,
          0.21827575230038399,
          0.15327066786693733,
          0.2064250607208693,
          0.19954878889551186,
          0.20834554866670152,
          0.12308179924985385,
          0.21264530769135115,
          0.2082155717726254,
          0.11624886436357962,
          0.17748165560729123,
          0.11833673033883452,
          0.14155641054665957,
          0.16386563138184518,
          0.14008372257195112,
          0.17769597798414738,
          0.2122623881510186,
          0.14023479180736054,
          0.2886336767251829,
          0.28244064052510376,
          0.20556360613349423,
          0.20247695601457993,
          0.240772668313193,
          0.24385971160060627,
          0.1529785854164708,
          0.28294733711115805,
          0.14532841402080088,
          0.1779391780051155,
          0.23372997721534228,
          0.1096387192645526,
          0.15412743763065062,
          0.2575805657060113,
          0.2203651673867345,
          0.18480164353924933,
          0.1948377786382841,
          0.20415272037090293,
          0.2740460401565137,
          0.2515343352837571,
          0.2321699956708787,
          0.297211994906908,
          0.15388700920398346,
          0.1831558148660425,
          0.22697439471308678,
          0.20365502133523283,
          0.22618175934748685,
          0.11005309802679064,
          0.21090513315455572,
          0.20108042473755483,
          0.16232125435446287,
          0.20386585319425388,
          0.2638420900405143,
          0.2155753572979846,
          0.17335014794230408,
          0.13901025715965276,
          0.16848900118483867,
          0.1714009510673732,
          0.20926386751569448,
          0.21655966356183576,
          0.18854088538306168,
          0.15435623096335516,
          0.1623002645650048,
          0.1312742196901029,
          0.16991392684024065,
          0.11651174779910167,
          0.18119189311173922
         ],
         "xaxis": "x",
         "y": [
          0.16817073020539905,
          0.13281907694458656,
          0.26587472931717737,
          0.229823726344133,
          0.27637092621193216,
          0.2063320750110497,
          0.0965390972730383,
          0.14312766382913603,
          0.21249075344963503,
          0.1841559136657353,
          0.17459775915048173,
          0.2815446367193863,
          0.1708648351080454,
          0.203490702965537,
          0.0872783121256787,
          0.21631447277259605,
          0.328779679558586,
          0.24176436794451514,
          0.1289344619564702,
          0.22608196734258346,
          0.1895237187471478,
          0.2326690647297844,
          0.2228941039692795,
          0.1517072303016132,
          0.18282184170573712,
          0.25858626316247,
          0.1691650590997361,
          0.20531996210352293,
          0.22615771637204818,
          0.1722000148471856,
          0.17869999662817962,
          0.25118857615222356,
          0.17672387838663262,
          0.2280033264284217,
          0.1780101562727151,
          0.1467253780419177,
          0.15370244838243124,
          0.21112635066621624,
          0.15764798945712574,
          0.16809297595028475,
          0.1591726776306799,
          0.2482096523311098,
          0.15780434765111814,
          0.15785166582424714,
          0.2819747162695745,
          0.18524202485698574,
          0.27303200573365977,
          0.24194748671592725,
          0.19863788578768296,
          0.22265365443165247,
          0.18214143880755101,
          0.1551319430583461,
          0.21720794883738412,
          0.10951939365739566,
          0.09886682548385455,
          0.1564847743908921,
          0.16132207604157484,
          0.13784575578228384,
          0.1263869083453927,
          0.21448585243135632,
          0.1240874178882436,
          0.22954936303962378,
          0.1919715688626269,
          0.13177523927267049,
          0.2926179856576788,
          0.215155098869059,
          0.12897768925618305,
          0.14296565601276648,
          0.18070546611683966,
          0.16963675278129736,
          0.16104570792618234,
          0.11719823552868618,
          0.13525414927001472,
          0.1410068458449218,
          0.10471851749693374,
          0.21497976148052028,
          0.17951945458816634,
          0.1351998482972944,
          0.15503664822898186,
          0.13692517799510506,
          0.29584730877418164,
          0.15575089491728408,
          0.16575941016224938,
          0.20421363779514012,
          0.156889074351938,
          0.11050023286440018,
          0.14596649220201205,
          0.19011319238185911,
          0.22020667809677646,
          0.19182471561132988,
          0.19265483344289186,
          0.14773074994372004,
          0.15668238032909174,
          0.17098632054332974,
          0.21794772534511,
          0.2035197972177935,
          0.2573047511329308,
          0.19641054421014528,
          0.2908462051380574,
          0.18377776848128302
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "shape": "linear"
         },
         "name": "y=x",
         "type": "scatter",
         "x": [
          0,
          1
         ],
         "y": [
          0,
          1
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Pokeaman Dataset: In-Sample vs Out-of-Sample R"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "In Sample Performance (Rsquared) - Pokeaman"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Out of Sample Performance (Rsquared) - Pokeaman"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ccd84aba-9edd-4f9a-a5b5-260037fe4a25\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ccd84aba-9edd-4f9a-a5b5-260037fe4a25\")) {                    Plotly.newPlot(                        \"ccd84aba-9edd-4f9a-a5b5-260037fe4a25\",                        [{\"hovertemplate\":\"In Sample Performance (Rsquared) - Pokeaman=%{x}\\u003cbr\\u003eOut of Sample Performance (Rsquared) - Pokeaman=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.2000664556666908,0.23524823658591643,0.12344872382804506,0.13603460861304895,0.11409935324213782,0.15790248806148532,0.3218427503534491,0.22842519786324378,0.15423934251705262,0.17705315786960873,0.1881609499697945,0.10734436178227724,0.1955825248909222,0.16008778782321675,0.33135411718844476,0.15467916227976675,0.09871599820122057,0.1381173082562055,0.2651288754682992,0.14735865110056157,0.1748760996273705,0.13831809859618127,0.1472822997517096,0.2149010324528925,0.1772344701872005,0.11975079855033766,0.19555024253990516,0.15926885913135103,0.1449560260571271,0.19116205017025956,0.184898561484805,0.1205191247298264,0.18575535177435865,0.14309096643523556,0.18849875836652086,0.22487050530768016,0.21827575230038399,0.15327066786693733,0.2064250607208693,0.19954878889551186,0.20834554866670152,0.12308179924985385,0.21264530769135115,0.2082155717726254,0.11624886436357962,0.17748165560729123,0.11833673033883452,0.14155641054665957,0.16386563138184518,0.14008372257195112,0.17769597798414738,0.2122623881510186,0.14023479180736054,0.2886336767251829,0.28244064052510376,0.20556360613349423,0.20247695601457993,0.240772668313193,0.24385971160060627,0.1529785854164708,0.28294733711115805,0.14532841402080088,0.1779391780051155,0.23372997721534228,0.1096387192645526,0.15412743763065062,0.2575805657060113,0.2203651673867345,0.18480164353924933,0.1948377786382841,0.20415272037090293,0.2740460401565137,0.2515343352837571,0.2321699956708787,0.297211994906908,0.15388700920398346,0.1831558148660425,0.22697439471308678,0.20365502133523283,0.22618175934748685,0.11005309802679064,0.21090513315455572,0.20108042473755483,0.16232125435446287,0.20386585319425388,0.2638420900405143,0.2155753572979846,0.17335014794230408,0.13901025715965276,0.16848900118483867,0.1714009510673732,0.20926386751569448,0.21655966356183576,0.18854088538306168,0.15435623096335516,0.1623002645650048,0.1312742196901029,0.16991392684024065,0.11651174779910167,0.18119189311173922],\"xaxis\":\"x\",\"y\":[0.16817073020539905,0.13281907694458656,0.26587472931717737,0.229823726344133,0.27637092621193216,0.2063320750110497,0.0965390972730383,0.14312766382913603,0.21249075344963503,0.1841559136657353,0.17459775915048173,0.2815446367193863,0.1708648351080454,0.203490702965537,0.0872783121256787,0.21631447277259605,0.328779679558586,0.24176436794451514,0.1289344619564702,0.22608196734258346,0.1895237187471478,0.2326690647297844,0.2228941039692795,0.1517072303016132,0.18282184170573712,0.25858626316247,0.1691650590997361,0.20531996210352293,0.22615771637204818,0.1722000148471856,0.17869999662817962,0.25118857615222356,0.17672387838663262,0.2280033264284217,0.1780101562727151,0.1467253780419177,0.15370244838243124,0.21112635066621624,0.15764798945712574,0.16809297595028475,0.1591726776306799,0.2482096523311098,0.15780434765111814,0.15785166582424714,0.2819747162695745,0.18524202485698574,0.27303200573365977,0.24194748671592725,0.19863788578768296,0.22265365443165247,0.18214143880755101,0.1551319430583461,0.21720794883738412,0.10951939365739566,0.09886682548385455,0.1564847743908921,0.16132207604157484,0.13784575578228384,0.1263869083453927,0.21448585243135632,0.1240874178882436,0.22954936303962378,0.1919715688626269,0.13177523927267049,0.2926179856576788,0.215155098869059,0.12897768925618305,0.14296565601276648,0.18070546611683966,0.16963675278129736,0.16104570792618234,0.11719823552868618,0.13525414927001472,0.1410068458449218,0.10471851749693374,0.21497976148052028,0.17951945458816634,0.1351998482972944,0.15503664822898186,0.13692517799510506,0.29584730877418164,0.15575089491728408,0.16575941016224938,0.20421363779514012,0.156889074351938,0.11050023286440018,0.14596649220201205,0.19011319238185911,0.22020667809677646,0.19182471561132988,0.19265483344289186,0.14773074994372004,0.15668238032909174,0.17098632054332974,0.21794772534511,0.2035197972177935,0.2573047511329308,0.19641054421014528,0.2908462051380574,0.18377776848128302],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"line\":{\"shape\":\"linear\"},\"name\":\"y=x\",\"x\":[0,1],\"y\":[0,1],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"In Sample Performance (Rsquared) - Pokeaman\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Out of Sample Performance (Rsquared) - Pokeaman\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Pokeaman Dataset: In-Sample vs Out-of-Sample R\\u00b2\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ccd84aba-9edd-4f9a-a5b5-260037fe4a25');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px  # etc.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming 'pokeaman' is already loaded as a dataset\n",
    "\n",
    "# 50-50 split for pokeaman dataset (model 3)\n",
    "pokeaman_training_data, pokeaman_testing_data = train_test_split(pokeaman, train_size=0.5)\n",
    "\n",
    "# Linear form for pokeaman dataset (model 3)\n",
    "linear_form_pokeaman = 'HP ~ Attack + Defense'\n",
    "\n",
    "# Number of repetitions for the simulation\n",
    "reps = 100\n",
    "\n",
    "# Arrays to store R-squared values\n",
    "in_sample_Rsquared_pokeaman = np.array([0.0] * reps)\n",
    "out_of_sample_Rsquared_pokeaman = np.array([0.0] * reps)\n",
    "\n",
    "for i in range(reps):\n",
    "    # Re-split pokeaman dataset for each iteration\n",
    "    pokeaman_training_data, pokeaman_testing_data = train_test_split(pokeaman, train_size=0.5)\n",
    "    \n",
    "    # Fit model for pokeaman data (linear_form_pokeaman - model3)\n",
    "    final_model_fit_pokeaman = smf.ols(formula=linear_form_pokeaman, data=pokeaman_training_data).fit()\n",
    "    in_sample_Rsquared_pokeaman[i] = final_model_fit_pokeaman.rsquared\n",
    "    out_of_sample_Rsquared_pokeaman[i] = np.corrcoef(pokeaman_testing_data.HP, \n",
    "                                                      final_model_fit_pokeaman.predict(pokeaman_testing_data))[0, 1]**2\n",
    "\n",
    "# Create DataFrame to store R-squared values for Pokeaman dataset\n",
    "df = pd.DataFrame({\n",
    "    \"In Sample Performance (Rsquared) - Pokeaman\": in_sample_Rsquared_pokeaman,\n",
    "    \"Out of Sample Performance (Rsquared) - Pokeaman\": out_of_sample_Rsquared_pokeaman\n",
    "})\n",
    "\n",
    "# Create scatter plot for Pokeaman dataset\n",
    "fig_pokeaman = px.scatter(df, \n",
    "                           x=\"In Sample Performance (Rsquared) - Pokeaman\", \n",
    "                           y=\"Out of Sample Performance (Rsquared) - Pokeaman\", \n",
    "                           title=\"Pokeaman Dataset: In-Sample vs Out-of-Sample R\")\n",
    "\n",
    "# Add the y=x line for reference\n",
    "fig_pokeaman.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name=\"y=x\", line_shape='linear'))\n",
    "\n",
    "# Show plot for Pokeaman data\n",
    "fig_pokeaman.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280726ae",
   "metadata": {},
   "source": [
    "according to the sample, the data here will shows an negative linar relationship, the more in sample performance r suqare, the less the out of sample performance r square. Meaning that a better fit of in smaple model fitness will result in a poorer fitness of out of sample performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860d40a",
   "metadata": {},
   "source": [
    "Heres a summary of our conversation:\n",
    "\n",
    "1. **Objective**: You are working on a model that evaluates performance using in-sample and out-of-sample R-squared values. This involves running multiple iterations (100 reps) where the data is split into training and testing sets, and a linear regression model is trained and tested. The results are then visualized in a scatter plot.\n",
    "\n",
    "2. **Purpose of the Code**: The goal is to assess how well the model generalizes to unseen data. By running multiple iterations, you obtain a distribution of R-squared values to evaluate model stability and performance.\n",
    "\n",
    "3. **Explanation of Key Components**:\n",
    "   - **In-sample R** measures how well the model fits the training data.\n",
    "   - **Out-of-sample R** measures how well the model generalizes to unseen testing data.\n",
    "   - **Reps (100 iterations)**: This simulates repeated training and testing to understand model performance variability.\n",
    "   - The **scatter plot** displays the in-sample vs out-of-sample performance, with a **y = x line** drawn as a reference to show where the model performs equally on both the training and testing data.\n",
    "\n",
    "4. **y = x Line**:\n",
    "   - It represents the ideal scenario where the model performs the same on both the training and testing sets.\n",
    "   - The points above or below this line help identify issues like overfitting or good generalization.\n",
    "\n",
    "5. **Key Takeaways**:\n",
    "   - A model that performs consistently on both training and testing sets (points near the y = x line) is likely a good model.\n",
    "   - Significant deviation from the line suggests potential issues, such as overfitting or model instability.\n",
    "\n",
    "In essence, the purpose of the process is to evaluate model generalization across multiple random data splits, ensuring that the model is not overfitting and is capable of making reliable predictions on unseen data.\n",
    "\n",
    "https://chatgpt.com/share/6736c917-0904-8003-bde7-5cd5fb489f9e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0145e1",
   "metadata": {},
   "source": [
    "### 9. Work with a ChatBot to understand the meaning of the illustration below; and, explain this in your own words<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _While we had seemed to **validate** the **generalizability** of `model7_fit` in **model building** exercise of the previous \"Question 7\" above, as well as the improved **model performance** of `model7_fit` comapred to `model6_fit`, the `model7_fit` model was always nonetheless more complex than `model6_fit` model (as seen by comparing their `.summary()` methods). This complexity, despite the minimal concerns regarding **multicollinearity**, should always have suggested some room for caution. This is because, as previously discussed in \"Question 6\" above, a complex **linear form** specification can allow a \"**model fit** to 'detect' idiosyncratic associations spuriously present specifically in the **training** dataset but which did not **generalize** to the **testing** dataset.\" Indeed, a close look at the **p-values** in `model7_fit.summary()` will show that the **evidence** (in the data) for many of the **estimated coefficients** of `model7_fit` is in fact not very strong. In comparision, the **evidence** (in the data) for many of the **estimated coefficients** of `model6_fit.summary()` is consistently stronger._\n",
    ">\n",
    "> _As discussed towards the end of the commentary in the previous \"Question 7\" above, the primary purpose of **multiple linear regression** methodology is to allow us to assess the **evidence** (in the data) for a given **linear form** specification based on **coefficient hypothesis testing**. In this regard, then, `model6_fit` might be preferred over `model7_fit` despite the better \"out of sample\" **model performance** of `model7_fit` over `model6_fit`. This may not be enough to convince everyone however, so an additional consideration that might be made here is that the more simpler (more parsimoneous) nature of `model6_fit` should be preferred over `model7_fit` from the perspective of **model interpretability**. Indeed, it is quite unclear how exactly one should think about and understand a four-way **interaction** variable such as `Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")` in conjunction with the whole host of the additional lower order interations. From a **model interpretability** perspective, understanding the meaning of the complex specification of `model7_fit` is \"challenging\" and \"complicated\" to say the least._\n",
    ">\n",
    "> - _There are also often circumstances where **model interpretability** can be MORE IMPORTANT than raw **model performance** in \"out of sample\" **prediction**._\n",
    "> - _This is ESPECIALLY true if **predictive model performance** is relatively comparable between models of two different complexity levels. In such cases, the benefits of better **model interpretability** might provide a clear argument for the simpler (more parsimoneous) model, not to mention the additional potential benefit of more consistent improved **generalizability** over the the more complex model this might offer._\n",
    ">\n",
    "> _This question drives home the point that a simpler (more parsimoneous) model always offers the potential benefit of more consistent **generalizability**, not to mention **interpretability**, over more complex models. We should *ONLY* use increasingly complex models that without questions outperfrm simler models. The code below illustrates this by further additionally raising the consideration that the random **train-test** approach used above is actually not the most natural one available for our dataset, which has different \"Generations\". In fact, if we were actually using this model to make **predictions**, we would increasingly acquire more data over time which we would use to make **precictions** about future data which we haven't yet seen, which is what the code demonstrates. And low and behold, this exposes **generalizability** concerns that we missed when we used the dataset in an idealized way and not actually how we would use such a dataset in practice in the real world (where data would arrive sequentially, and current data is used to predict future data). These **generalizability** concerns do affect both models, but the appear to be more problematic for `model7_fit` than `model6_fit`, which is certainly a result of the increased complexity of `model7_fit` which always opens up the possibility of model **overfitting**._\n",
    "\n",
    "<details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41d57caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fec2b",
   "metadata": {},
   "source": [
    "four r^2 are related to model 7. firstly, the line sets up an OLS regression model (smf.ols), by using formula=model7_linear_form( formula) and trained with Generation 1 data. And then the model7_gen1_predict_future_fit means that the line fits the model to the data for Generation 1 and stores the fit results in model7_gen1_predict_future_fit. Then we printed the \"In sample r^2\", which is based on the performance of the original model that was trained on all generations (not just Generation 1),telling how well the model7 is fitting the model we are trainning; \"out of sample r^2\", which is the original model by computing the correlation coefficient between the actual values (y) and the predicted values,looking for how well the data unseen is for the model we are trainning for ; while in the third onem  \"in sample r^2\", which is only about the model trained by G1, and the last one is about how well the model we are training based on G1 is for those unseen data(like G2-G6). From those values of r square, it can be seen that the G1 is very good at fitting the training model, better than the original one, but it is not good for a generalization for unseen data. The results show that the original model is about the same between the \"in -sample r square and out of smaple r sqaure\", while for G1 model it is good for it self to fit a in sample r square, but worse to fit the out of smaple with 0.115 r square. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "628daf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d81b00",
   "metadata": {},
   "source": [
    "This is basically repeating the steps from the first illustration, except the data excludes the data G6, using G1 to G5 data. The first r^2, is for the data from all Generations excludeing G6, and then the second one for adjusting the unseen data to get r-square. The third here,  is to using the data From G1 to G5 only to find out how well the G1 to G5 data is fitted to the line. Then the final one if to use the G1 to G5 data to predict G6,calculating out-of-sample R-squared, finding out the ability to fit G6. Here, the G1 to G5 model is still poor to fit the out of sample data G6 with a r square 0.2339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c52c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c6a00",
   "metadata": {},
   "source": [
    "Here, it is using the same steps as usual, but this time the model 7 is replaced by model 6, where the orginal data and G1 data is imported into model 6. Again, the model 6 data has a lower 'In sample' R-squared and  'Out of sample' R-squared than model 7,( by using the model 6 to fit with the all available data from G1 to G6). Also, by using the model 6, it also only imports the G1 data only, and 'In sample' R-squared shows how well the G1 data only fits in the model, which has a  0.4433880517727282 value, relatively good for fitting. on the other hand, the 'Out of sample' R-squared here is using unseen data except G1(like from G2 to G6), but the 'Out of sample' R-squared has a 0.193 value, showing that the model for fitting G2-G6 is relatively poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ea14cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259180d",
   "metadata": {},
   "source": [
    "Finally, this model using model 6 is excluding data G6 again. Here, it first used 'In sample' R-squared and 'Out of sample' R-squared of model 6 original data, showing that model 6 is fitting with a value of 0.332 and 0.296, relatively. On the other hand, it is using model 6 buy only importing data from G1 to G5, fitting G1 to G5 into the gen1to5_predict_future model itself, and then fitting unseen data (such as G6) here, to find that the 'Out of sample' R-squared with a 0.263 is still not as good as 'In sample' R-squared of 0.335."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347b402",
   "metadata": {},
   "source": [
    "By comparsion above, we can find out that model 6 is not as effctive as model 7 in terms of original data model fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e4c6c",
   "metadata": {},
   "source": [
    "Heres a concise summary of your explanation:\n",
    "\n",
    "- **Model 6 vs. Model 7**: You are comparing two models, model 6 and model 7. Model 6 is trained on data from Generation 1 (G1) and then tested on both G1 (in-sample) and unseen data from Generations 2 to 6 (out-of-sample). Model 7, however, uses data from all generations (G1 to G6) for training and testing.\n",
    "  \n",
    "- **In-Sample R-squared**: The **\"In sample\" R-squared** for model 6 (0.443) indicates a good fit to Generation 1 data, but **model 7** has a higher R-squared, suggesting it fits the entire dataset (G1 to G6) better.\n",
    "\n",
    "- **Out-of-Sample R-squared**: The **\"Out of sample\" R-squared** for model 6 (0.193) shows that when model 6 is tested on unseen data (G2 to G6), it performs poorly, indicating that a model trained only on G1 data does not generalize well to other generations. **Model 7** performs better in this regard.\n",
    "\n",
    "- **Key Conclusion**: Model 6 is more specialized to Generation 1, but less adaptable to other generations, while model 7 performs better on the whole dataset and generalizes better to unseen data.\n",
    "\n",
    "https://chatgpt.com/share/67355af5-ecf8-8003-a058-680d462ae0dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992924af",
   "metadata": {},
   "source": [
    "## Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course\n",
    "\n",
    "<br>\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "### Ethical Professionalism Considerations\n",
    "\n",
    "This week addresses **multiple linear regression**, perhaps best exemplified through the consideration on **interactions** and their impact on the **model interpretation**, **evidence** and **validity* of models using **coefficient hypothesis testing** and \"in sample\" versus \"out of sample\" **model performance** comparision. Exactly, as in **simple linear regression**, the correctness of **p-values** used to give **evidence** for **predictive associations** that are **estimated** from a dataset depends on the (at least approximate) \"truth\" of the assumptions of the **multiple linear regression**, which are the same as those of the **simple linear regression** with the exception that the specification **linear form** can now model a much richer set of relationships between **predictor** and **outcome variables** based on **predictive associations** observed and **evidenced** in the data. With all this in mind, and reflecting back on the **Ethical Professionalism Considerations** from the previous week concerning **simple linear regression**...\n",
    "\n",
    "> - Which of the methods used for diagnostically assessing the assumptions of a **simple linear regression** specification could be used analogously generalized to the **multiple linear regression** context for the same purpose? \n",
    "> \n",
    "> - Examining the assumption of the **linear form** is more challenging in **multiple linear context**, but can be done using so-called **partial regression** (or **added variable**) **plot**. Is a ChatBot able to provide code to perform this diagnostic and instructions regarding its purpose, interpretation, and appropriate usage?\n",
    ">     \n",
    "> - Are there other diagnostic analyses that a ChatBot might suggest for you to help you evaluate the appropriateness of the assumptions of **fitted multiple linear regression model** you are considering using for **interpretation** or **prediction**? And if so, s the ChatBot able to provide code to perform these additional diagnostic and instructions regarding their purpose, interpretation, and appropriate usages?\n",
    ">     \n",
    "> - What do you think your ethical and professional responsibilites are when it comes to using and leveraging **multiple linear regression** methodology (and associated assumptions therein) in your work? To illustrate and demonstrate your thoughts on these considerations, can you give any specific examples of decisions that might be made during your process of executing a **multiple linear regression** that could have ethical and professional implications, risks, or consequences? What do you think are the simplest steps can you take to ensure that the conclusions of your work are both valid and reliable? What steps do you think are the most challenging from a practical perspective? \n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "\n",
    "**Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times.**\n",
    "\n",
    "Information about the course project is available on the course github repo [here](https://github.com/pointOfive/stat130chat130/tree/main/CP), including a draft [course project specfication](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F23_course_project_specification.ipynb) (subject to change). \n",
    "- The Week 01 HW introduced [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb), and the [available variables](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view). \n",
    "- Please do not download the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage (or the course github repo) multiple times.\n",
    "    \n",
    "> ### NEW DEVELOPMENT<br>New Abilities Achieved and New Levels Unlocked!!!    \n",
    "> \n",
    "> \"Question 3\" as described below only addresses **multiple linear regression**... but you'll also perhaps notice that \"Question 3\" as described above extends this to address addresses **logistic regression**.\n",
    "    \n",
    "### Current Course Project Capability Level \n",
    "    \n",
    "This homework's \"Question 3\" introduced the idea of performing some **multiple linear regression** analyses on dataset from the Canadian Social Connection Survey. While other questions of this homework focussed on other datasets, the general analyses and principles they introduce are no doubt informative and applicable to this the dataset for our course project. Ideally, this should put you in a position to quite proficiently perform **multiple linear regression** analyses for the course project if you so desire and find appropriate for the objectives of you course project submission. Thus, the following (and more) should be possible at this stage... \n",
    "\n",
    "1. Select multiple **predictors predictor** from the Canadian Social Connection Survey data and examine how they jointly influence an outcome variable, paying special attention to the inclusion and interpretation of **categorical** and **indicator variables** and **interactions** (in terms of \"baseline\" reference groups and \"contrast\" or \"offsets\").\n",
    "\n",
    "2. Visualize different kinds of **predictive association** relationships, including **interactions** and relationship between **predictor** and the **outcome** variables that change across different levels of other **categorical** or **indicator predictor variables**, using tools like `plotly.express`.\n",
    "\n",
    "3. Use **coefficient hypothesis testing** and \"in sample\" versus \"out of sample\" **model performance** evaluation to perform **model building** and examine **generalizability** of **fitted models**.\n",
    "       \n",
    "4. Assess the presence of **multicollinearity** by considering the **condition numbers** of **fitted models** (with \"centering and scaling\") and their subsequent potential implications on **generalizability** of **fitted models**; and, perhaps even examine **pairwise correlation** and/or **variance inflation factors** for each **predictor variable** if you're feeling extra ambitious and want to go well \"above and beyond\" (in which case you could also consider the relationship between **multicollinearity** and why one level of a **categorical** variable is always omitted).\n",
    "\n",
    "5. Compare and contrast such analyses and their benefits with previous methodologies introduced and considered in the course.\n",
    "    \n",
    "6. Explore using model diagnostic to check assess the assumptions of your **multiple linear regression** analyses, and reflect on how failurse of these assumptions might impact the reliability of your findings and conlusions derived from your **fitted model**.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204df28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818ac97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94224a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
